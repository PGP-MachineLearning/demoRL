{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QL & DQN - Ta-Te-Ti.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1SJ_SZ7VTymSvCrT-1L9oi7SWUpDEAeIH","authorship_tag":"ABX9TyMNVAH4Qr3kpvmWu56vt6gX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para jugar al Ta-Te-Ti usando primero Q-Learning y luego una red DQN\n","\n"," Basado en los tutoriales de Tensor Flow: https://www.tensorflow.org/agents/tutorials/2_environments_tutorial"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659093483097,"user_tz":180,"elapsed":88895,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"932466ec-11eb-41df-e055-1018b5b48147"},"source":["#@title Instalar Paquete de TF-Agents\n","##!pip install -q tf-agents\n","\n","# usar esta versión para evitar error \n","!pip install tf-agents[reverb]\n","!git clone https://github.com/tensorflow/agents.git\n","!cd agents\n","!git checkout v0.13.0\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf-agents[reverb]\n","  Downloading tf_agents-0.13.0-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.2.0)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[K     |████████████████████████████████| 18.3 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.1.1)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n","Collecting dm-reverb~=0.8.0\n","  Downloading dm_reverb-0.8.0-cp37-cp37m-manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 56.2 MB/s \n","\u001b[?25hCollecting rlds\n","  Downloading rlds-0.1.4-py3-none-manylinux2010_x86_64.whl (37 kB)\n","Collecting tensorflow~=2.9.0\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 5.7 kB/s \n","\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (0.1.7)\n","Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (1.3.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.7.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.3.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.6.3)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 35.2 MB/s \n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (14.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.47.0)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.2.0)\n","Collecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.2)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (21.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (57.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.26.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-agents[reverb]) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-agents[reverb]) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.35.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.9)\n","Installing collected packages: gast, tensorflow-estimator, tensorboard, pygame, keras, flatbuffers, tf-agents, tensorflow, rlds, dm-reverb\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","Successfully installed dm-reverb-0.8.0 flatbuffers-1.12 gast-0.4.0 keras-2.9.0 pygame-2.1.0 rlds-0.1.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tf-agents-0.13.0\n","Cloning into 'agents'...\n","remote: Enumerating objects: 19513, done.\u001b[K\n","remote: Counting objects: 100% (3435/3435), done.\u001b[K\n","remote: Compressing objects: 100% (1107/1107), done.\u001b[K\n","remote: Total 19513 (delta 2418), reused 3151 (delta 2320), pack-reused 16078\u001b[K\n","Receiving objects: 100% (19513/19513), 11.86 MiB | 22.46 MiB/s, done.\n","Resolving deltas: 100% (14759/14759), done.\n","fatal: not a git repository (or any of the parent directories): .git\n","TF-Agentes instalado.\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"wJl4YsniURev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659093488740,"user_tz":180,"elapsed":5698,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"fb4e9465-e34a-4f19-b801-f2a632f0b9f9"},"source":["#@title Cargar Librerías\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_py_environment\n","\n","from tf_agents.environments import utils\n","from tf_agents.specs import array_spec\n","\n","from tf_agents.policies import TFPolicy\n","from tf_agents.policies import random_tf_policy\n","\n","from tf_agents.trajectories import time_step as ts\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents import CategoricalDqnAgent\n","from tf_agents.networks import q_network, categorical_q_network\n","from tf_agents.utils import common\n","\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","\n","tf.compat.v1.enable_v2_behavior()\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Librerías cargadas.\n"]}]},{"cell_type":"markdown","source":["## Clases sobre el Problema a resolver"],"metadata":{"id":"TilJ4dT4SdtK"}},{"cell_type":"code","source":["#@title Definir Juego Ta-Te-Ti (copia de AlphaZero)\n","\n","# define el tablero del juego \n","class Board():\n","\n","    # list of all 8 directions on the board, as (x,y) offsets\n","    __directions = [(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1),(0,1)]\n","\n","    def __init__(self, n=3):\n","        \"Set up initial board configuration.\"\n","\n","        self.n = n\n","        # Create the empty board array.\n","        self.pieces = [None]*self.n\n","        for i in range(self.n):\n","            self.pieces[i] = [0]*self.n\n","\n","    # add [][] indexer syntax to the Board\n","    def __getitem__(self, index): \n","        return self.pieces[index]\n","\n","    def get_legal_moves(self, color):\n","        \"\"\"Returns all the legal moves for the given color.\n","        (1 for white, -1 for black)\n","        @param color not used and came from previous version.        \n","        \"\"\"\n","        moves = set()  # stores the legal moves.\n","\n","        # Get all the empty squares (color==0)\n","        for y in range(self.n):\n","            for x in range(self.n):\n","                if self[x][y]==0:\n","                    newmove = (x,y)\n","                    moves.add(newmove)\n","        return list(moves)\n","\n","    def has_legal_moves(self):\n","        for y in range(self.n):\n","            for x in range(self.n):\n","                if self[x][y]==0:\n","                    return True\n","        return False\n","    \n","    def is_win(self, color):\n","        \"\"\"Check whether the given player has collected a triplet in any direction; \n","        @param color (1=white,-1=black)\n","        \"\"\"\n","        win = self.n\n","        # check y-strips\n","        for y in range(self.n):\n","            count = 0\n","            for x in range(self.n):\n","                if self[x][y]==color:\n","                    count += 1\n","            if count==win:\n","                return True\n","        # check x-strips\n","        for x in range(self.n):\n","            count = 0\n","            for y in range(self.n):\n","                if self[x][y]==color:\n","                    count += 1\n","            if count==win:\n","                return True\n","        # check two diagonal strips\n","        count = 0\n","        for d in range(self.n):\n","            if self[d][d]==color:\n","                count += 1\n","        if count==win:\n","            return True\n","        count = 0\n","        for d in range(self.n):\n","            if self[d][self.n-d-1]==color:\n","                count += 1\n","        if count==win:\n","            return True\n","        \n","        return False\n","\n","    def execute_move(self, move, color):\n","        \"\"\"Perform the given move on the board; \n","        color gives the color pf the piece to play (1=white,-1=black)\n","        \"\"\"\n","\n","        (x,y) = move\n","\n","        # Add the piece to the empty square.\n","        assert self[x][y] == 0\n","        self[x][y] = color\n","\n","# define el juego a aprender\n","class ProblemGame:\n","    def __init__(self, n=3):\n","        self.n = n\n","\n","    def getInitBoard(self):\n","        # return initial board (numpy board)\n","        b = Board(self.n)\n","        return np.array(b.pieces)\n","\n","    def getBoardSize(self):\n","        # (a,b) tuple\n","        return (self.n, self.n)\n","\n","    def getActionSize(self):\n","        # return number of actions\n","        return self.n*self.n + 1\n","\n","    def getNextState(self, board, player, action):\n","        # if player takes action on board, return next (board,player)\n","        # action must be a valid move\n","        if action == self.n*self.n:\n","            return (board, -player)\n","        b = Board(self.n)\n","        b.pieces = np.copy(board)\n","        move = (int(action/self.n), action%self.n)\n","        b.execute_move(move, player)\n","        return (b.pieces, -player)\n","\n","    def getValidMoves(self, board, player):\n","        # return a fixed size binary vector\n","        valids = [0]*self.getActionSize()\n","        b = Board(self.n)\n","        b.pieces = np.copy(board)\n","        legalMoves =  b.get_legal_moves(player)\n","        if len(legalMoves)==0:\n","            valids[-1]=1\n","            return np.array(valids)\n","        for x, y in legalMoves:\n","            valids[self.n*x+y]=1\n","        return np.array(valids)\n","\n","    def getGameEnded(self, board, player):\n","        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n","        b = Board(self.n)\n","        b.pieces = np.copy(board)\n","        if b.is_win(player):\n","            return 1\n","        if b.is_win(-player):\n","            return -1\n","        if b.has_legal_moves():\n","            return 0\n","        # draw has a very little value \n","        return 1e-4\n","\n","    def getCanonicalForm(self, board, player):\n","        # return state if player==1, else return -state if player==-1\n","        return player*board\n","\n","    def getSymmetries(self, board, pi):\n","        # Si el tablero del juego es simétrico\n","        # este método permite rotarlo (mirror, rotational)\n","        # para generar más ejemplos\n","        # asociado a las mismas probabilidades (genera más ejemplos para aprender)\n","        assert(len(pi) == self.n**2+1)  # 1 for pass\n","        pi_board = np.reshape(pi[:-1], (self.n, self.n))\n","        l = []\n","\n","        for i in range(1, 5):\n","            for j in [True, False]:\n","                newB = np.rot90(board, i)\n","                newPi = np.rot90(pi_board, i)\n","                if j:\n","                    newB = np.fliplr(newB)\n","                    newPi = np.fliplr(newPi)\n","                l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n","        return l\n","\n","    def stringRepresentation(self, board):\n","        # 8x8 numpy array (canonical board)\n","        return board.tostring()\n","\n","    @staticmethod\n","    def display(board):\n","        n = board.shape[0]\n","        sangria = \"     \"\n","        print(sangria, \"   \", end=\"\")\n","        for y in range(n):\n","            print(y, \"\", end=\"\")\n","        print(\"\")\n","        print(sangria, \"  \", end=\"\")\n","        for _ in range(n):\n","            print(\"-\", end=\"-\")\n","        print(\"--\")\n","        for y in range(n):\n","            print(sangria, y, \"|\",end=\"\")    # print the row #\n","            for x in range(n):\n","                piece = board[y][x]    # get the piece to print\n","                if piece == -1: print(\"X \",end=\"\")\n","                elif piece == 1: print(\"O \",end=\"\")\n","                else:\n","                    if x==n:\n","                        print(\"-\",end=\"\")\n","                    else:\n","                        print(\"- \",end=\"\")\n","            print(\"|\")\n","        print(sangria, \"  \", end=\"\")\n","        for _ in range(n):\n","            print(\"-\", end=\"-\")\n","        print(\"--\")\n","\n","# define jugador que juega al azar  \n","class RandomPlayer():\n","    def __init__(self, game):\n","        self.game = game\n","\n","    def play(self, board):\n","        a = np.random.randint(self.game.getActionSize())\n","        valids = self.game.getValidMoves(board, 1)\n","        while valids[a]!=1:\n","            a = np.random.randint(self.game.getActionSize())\n","        return a\n","\n","\n","# define jugador humano para jugar \n","class HumanPlayer():\n","   \n","    def __init__(self, game):\n","        self.game = game\n","\n","    def play(self, board):\n","        # display(board)\n","        valid = self.game.getValidMoves(board, 1)\n","        while True: \n","            print(\"Indique la coordenada donde desea jugar: \")\n","            a = input()\n","            x, y = -1, -1\n","            if a.find(' ') > 0:\n","              x, y = [int(x) for x in a.split(' ')]\n","            else:\n","              if a[0].isnumeric() and a[1].isnumeric():\n","                x = int(a[0])\n","                y = int(a[1])\n","            a = self.game.n * x + y if x!= -1 else self.game.n ** 2\n","            if (a>=0) and (a<len(valid)) and valid[a]:\n","                break\n","            else:\n","                print('Coordenada inválida')\n","\n","        return a\n","\n","print(\"Clases del Juego definidas\")"],"metadata":{"id":"o3xJerQUbt7u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659093489273,"user_tz":180,"elapsed":552,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"954b5965-4b3f-4582-add0-76dd46780fc4","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases del Juego definidas\n"]}]},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1659100410779,"user_tz":180,"elapsed":1214,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"06db8b13-59f2-4287-f840-f83c317a44f1"},"source":["#@title Definir Entorno del Problema \n","\n","# función auxiliar para saber si se define función de torneo\n","preparadoHacerCompetir = False\n","\n","# funciones y variables auxiliares\n","def formateaAccion(action, n=3):\n","  return (int(action/n), action%n)\n","\n","# indica si observation es un vector o matriz\n","USAR_OBS_VECTOR = False\n","\n","# utiliza internamente la clase ProblemGame definida anteriomente\n","# al momento de crear se determina el tipo de contrincante \n","class ProblemGameEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self, jugContrincante=None):\n","    # inicializa el juego\n","    self._juego = ProblemGame()\n","    # inicializa si corresponde el contrincante \n","    # (usado para usar agentes no TF-Agents)\n","    if jugContrincante is None:\n","      self._jugadorContrincante = None\n","    else:\n","      self._jugadorContrincante = jugContrincante(self._juego)\n","    # inicializa variables auxiliares\n","    cantPosiblesAcciones = self._juego.getActionSize() - 2\n","    auxiTTamTablero = self._juego.getBoardSize()    \n","    # determina formas\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=cantPosiblesAcciones, name='action')\n","    if USAR_OBS_VECTOR:\n","      # devuelve como vector\n","      cantReprsTablero = auxiTTamTablero[0] * auxiTTamTablero[1]\n","      self._observation_spec = array_spec.BoundedArraySpec(\n","          shape=(cantReprsTablero,), dtype=np.int32, minimum=-1, maximum=1, name='observation')\n","    else:\n","      # devuelve como matriz\n","      self._observation_spec = array_spec.BoundedArraySpec(\n","          shape=(auxiTTamTablero[0], auxiTTamTablero[1],), dtype=np.int32, minimum=-1, maximum=1, name='observation')\n","    # inicializa el tablero\n","    self._reset()\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def formateaJugador(self, idJugador):\n","    if (idJugador==1):\n","      return \"O\"\n","    else:\n","      return \"X\"\n","\n","  def jugarContrincante(self):\n","    # si puede jugar    \n","    if self._jugadorContrincante is None:\n","      return False\n","    if self._juego.getGameEnded(self._board, -1) == 0:\n","      # determina la acción y la aplica\n","      action = self._jugadorContrincante.play(self._juego.getCanonicalForm(self._board, -1))\n","      self._board, sgtIdJugador = self._juego.getNextState(self._board, -1, action)  \n","      self._textoRender = self._textoRender + \"\\t\" + self.formateaJugador(-1) + \" juega pos \" + str(formateaAccion(action)) + \"\\n\"        \n","    return True \n","\n","  def devolverEstadoTablero(self, idJug=1):\n","    # devuelve estado de tablero\n","    # (siempre el policy es jugador 1)\n","    # como matriz\n","    auxTablero = self._juego.getCanonicalForm(self._board, idJug)\n","    if USAR_OBS_VECTOR:\n","      # como vector\n","      auxTablero = np.array(auxTablero).flatten()\n","    return auxTablero\n","    \n","  def _reset(self):\n","    # resetea el entorno\n","    self._board = self._juego.getInitBoard()\n","    self._rewardVal = 0\n","    # determina si empieza contrincante    \n","    self._textoRender = \"\"    \n","    if self._jugadorContrincante is not None:\n","      if randint(0,9) < 5 :\n","        self._textoRender = \"\\tcomienza \" + self.formateaJugador(-1) + \"\\n\"\n","        self.jugarContrincante()\n","    # continua con el otro jugador\n","    self._idJugador = 1      \n","    return ts.restart( np.array(self.devolverEstadoTablero(1), dtype=np.int32) )\n","\n","  def _step(self, action):\n","    # ejecuta la acción sobre el entorno \n","\n","    if self._juego.getGameEnded(self._board, self._idJugador) != 0:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n"," \n","    # determina id del jugador actual\n","    idJug = self._idJugador\n","\n","    # Controla que la acción es válida y la aplica\n","    validAccs = self._juego.getValidMoves(self._board, idJug)\n","    if (action>=0) and (action<len(validAccs)) and (validAccs[action] == 1):\n","        # Aplica la acción\n","        self._board, sgtJugador = self._juego.getNextState(self._board, idJug, action)          \n","        penaliza = 0.0\n","        self._textoRender = self._textoRender + \"\\t\" + self.formateaJugador(idJug) + \" juega pos \" + str(formateaAccion(action)) + \"\\n\"\n","    else:\n","        # Si no es válida, la ignora pero la penaliza\n","        # (si tira error, corta la jugada)\n","        penaliza = -10.0\n","        self._textoRender = self._textoRender + \"\\t\" + self.formateaJugador(idJug) + \" intenta acción inválida con pos \" + str(formateaAccion(action)) + \"\\n\"\n","    ##  raise ValueError('Acción ' + str(action) + ' inválida ' + str(validAccs) + '.')\n","\n","    # si juega el contrincante\n","    if self.jugarContrincante():\n","      # mantiene id 1 del jugador (contrincante siempre es -1)\n","      self._idJugador = 1  \n","    else:\n","      # alterna el id del jugador \n","      self._idJugador = idJug * (-1) \n","    \n","    # determina el estado del tablero (para devolver)\n","    estadoTablero = self.devolverEstadoTablero(self._idJugador)\n","\n","    # analiza si finaliza (siempre de jugador 1)\n","    terminaJuego = self._juego.getGameEnded(self._board, 1) \n","    if terminaJuego != 0:\n","      # si finaliza\n","      # determina el reward \n","      # (positivo para jugador 1 y negativo para jugador -1)\n","      self._rewardVal = terminaJuego * 100 \n","      if terminaJuego < 0:\n","        self._rewardVal = self._rewardVal - penaliza\n","      else:\n","        self._rewardVal = self._rewardVal + penaliza\n","      return ts.termination(\n","              np.array(estadoTablero, dtype=np.int32),\n","              reward=self._rewardVal)\n","    else:      \n","      if penaliza < 0:\n","        # si no aplico una acción válida, devuelve el valor que penaliza\n","        auxRewardVal = penaliza \n","      else:\n","        # si aplico una acción válida, devuelve la cantidad de fichas que coloco\n","        auxRewardVal = np.count_nonzero(estadoTablero == 1) * 10.0\n","      # si no finaliza\n","      return ts.transition(\n","              np.array(estadoTablero, dtype=np.int32),\n","              reward=auxRewardVal, discount=0.9)\n","\n","\n","  def render(self, mode = 'human'):\n","    # muestra información sobre el entorno\n","    if mode == 'human':\n","      # (en este caso el tablero actual y jugadas)\n","      if (self._textoRender != \"\"):\n","        # muestra texto con jugadas \n","        print(self._textoRender)\n","        self._textoRender = \"\"\n","        # muestra el tablero\n","        self._juego.display(self._board)\n","    elif mode == 'humanFinal':\n","      # (en este caso el tablero final y reward)\n","        # muestra el tablero\n","        self._juego.display(self._board)\n","    elif mode == 'humanReward':\n","      if self._rewardVal != 0:\n","        #print('\\n = Recompensa final = ', round(self._rewardVal,3))\n","        if self._rewardVal >= 75:\n","          print(\"    -> GANA \" + self.formateaJugador(1) + \" y PIERDE \" + self.formateaJugador(-1) + \"\")\n","        elif self._rewardVal <= -75:\n","          print(\"    -> GANA \" + self.formateaJugador(-1) + \" y PIERDE \" + self.formateaJugador(1) + \"\")\n","        else:\n","          print(\"    -> EMPATAN \" + self.formateaJugador(1) + \" y \" + self.formateaJugador(-1) + \"\")\n","    # devuelve estado del tablero\n","    return self._board\n","\n","print(\"Entorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y evaluación\n","train_py_env = ProblemGameEnv(RandomPlayer)\n","evalRP_py_env = ProblemGameEnv(RandomPlayer)\n","eval2P_py_env = ProblemGameEnv()\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","evalRP_env = tf_py_environment.TFPyEnvironment(evalRP_py_env)\n","eval2P_env = tf_py_environment.TFPyEnvironment(eval2P_py_env)\n","eval_env = evalRP_env\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"Entornos de entrenamiento y prueba definidos. \")\n","\n","# definir simulador para probar el entorno jugando dos jugadores\n","def SimularEntorno(env, policyList, descAgList, titulo, num_episodes=1, mostrarDetalle=True):\n","    # controla parámetros\n","    if num_episodes <=0:\n","      num_episodes = 1    \n","    if (policyList is None) or (len(policyList) < 1) or (len(policyList) > 2):\n","      raise ValueError('Se deben indicar 1 o 2 políticas para simular.')\n","      return -1\n","    if (descAgList is None):\n","      descAgList = [\"Agente 1\", \"Agente 2\"]\n","    elif len(policyList) < 1:\n","      descAgList.append(\"Otro Agente\")\n","    # inicializa acumulador auxiliar \n","    cantGanados = [0.0, 0.0]\n","    # muestra títulos\n","    print(\"\\n** \", titulo, \" **\")     \n","    print(\"    (\" + descAgList[0] + \" usa 'O', \" + descAgList[1] + \" usa 'X') \")              \n","    for i in range(num_episodes):\n","      if num_episodes > 1:\n","        print(\"\\n> Episodio \" + str(i+1) + \": \")\n","      # muesta estado inicial\n","      time_step = env.reset()  \n","      ##print(time_step)\n","      if mostrarDetalle:\n","        print(\" Ini: \")      \n","        env.pyenv.render(\"human\")\n","        print(\" \")\n","      j = 1\n","      corteXcant = False      \n","      while not time_step.is_last():     \n","        for policy in policyList:\n","          # la política determina la acción a realizar           \n","          action_step = policy.action(time_step)\n","          ##print( time_step , \"\\n\", policy, \"\\n\",  action_step )          \n","          time_step = env.step(action_step.action)\n","          if time_step.is_last():\n","            # se termino\n","            break\n","        # recupera la observación y muestra el nuevo estado \n","        if mostrarDetalle:\n","          print(\" #\" + str(j) + \":\", end=\"\")\n","          # muestra estado del tablero\n","          env.pyenv.render(\"human\")\n","          print(\" \")\n","        j = j + 1\n","        if j >= 101:\n","          print(\"Se finaliza por hacer más de \" + str(j-1) + \"!!!\")\n","          corteXcant = True\n","          break\n","        \n","      # muestra estado final\n","      ob = time_step.observation.numpy()[0]\n","      r = time_step.reward.numpy()[0]\n","      if mostrarDetalle:\n","        print(\" Fin: \")\n","      else:\n","        # muestra tablero final solamente\n","        env.pyenv.render(\"humanFinal\")  \n","      # muestra reward\n","      env.pyenv.render(\"humanReward\")\n","      if corteXcant:\n","        # se corta por cantidad de ciclos\n","        # es un empate pero ambos pierden\n","        cantGanados[0] -= 0.5\n","        cantGanados[1] -= 0.5        \n","      elif r >= 75:\n","        # gana 1\n","        cantGanados[0] += 1.0\n","      elif r <= -75:\n","        # gana -1\n","        cantGanados[1] += 1.0\n","      else:\n","        # empatan\n","        cantGanados[0] += 0.5\n","        cantGanados[1] += 0.5\n","    if num_episodes > 1:\n","      print(\"\\n= Total Partidos Ganados: \")\n","      maxGan = np.max(cantGanados)\n","      fichaJuega = \"O\"\n","      for agDesc, ganCant in zip(descAgList, cantGanados):\n","        print(\"\\t \" + agDesc +\" con '\" + fichaJuega + \"': \" + str(round(ganCant,1)) + \" \" + (\"*\" if ganCant==maxGan else \"\") )\n","        fichaJuega = \"X\"\n","      return cantGanados\n","    else:\n","      return cantGanados\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","MostarDetalleJugada = True #@param {type:\"boolean\"}\n","\n","if Probar_Entorno:\n","  SimularEntorno(evalRP_env, [random_policy], [\"AzarPolicy\", \"AzarRP\"], \"Probando el entorno del problema al azar (con una policiy)\", 1, MostarDetalleJugada)\n","  print(\"\\n\")\n","  SimularEntorno(eval2P_env, [random_policy, random_policy], [\"AzarPolicy1\", \"AzarPolicy2\"], \"Probando el entorno del problema al azar (con dos policy)\", 1, MostarDetalleJugada)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entorno del Problema definido.\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema al azar (con una policiy)  **\n","    (AzarPolicy usa 'O', AzarRP usa 'X') \n"," Ini: \n"," \n"," #1:\tO juega pos (2, 1)\n","\tX juega pos (2, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |- - - |\n","      1 |- - - |\n","      2 |X O - |\n","        --------\n"," \n"," #2:\tO juega pos (0, 0)\n","\tX juega pos (1, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |X - - |\n","      2 |X O - |\n","        --------\n"," \n"," #3:\tO intenta acción inválida con pos (1, 0)\n","\tX juega pos (0, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |O X - |\n","      1 |X - - |\n","      2 |X O - |\n","        --------\n"," \n"," #4:\tO intenta acción inválida con pos (1, 0)\n","\tX juega pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |O X - |\n","      1 |X - X |\n","      2 |X O - |\n","        --------\n"," \n"," #5:\tO intenta acción inválida con pos (0, 1)\n","\tX juega pos (0, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |O X X |\n","      1 |X - X |\n","      2 |X O - |\n","        --------\n"," \n"," #6:\tO intenta acción inválida con pos (0, 2)\n","\tX juega pos (1, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |O X X |\n","      1 |X X X |\n","      2 |X O - |\n","        --------\n"," \n"," Fin: \n","    -> GANA X y PIERDE O\n","\n","\n","\n","**  Probando el entorno del problema al azar (con dos policy)  **\n","    (AzarPolicy1 usa 'O', AzarPolicy2 usa 'X') \n"," Ini: \n"," \n"," #1:\tO juega pos (2, 2)\n","\tX juega pos (0, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - - |\n","      1 |- - - |\n","      2 |- - O |\n","        --------\n"," \n"," #2:\tO juega pos (1, 2)\n","\tX juega pos (2, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |X - - |\n","      1 |- - O |\n","      2 |- X O |\n","        --------\n"," \n"," #3:\tO intenta acción inválida con pos (0, 0)\n","\tX intenta acción inválida con pos (2, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |X - - |\n","      1 |- - O |\n","      2 |- X O |\n","        --------\n"," \n"," #4:\tO juega pos (1, 1)\n","\tX juega pos (0, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |X - X |\n","      1 |- O O |\n","      2 |- X O |\n","        --------\n"," \n"," #5:\tO intenta acción inválida con pos (2, 2)\n","\tX juega pos (2, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - X |\n","      1 |- O O |\n","      2 |X X O |\n","        --------\n"," \n"," #6:\tO juega pos (1, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - X |\n","      1 |O O O |\n","      2 |X X O |\n","        --------\n"," \n"," Fin: \n","    -> GANA O y PIERDE X\n"]}]},{"cell_type":"markdown","source":["##Q-Learning"],"metadata":{"id":"KmGJpYXvJI-W"}},{"cell_type":"code","source":["#@title Define clase auxiliar Policy para Q-Learning\n","\n","## funciones auxiliares basadas en:\n","##  https://rubikscode.net/2019/06/24/introduction-to-q-learning-with-python-and-open-ai-gym/\n","\n","from tf_agents.policies.fixed_policy import FixedPolicy\n","from tf_agents.trajectories import policy_step\n","from tf_agents.utils import nest_utils\n","from tf_agents.specs import tensor_spec\n","from tf_agents.typing import types\n","from typing import Optional, Text\n","\n","## funciones auxiliares para manejar TS->Obs\n","## (se puede definir otras si es necesario)\n","\n","# devuelve el primer valor de OBS \n","# como número de estado\n","def obtenerEstadoDeObs(obs):  \n","  return obs[0]\n","\n","# devuelve valor unico en base a vector OBS \n","# para determinar fila de estado\n","def generarNroEstadoVector(obs):\n","  ntarget = np.array(obs).flatten()\n","  valorTotal = 0        \n","  for i in range(1, len(ntarget)):\n","    if ntarget[i] == -1:\n","      valorTotal = valorTotal + 2**i    \n","    elif ntarget[i] == 1:\n","      valorTotal = valorTotal + 3**i      \n","  return valorTotal\n","\n","# devuelve la cantidad de estados máxima\n","# que se pueden generar usando un ejemplo \n","# del vector de OBS \n","def calcMaxNroEstadoVector(ejemploObs):\n","  ntarget = np.array(ejemploObs).flatten()  \n","  cant = len(ntarget)\n","  return ( 3**cant + 2**cant + 1 )\n","\n","\n","# Clase Policy Wrapper para Q-Learning\n","# (se hereda de FixedPolicy porque es una simple para tener como base)\n","class QL_TF_Policy(FixedPolicy):\n","    \n","  def __init__(self,\n","               posiblesEstadosList, \n","               posiblesAccionesList,\n","               time_step_spec: ts.TimeStep,            \n","               action_spec: types.NestedTensorSpec,                              \n","               funcDevuelveNroEstado = obtenerEstadoDeObs,   \n","               policy_info: types.NestedTensorSpec = (),\n","               info_spec: types.NestedTensorSpec = (),\n","               name: Optional[Text] = None):    \n","    \n","      # llama al padre\n","      super(FixedPolicy, self).__init__(time_step_spec, action_spec, clip=False,\n","                                      info_spec=info_spec,\n","                                      name=name,\n","                                      emit_log_probability=False)             \n","      # guarda valores auxiliares\n","      self._policy_info = policy_info\n","      self._time_step_spec = tensor_spec.from_spec(time_step_spec)\n","      self._action_spec = tensor_spec.from_spec(action_spec)   \n","      self._funcDevuelveNroEstado = funcDevuelveNroEstado\n","      # inicializa parametros de matriz\n","      if (posiblesEstadosList is None) or (len(posiblesEstadosList)<2):\n","       raise ValueError('No se ha definida la lista de posibles estdos!' )\n","      self._posiblesEstadosList = posiblesEstadosList\n","      self._cantEstados = len(posiblesEstadosList)\n","      if (posiblesAccionesList is None) or (len(posiblesAccionesList)<2):\n","       raise ValueError('No se ha definida la lista de posibles acciones!' )\n","      self._posiblesAccionesList = posiblesAccionesList\n","      self._cantAcciones = len(posiblesAccionesList)\n","      self.ResetQ()\n","  \n","  # funcion auxiliar para inicializar la matriz\n","  def ResetQ(self):     \n","      self._Qtable = np.zeros([self._cantEstados, self._cantAcciones])\n","      self._QtableEntrenada = False \n","\n","  # función auxiliar de entrenamiento\n","  def TrainQ(self, env, train_policy, alpha = 0.1, gamma = 0.6, epsilon = 0.1, cant_ciclos_entrenamiento = 100000, log_cada_ciclos = 1000, mostrarDetalleAcciones=False):\n","      ##print(self._Qtable.shape)\n","      ##print(self._usarOBSVector)\n","      # ejecuta el entrenamiento\n","      for step in range(1, cant_ciclos_entrenamiento+1):\n","          # Resetea el enviroment\n","          time_step = env.reset()  \n","          ob = time_step.observation.numpy()[0]\n","          state = self._funcDevuelveNroEstado(ob)\n","          secuenciaAcciones = \"\"\n","          j = 1\n","          # Simula      \n","          while not time_step.is_last():\n","              # Considera lo aprendido o toma al azar depende de azar y epsilon\n","              if random.uniform(0, 1) < epsilon:\n","                  # toma de Matriz-Q\n","                  accionAplicar = np.argmax( self._Qtable[state] )              \n","              else:\n","                  # toma al azar\n","                  action_step = train_policy.action( time_step ) \n","                  accionAplicar = action_step.action.numpy()[0]\n","              if j > 1:\n","                secuenciaAcciones = secuenciaAcciones + \" + \"\n","              secuenciaAcciones = secuenciaAcciones + self._posiblesAccionesList[ accionAplicar ]\t\n","              j = j + 1\n","              # Aplica la Accion  \n","              time_step = env.step( accionAplicar )\n","              ob = time_step.observation.numpy()[0]\n","              next_state = self._funcDevuelveNroEstado(ob)\n","              r = time_step.reward.numpy()[0] \n","              # Recalcula Q\n","              q_value = self._Qtable[state, accionAplicar]\n","              max_value = np.max(self._Qtable[next_state])\n","              new_q_value = (1 - alpha) * q_value + alpha * (r + gamma * max_value)                       \n","              # Actualiza Matriz-Q\n","              self._Qtable[state, accionAplicar] = new_q_value\n","              state = next_state        \n","          # muestra estado\n","          if log_cada_ciclos > 0:\n","            if (step == 1) or (step % log_cada_ciclos == 0):\n","              if mostrarDetalleAcciones:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) = {3} '.format(step, r, (j-1), secuenciaAcciones))  \n","              else:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) '.format(step, r, (j-1)))  \n","      # Devuelve Matriz-Q\n","      self._QtableEntrenada = True\n","      return self._Qtable   \n","\n","  # obtiene Q-Table como DataFrame\n","  def getQ_DF(self):\n","        cm = self._Qtable \n","        cmtx = pd.DataFrame(\n","            cm, \n","            index=self._posiblesEstadosList, \n","            columns=self._posiblesAccionesList\n","          )\n","        return cmtx\n","\n","  # graba Q-Table como CSV\n","  def saveQ(self, fileName):\n","        # obtiene data frame de matriz Q\n","        df = self.getQ_DF()\n","        # graba matriz Q como CSV\n","        df.to_csv(fileName, index=True)\n","        return \n","\n","  # recupera Q-Table de  CSV\n","  def loadQ(self, fileName):\n","        # lee matriz Q de CSV\n","        df = pd.read_csv(fileName)         \n","        # carga elementos en memoria\n","          # saca nombre de estados\n","        self._posiblesEstadosList = list(df.pop(df.columns[0]))  \n","          # acciones\n","        self._posiblesAccionesList = list(df.columns.tolist())\n","          # borra el primer elemento de titulo\n","        ##self._posiblesAccionesList = np.delete(self._posiblesAccionesList, 0, axis=0)      \n","         # matriz Q\n","        self._Qtable = list(df.to_numpy())\n","        # re-inicializa las cantidades\n","        self._cantEstados = len(self._posiblesEstadosList)\n","        self._cantAcciones = len(self._posiblesAccionesList)\n","        self._QtableEntrenada = True\n","        # muestra resultados\n","        self.MostrarQ(\"Matriz-Q recuperada:\")\n","        return \n","\n","  # función auxiliar para mostrar matri Q\n","  def MostrarQ(self, titulo=\"Matriz-Q entrenada:\"):\n","        # muestra Q table\n","        cmtx = self.getQ_DF()\n","        print('\\n ' + titulo + ' ')        \n","        # agrega para poder mostrar la matrix de confusión completa\n","        pd.options.display.max_rows = 100\n","        pd.options.display.max_columns = 100\n","        print(cmtx)\n","        print(\"\\n\")\n","        return \n","\n","  # devuelve la accion que se debe aplicar usando la matrix Q entrenada\n","  def _action(self, time_step, policy_state, seed):    \n","      # determina la accion a realizar\n","      # obtiene estado actual\n","      ob = time_step.observation.numpy()[0]\n","      state = self._funcDevuelveNroEstado(ob)\n","      # toma de Matriz-Q      \n","      accionAplicar = np.argmax( self._Qtable[state] )\n","      # formatea el valor a devolver usando la action_spec y time_step_spec\n","      def convert(action, spec):\n","        return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n","      self._action_value = tf.nest.map_structure(convert, accionAplicar,\n","                                                  self._action_spec)\n","      outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n","      action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n","                                   self._action_value)\n","      # devuelve la accion\n","      return policy_step.PolicyStep(action, policy_state, self._policy_info)\n","\n","\n","print(\"Clase QL_TF_Policy creada.\")"],"metadata":{"cellView":"form","id":"vNLy2mNyi2xy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659093565645,"user_tz":180,"elapsed":662,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"e6b2508b-a98f-4a18-9033-7b531dc7ffe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase QL_TF_Policy creada.\n"]}]},{"cell_type":"code","source":["#@title Entrenar con algoritmo Q-Learning\n","\n","# parámetros\n","entrenar_QL = True #@param {type:\"boolean\"}\n","alpha = 0.1 #@param {type:\"number\"}\n","gamma = 0.7 #@param {type:\"number\"}\n","epsilon = 0.6 #@param {type:\"number\"}\n","cant_ciclos_entrenamiento_finalizar = 7500 #@param {type:\"integer\"}\n","mostar_recompensa_cada = 500  #@param {type:\"integer\"}\n","if alpha <= 0.0:\n","   alpha = 0.001\n","if gamma <= 0.0:\n","    gamma = 0.001\n","if epsilon <= 0.0:\n","   epsilon = 0.001    \n","if cant_ciclos_entrenamiento_finalizar <= 10:\n","   cant_ciclos_entrenamiento_finalizar = 10    \n","if mostar_recompensa_cada < 1:\n","  mostar_recompensa_cada = 1\n","\n","\n","# valores auxiliares para TaTeTi\n","auxjuego = ProblemGame()\n","cantPosiblesAcciones = auxjuego.getActionSize() - 1\n","posiblesAccionesDescrip = [str(formateaAccion(x)) for x in range(0, cantPosiblesAcciones)]\n","ejObs = train_env.reset().observation\n","cantPosiblesEstados = calcMaxNroEstadoVector(ejObs)\n","posiblesEstadosDescrip = ['E{:}'.format(x) for x in range(0, cantPosiblesEstados+1)]\n","\n","# instancia política de Q Learning\n","ql_policy = QL_TF_Policy(posiblesEstadosList = posiblesEstadosDescrip,\n","                         posiblesAccionesList = posiblesAccionesDescrip,                         \n","                         time_step_spec = train_env.time_step_spec(),\n","                         action_spec = train_env.action_spec(),\n","                         funcDevuelveNroEstado = generarNroEstadoVector\n","                         )\n","\n","if entrenar_QL:\n","  # hace el entrenamiento\n","  print(\"** Comienza el Entrenamiento:\\n\")\n","  ql_policy.TrainQ(env = train_env, \n","                   train_policy = random_policy, \n","                   alpha = alpha, gamma = gamma, epsilon = epsilon, \n","                   cant_ciclos_entrenamiento = cant_ciclos_entrenamiento_finalizar, \n","                   log_cada_ciclos = mostar_recompensa_cada,\n","                   mostrarDetalleAcciones = False)\n","  print(\"\\n** Entrenamiento Finalizado **\")\n","  # muestra matriz\n","  ql_policy.MostrarQ()\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente Q-Learning.\")  "],"metadata":{"cellView":"form","id":"lQZNJ9kVoDLG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659093622505,"user_tz":180,"elapsed":56866,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"2a57637d-213b-42a3-bed6-7a591f040e9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** Comienza el Entrenamiento:\n","\n","step = 1: Recompensa = -90.0 - Acciones(6) \n","step = 500: Recompensa = -90.0 - Acciones(4) \n","step = 1000: Recompensa = -90.0 - Acciones(5) \n","step = 1500: Recompensa = -90.0 - Acciones(4) \n","step = 2000: Recompensa = -90.0 - Acciones(5) \n","step = 2500: Recompensa = -90.0 - Acciones(5) \n","step = 3000: Recompensa = -100.0 - Acciones(2) \n","step = 3500: Recompensa = 100.0 - Acciones(3) \n","step = 4000: Recompensa = 100.0 - Acciones(4) \n","step = 4500: Recompensa = -100.0 - Acciones(2) \n","step = 5000: Recompensa = -90.0 - Acciones(5) \n","step = 5500: Recompensa = -90.0 - Acciones(5) \n","step = 6000: Recompensa = -90.0 - Acciones(3) \n","step = 6500: Recompensa = 100.0 - Acciones(4) \n","step = 7000: Recompensa = -100.0 - Acciones(4) \n","step = 7500: Recompensa = -100.0 - Acciones(5) \n","\n","** Entrenamiento Finalizado **\n","\n"," Matriz-Q entrenada: \n","           (0, 0)     (0, 1)     (0, 2)     (1, 0)     (1, 1)     (1, 2)  \\\n","E0      20.859320  29.295749  23.008208  23.300199  24.259172  43.744533   \n","E1       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","E2      21.186880  -0.163247  18.777047  12.567027   7.752092  16.950754   \n","E3      -0.734000   0.220939   2.000000   2.000000   4.486000   5.941079   \n","E4      25.176382  14.338724  -2.537792  13.761379   6.999106  17.293247   \n","...           ...        ...        ...        ...        ...        ...   \n","E20192   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","E20193   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","E20194   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","E20195   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","E20196   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","\n","           (2, 0)     (2, 1)     (2, 2)  \n","E0      20.197500  21.146140  22.664955  \n","E1       0.000000   0.000000   0.000000  \n","E2      13.616035  14.826426  14.228360  \n","E3       0.000000   0.000000  16.866513  \n","E4      16.070480  11.494637  11.993439  \n","...           ...        ...        ...  \n","E20192   0.000000   0.000000   0.000000  \n","E20193   0.000000   0.000000   0.000000  \n","E20194   0.000000   0.000000   0.000000  \n","E20195   0.000000   0.000000   0.000000  \n","E20196   0.000000   0.000000   0.000000  \n","\n","[20197 rows x 9 columns]\n","\n","\n"]}]},{"cell_type":"code","source":["#@title Probar Q-Learning Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","MostarDetalleJugada = False #@param {type:\"boolean\"}\n","\n","if ql_policy._QtableEntrenada:\n","  res = SimularEntorno(eval2P_env, [ql_policy, random_policy], [\"Q-Learning\", \"AzarPolicy\"], \"Probando el Agente Q-Learning entrenado contra azar\", cantidad_probar, MostarDetalleJugada)\n"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"T5Am3MWiJI-a","executionInfo":{"status":"ok","timestamp":1659100415477,"user_tz":180,"elapsed":1014,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"7dda701c-8691-4e4c-e275-db180fe80bf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","**  Probando el Agente Q-Learning entrenado contra azar  **\n","    (Q-Learning usa 'O', AzarPolicy usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |X O O |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |O X O |\n","      2 |O X O |\n","        --------\n","    -> EMPATAN O y X\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |X - O |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |- X O |\n","      2 |- X O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |- X O |\n","      1 |- - O |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 6: \n","         0 1 2 \n","        --------\n","      0 |- - - |\n","      1 |O O O |\n","      2 |- - X |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 7: \n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |- X O |\n","      2 |X X O |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 8: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |- X O |\n","      2 |X - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 9: \n","         0 1 2 \n","        --------\n","      0 |- O - |\n","      1 |- O O |\n","      2 |X O X |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 10: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |- X O |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","= Total Partidos Ganados: \n","\t Q-Learning con 'O': 8.5 *\n","\t AzarPolicy con 'X': 1.5 \n"]}]},{"cell_type":"markdown","source":["##DQN"],"metadata":{"id":"3iip7y2pJI-b"}},{"cell_type":"code","metadata":{"cellView":"form","id":"diEOEg3JaMHa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659103368499,"user_tz":180,"elapsed":281,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"100846d3-39af-41e7-e324-d5d4894af683"},"source":["#@title Definir el Agente tipo DQN\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","DQNpolicy = None\n","\n","if entrenar_DQN:\n","  tipo_agente = \"DQN\" #@param [\"DQN\", \"DQN Categorico (C51)\"]\n","  learning_rate = 1e-3  # @param {type:\"number\"}\n","  cant_neuronas_ocultas = \"100, 50, 10\" # @param {type:\"string\"}\n","  DQNCat_num_atoms = 51  # param {type:\"integer\"}\n","\n","  # controla cantidad de atoms para DQN Cat\n","  if DQNCat_num_atoms <= 1:\n","    DQNCat_num_atoms = 51\n","\n","  # Define cantidad de neuronas ocultas para RNA-Q\n","  hidden_layers = []\n","  for val in cant_neuronas_ocultas.split(','):\n","    if  int(val) < 1:\n","      hidden_layers.append( 10 )\n","    else:\n","      hidden_layers.append( int(val) )\n","  fc_layer_params = tuple(hidden_layers, )\n","\n","  if tipo_agente==\"DQN\":\n","\n","    #define las capas convolutional\n","    CNN_preprocessing_layers = None\n","\n","    # Define RNA-Q\n","    q_net = q_network.QNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.Variable(0)\n","\n","    # Define el agente de tipo Q\n","    ag = dqn_agent.DqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        q_network=q_net,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        train_step_counter=train_step_counter)\n","\n","    ag.initialize()\n","\n","    print(\"Agente DQN inicializado. \")\n","\n","  elif tipo_agente == \"DQN Categorico (C51)\":\n","    \n","    # Define RNA-Q Categórico\n","    categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        num_atoms=DQNCat_num_atoms,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.compat.v2.Variable(0)\n","    \n","    # parámetros especificos (por defecto)\n","    n_step_update = 2\n","    gamma = 0.99\n","\n","    # Define el agente de tipo Q Categórico\n","    ag = CategoricalDqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        categorical_q_network=categorical_q_net,\n","        optimizer=optimizer,\n","        n_step_update=n_step_update,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        gamma=gamma,\n","        train_step_counter=train_step_counter)\n","    \n","    ag.initialize()\n","    \n","    print(\"Agente DQN Categorico (C51) inicializado. \")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Agente DQN inicializado. \n"]}]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659103391073,"user_tz":180,"elapsed":22181,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"1166eaf7-7824-4fee-9234-7876a26102ef"},"source":["#@title Métricas para evaluación y Preparar datos para Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  # Definir Métricas para evaluación para Agente DQN\n","    \n","  # Se usa el promedio de la recompensa (la más común)\n","  # See also the metrics module for standard implementations of different metrics.\n","  # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n","\n","  def compute_avg_return(environment, policy, num_episodes=10):\n","    if num_episodes == 0:\n","      return 0.0 \n","    total_return = 0.0\n","    for _ in range(num_episodes):\n","\n","      time_step = environment.reset()\n","      episode_return = 0.0\n","\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = environment.step(action_step.action)\n","        episode_return += time_step.reward\n","      total_return += episode_return\n","\n","    avg_return = total_return / num_episodes\n","    return avg_return.numpy()[0]\n","\n","  initial_collect_steps = 5000  # @param {type:\"integer\"} \n","  collect_steps_per_iteration =   50# @param {type:\"integer\"}\n","  replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","  batch_size = 64  # @param {type:\"integer\"}\n","\n","  # Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\n","  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","      data_spec = ag.collect_data_spec,\n","      batch_size = train_env.batch_size,\n","      max_length = replay_buffer_max_length)\n","\n","  # Recolecta datos generados al azar\n","  # This loop is so common in RL, that we provide standard implementations. \n","  # For more details see the drivers module.\n","  # https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n","\n","  def collect_step(environment, policy, buffer):\n","    time_step = environment.current_time_step()\n","    action_step = policy.action(time_step)\n","    next_time_step = environment.step(action_step.action)\n","    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","    # Add trajectory to the replay buffer\n","    buffer.add_batch(traj)\n","\n","  def collect_data(env, policy, buffer, steps):\n","    for _ in range(steps):\n","      collect_step(env, policy, buffer)\n","\n","  collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n","\n","  print(\"\\nDatos recolectados.\")\n","\n","  # Muestra ejemplo de los datos recolectados\n","  ##iter(replay_buffer.as_dataset()).next()\n","\n","  # Preparar los datos recolectados con trajectories de shape [Bx2x...]\n","  dataset = replay_buffer.as_dataset(\n","      num_parallel_calls=3, \n","      sample_batch_size=batch_size, \n","      num_steps=2).prefetch(3)\n","  iterator = iter(dataset)\n","  # Muestra ejemplo \n","  ##iterator.next()\n","  print(\"\\nDataset creado.\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Datos recolectados.\n","\n","Dataset creado.\n"]}]},{"cell_type":"code","source":["#@title Entrenar al Agente DQN\n","\n","if entrenar_DQN:\n","\n","  cant_ciclos_entrenamiento_finalizar =  15000# @param {type:\"integer\"}\n","  log_cada_ciclos = 500  # @param {type:\"integer\"}\n","  mostar_recompensa_cada = 1000  # @param {type:\"integer\"}\n","  cant_episodios_evaluacion =  10# @param {type:\"integer\"}\n","  minima_recompensa_promedio_finalizar = 100.0 # @param {type:\"number\"}\n","  \n","  #  Optimize by wrapping some of the code in a graph using TF function (Optional)\n","  ag.train = common.function(ag.train)\n","\n","  # Reset the train step\n","  ag.train_step_counter.assign(0)\n","\n","  # Evaluate the agent's policy once before training.\n","  avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","  ar_cicloL = []\n","  ar_cicloR = []\n","  ar_returns = []\n","  ar_loss = []\n","\n","  print(\"\\n** Comienza el Entrenamiento **\\n\")\n","  for _ in range(cant_ciclos_entrenamiento_finalizar):\n","\n","    # Collect a few steps using collect_policy and save to the replay buffer.\n","    collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\n","\n","    # Sample a batch of data from the buffer and update the agent's network.\n","    experience, unused_info = next(iterator)\n","    train_loss = ag.train(experience).loss\n","\n","    step = ag.train_step_counter.numpy()\n","\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\n","      print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \n","      ar_cicloL.append( step )\n","      ar_loss.append( train_loss )\n","    \n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\n","      avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","      ar_cicloR.append( step )\n","      ar_returns.append( avg_return )\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\n","\n","      if (avg_return >= minima_recompensa_promedio_finalizar):\n","        print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \n","        break\n","\n","  DQNpolicy = ag.policy\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"metadata":{"cellView":"form","id":"LQbSlCJW8BeN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659112313168,"user_tz":180,"elapsed":8922122,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"e2479e50-5895-4ec8-bed5-d0f0b34d584f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","** Comienza el Entrenamiento **\n","\n","step = 1: loss = 1733.570\n","step = 1: Promedio Recompensa = -69.0\n","step = 500: loss = 220.703\n","step = 1000: loss = 67.352\n","step = 1000: Promedio Recompensa = -108.0\n","step = 1500: loss = 70.826\n","step = 2000: loss = 38.904\n","step = 2000: Promedio Recompensa = -103.0\n","step = 2500: loss = 51.564\n","step = 3000: loss = 562.388\n","step = 3000: Promedio Recompensa = -97.0\n","step = 3500: loss = 47.422\n","step = 4000: loss = 26.785\n","step = 4000: Promedio Recompensa = -85.0\n","step = 4500: loss = 70.664\n","step = 5000: loss = 443.868\n","step = 5000: Promedio Recompensa = -27.0\n","step = 5500: loss = 487.200\n","step = 6000: loss = 881.811\n","step = 6000: Promedio Recompensa = 19.0\n","step = 6500: loss = 1479.381\n","step = 7000: loss = 647.187\n","step = 7000: Promedio Recompensa = 18.0\n","step = 7500: loss = 1708.342\n","step = 8000: loss = 2054.122\n","step = 8000: Promedio Recompensa = 21.0\n","step = 8500: loss = 1567.402\n","step = 9000: loss = 1013.415\n","step = 9000: Promedio Recompensa = 42.0\n","step = 9500: loss = 1316.117\n","step = 10000: loss = 1448.053\n","step = 10000: Promedio Recompensa = 37.0\n","step = 10500: loss = 658.013\n","step = 11000: loss = 1514.938\n","step = 11000: Promedio Recompensa = 69.0\n","step = 11500: loss = 584.148\n","step = 12000: loss = 1073.952\n","step = 12000: Promedio Recompensa = 40.0\n","step = 12500: loss = 995.075\n","step = 13000: loss = 1019.630\n","step = 13000: Promedio Recompensa = 77.0\n","step = 13500: loss = 1408.738\n","step = 14000: loss = 2579.927\n","step = 14000: Promedio Recompensa = 130.0\n","** Finaliza en step 14000 por buen valor de recompensa promedio: 130.0\n","\n","** Entrenamiento Finalizado **\n","\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"9EBBl7mRkQYa","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1659112313888,"user_tz":180,"elapsed":752,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"58d3e092-382f-4887-d6b7-a77660c8dcdd"},"source":["#@title Mostrar Gráficos del Entrenamiento del Agente DQN\n","\n","\n","if entrenar_DQN:\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloR, ar_returns)\n","  plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloR))   \n","  plt.grid(True)\n","  plt.show()\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloL, ar_loss, color=\"red\" )\n","  plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloL))   \n","  plt.grid(True)\n","  plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAukAAAFNCAYAAAC9ofFuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU5fXH8c9hExRQFERAAUVIROu+r0RFgwaXn61rXarW1t3WXdtq3fcVl4p767607gJa41JBTVQQUQRXQBBFUJAdnt8f544MMQmTZO7cmcn3/XrNa5I7M/ee3JkkZ545z3kshICIiIiIiOSPFkkHICIiIiIiy1OSLiIiIiKSZ5Ski4iIiIjkGSXpIiIiIiJ5Rkm6iIiIiEieUZIuIiIiIpJnlKSL5ICZVZrZsVnc3xdmtnu29lfHMTKO2cyCma0fZzyFwMw+NLMBScdRk5kdZWZvZHjfe83skrhjkrpl++9FMTKzncxsfNJxiMRJSbo0O1GCO8/M5pjZtCgpaZ/D42ecMBWDKOGYH53v1OWZDB9bUAljCGHDEEJlU/djZhea2b+yEFIszGxA9Mbs7BweM9ZzEv1Mk+Pafy6YWe/oeZlT43JQho8vmDfbIYTXQwgl2dhXLgY9RBpDSbo0V4NDCO2BTYHNgHMTjqfYnRRCaJ92GZyNnZpZq2zsRxrsSOB74IikA5FarVbj9+2RbOxUv28iuaUkXZq1EMI0YBierANgZtua2ZtmNsvMRqeXL0Sj4J+Z2Wwz+9zMDou2LzfKlzaitdw/NTPbALgd2C4a4ZoVbd/bzN4zsx/NbJKZXVjjcYeb2ZdmNsPMzq9x20pmdoOZfR1dbjCzlaLbOpvZs9HP8r2ZvW5mtf7em9lAM/vYzH4wsyGA1bj9aDP7yMxmmtkwM+uV8YmuQ2r00sxON7PpZjbVzH4X3XYccBhwVvroezTqdbaZjQF+MrNWK3jOKs3sYjP7X/S8DTezzmm3PxZ9ovKDmb1mZhum3Xavmd1qZi9EMfzPzNaKzvHM6Hxtlnb/n0fkzKyFmZ1jZp9Gz9ujZrZ6dFvq9XGkmX1lZt+lnlczKwfOAw6Kjjk62t7dzJ6OnseJZvb7es7rGtF9fzSzt4E+NW4vNbMR0b7Gm9mBDXjOVgF+DZwI9DWzLWvcfkTaa/WvMZ+TVc3sruh1M8XMLjGzlpn+LA34mTeIXkezzEua9km7bS8zGxe9tqaY2RnR9rz73Ytez7eY2XNRvG+ZWZ/otteiu42OzvFBtuz382wzmwbc09jnMLp9azMbGZ2TqWY2xMzapN0ezOwEM5sQxXexmfUx/93+MTpWm+i+y33yEf1+PGFm35r/bT4l7bYLo8feH+33w9Tr1sz+CfQEnol+7rOi7ftE95sVPfcbNOacizRJCEEXXZrVBfgC2D36em3gA+DG6PsewAxgL/xN7MDo+y7AKsCPQEl0327AhtHXFwL/SjtGbyAAraLvK4Fjo6+PAt6oEdMA4FfRMTcGvgH2i27rD8wBdgZWAq4DFqf9DBcBo4A1ozjfBC6Obrscf1PQOrrsBFgt56QzMBtPvloDf4qOkYp5X2AisAHQCvgL8Gba4wOwfh3n++efvZbbBkTHuSg67l7AXKBTdPu9wCW1PH/vA+sA7ep7ztKO/ynQL7p/JXBF2v6OBjpE5/YG4P202+4FvgO2ANoC/wU+x0eQWwKXAK/U8do6NXpe1o72/Q/goRqvj6FRTJsAC4ANans9RdteA26N4tgU+BbYtY7z+jDwKP6a3QiYQvSai7ZNAn4XPZebRT9j/7rOeY19Hw5MjX7+Z4Cb025LvVZ3BNoA1wCLYjwn/472sQr++n8b+EMj/y4MACbXsr01/to/L/qZdsV/V1J/B6YCO0VfdwI2z+XvXo19pc5hqzpuvxf/3dg62tcDwMN1/R6z7Pfzyuj5atfE53ALYNvo2L2Bj4DTahz/KaAjsGH02JeB9YBVgXHAkTWfL/z3vhr4W/QcrQd8BuyZ9tqZj/+NaBk9N6Nq+72Nvu8H/IT/LWkNnBU9B20a89rSRZfGXhIPQBddcn2J/iDPif4xhuifwGrRbWcD/6xx/2H4x/urALOAA4B2Ne5zIU1I0muJ8Qbg+ujrv9X4R7oKsJBlic+nwF5pt+8JfBF9fVH0T6/WBDrtMUfU+KdlwOS0mF8Ajkm7vQWeTPeKvl9Rkj43OnepS+pNxABgHmlJBTAd2Db6+l5qT9KPTvu+zucs7fh/SbvtBODFOmJdLfpZVk07/tC0208GPkr7/lfArBqxpZ6Xj4Dd0m7rhiesqQQlAGun3f42cHAdr6d1gCVAh7RtlwP31vIztIyOU5q27TKWJekHAa/XeMw/gAvqOuc17vsScEP09SH4m4XWaa/Vh9LuuzLLv1azeU664klcu7Rth5D2pqmBfxcGUHuSvhMwDWiRtu0h4MLo66+APwAdazwuJ797NfaVOoezalxSSfK9wJ1p998L+Djt+9qS9IVA27RtjX4Oa4n3NODfNY6/Q9r31cDZad9fm/ba+/n5ArYBvqqx73OBe9JeOy+l3dYfmFfb7230/V+BR2uc8ynAgMa8tnTRpbEXlbtIc7VfCKED/oe+FB/NAugF/Cb6iHOWeTnKjkC3EMJPeILzR2Bq9JFxaTaCMbNtzOyV6KPaH6JjpGLqjo98AhDFMSPt4d2BL9O+/zLaBnA1PgI03LxM55w6Qqh5jJD+PX5ebkw7J9/jyUSPDH/EU0IIq6Vd/pp224wQwuK07+cCK5rIWzO2Wp+ztPtMq23/ZtbSzK6IPrr/Ef9nDcvOPfinGinzavm+rlh7Af9Oi+kjPNHuuqK4atEd+D6EMDtt25fUfv674AnTpBr3TY9rmxrn6zBgrTqO/TMzWwcow0dgwZPQtsDeaXGmv47msvxrNZvnpBc+yjk1bX//wEfUa4s9fSJlzxX9rGm6A5NCCEvTtqWf+wPwZPdLM3vVzLaLtif5u9e5xu/bR2m3ZXp+U74NIcyvEU+jnkMz6xeVAE2Lft8uY/nfNWjc71svoHuN1/R5K4iprdVdY7/c39TouZ9E5n/vRLJCSbo0ayGEV/HRpWuiTZPwUdn0f3CrhBCuiO4/LIQwEE8AP8Y/1gX/aHTltF3Xl/CEWrY9CDwNrBNCWBX/mDxVlzoVH0kFwMxWBtZIe+zX+D+plJ7RNkIIs0MIp4cQ1gP2Af5sZrvVcvyax7D07/Hz8oca56VdCOHNen7ObKjtXNXcXu9ztgKH4uUEu+Mfp/eOtltdD2iAScCgGnG1DSFMyeCxNX/ur4HVzaxD2rae+OheTd/iJQrr1Lhvelyv1oirfQjh+AziOhz/v/FMVKP8GZ6kHxndPhUvgwDAzNqx/Gs1m+dkEj6Snp6QdgwhbFjLYwnLT6T8KoPjpXwNrGPL15P/fO5DCO+EEPbF3xz8By8zKobfvZTazntjn8Pb8L+bfUMIHfFEOlu/a5/XiKlDCGGvDB9f2+/bz39T056TTH5GkaxRki7ipSUDzWwT4F/AYDPbMxplbRtNUFrbzLqa2b7mE+cW4CUzqdG194Gdzaynma1K/d1ivgHWTp8whddEfx9CmG9mW+PJY8rjQIWZ7Rg95iKW/919CPiLmXUxnxD5t+jnwMwqzGz96J/MD/iIV/qIYMpzwIZm9n/R6NIpLP9G43bgXIsmVZpP2PtNPT9jtnyD15fWp87nLIP9d8Cfyxn4m6zLmhbucm4HLrVokl/0/Oyb4WO/AXqnEsMQwiR8rsHl0c+3MXAM0fOcLoSwBHgSuNDMVjaz/ixLogGeBfqZT0ZuHV22ynBi3JHA3/Ga+NTlAGAvM1sDf60ONrPto9fqhSyfhGXznEwFhgPXmllH8wmNfcxslwz3V6vo/P58wcs15uITmFubT0oeDDxsZm3M7DAzWzWEsAifs7I02k8h/u5l8vvWlOewA36O5ph/CpnJG8NMvA3MNp/g2i76O7CRmW2V4eNr/tyPAnub2W5m1ho4Hf87kas3RiKAknQRQgjfAvcDf4uSoX3xEZ5v8RGaM/HflRbAn/FRlu+BXYj+yYQQRgCPAGPwOspn6znkf4EPgWlm9l207QTgIjObjSfZj6bF9yHeSeNBfNRtJl6zmnIJUBUd+wPg3WgbQF+8hngOMBK4NYTwSi3n4DvgN8AVeMLaF/hf2u3/xiePPRx9TD0WGFTPz1jTkBrlBtUZPu4uoH/0EfZ/arvDCp6zFbkf/1h7Cj4pbVSGcWXiRvzTkeHR8zoKr53NxGPR9Qwzezf6+hB8pP9rfMLkBSGEl+p4/El4WcA0/JOie1I3RCUzewAHR/uaxrKJgXUys23x0cVbQgjT0i5P42Udh0Sv1ZPxiatT8dfddDzBgeyfkyPwiYLj8N+Lx1m+zKmheuAlFemXdfCkfBA+wfZW4IgQwsfRYw4Hvoh+L/6Ilw5Bsr97s2r8vv05w5//QuC+6Petro4/TXkOz8AHIGbjn0JmpTVk9Ma0An/T+Dn+PN2JfzqWicvxgY5ZZnZGCGE88Fvg5mhfg/G2vQuzEa9IpszL30RERLLLfJGwWXh5w+dJxyMiUkg0ki4iIlljZoOjMptV8LkeH7BsQq6IiGRISbqIiGTTvngZzdd46cbBQR/Ziog0mMpdRERERETyjEbSRURERETyjJJ0EREREZE8U9dqWwWtc+fOoXfv3okd/6effmKVVVZJ7PjNic51bul8547Ode7oXOeOznXu6FznTnV19XchhC7Z3m9RJum9e/emqqoqseNXVlYyYMCAxI7fnOhc55bOd+7oXOeOznXu6Fznjs517pjZl3HsV+UuIiIiIiJ5Rkm6iIiIiEieUZIuIiIiIpJnlKSLiIiIiOQZJekiIiIiInlGSbqIiIiISJ5Rki4iIiIikmeUpIuIiIiI5Bkl6SIiIiIieUZJuoiIiIhIY7zwQmy7VpIuIiIiItIYV14Z266VpIuIiIiINNTSpfDuu7HtXkm6iIiIiEhDTZgAs2fHtnsl6SIiIiIiDVVdHevuY0vSzWwdM3vFzMaZ2Ydmdmq0/UIzm2Jm70eXvdIec66ZTTSz8Wa2Z9r28mjbRDM7J66YRUREREQyUl0NbdvGtvtWse0ZFgOnhxDeNbMOQLWZjYhuuz6EcE36nc2sP3AwsCHQHXjJzPpFN98CDAQmA++Y2dMhhHExxi4iIiIiUreqKthkE3jrrVh2H9tIeghhagjh3ejr2cBHQI96HrIv8HAIYUEI4XNgIrB1dJkYQvgshLAQeDi6r4iIiIhI7qUmjW65ZWyHyElNupn1BjYDUm81TjKzMWZ2t5l1irb1ACalPWxytK2u7SIiIiIiuffJJzBnDmyxRWyHiLPcBQAzaw88AZwWQvjRzG4DLgZCdH0tcHQWjnMccBxA165dqaysbOouG23OnDmJHr850bnOLZ3v3NG5zh2d69zRuc4dnet4rTliBP2Bd0KI7RixJulm1hpP0B8IITwJEEL4Ju32ocCz0bdTgHXSHr52tI16tv8shHAHcAfAlltuGQYMGJCdH6IRKisrSfL4zYnOdW7pfOeOznXu6Fznjs517uhcx+ypp6BdO7Y64gg45phYDhFndxcD7gI+CiFcl7a9W9rd9gfGRl8/DRxsZiuZ2bpAX+Bt4B2gr5mta2Zt8MmlT8cVt4iIiIhIvaqrYdNNoVV8491xjqTvABwOfGBm70fbzgMOMbNN8XKXL4A/AIQQPjSzR4FxeGeYE0MISwDM7CRgGNASuDuE8GGMcYuIiIiI1G7JEnjvPTjqqFgPE1uSHkJ4A7Babnq+nsdcClxay/bn63uciIiIiEhO5GDSKGjFURERERGRzKVWGo2x/SIoSRcRERERyVxVFbRrB6WlsR5GSbqIiIiISKaqq2GzzWKdNApK0kVEREREMrNkia80GnM9OihJFxERERHJzPjxMHdu7PXooCRdRERERCQzVVV+rZF0EREREZE8UV0NK68c+6RRUJIuIiIiIpKZ1KTRli1jP5SSdBERERGRFUmtNJqDUhdQki4iIiIismIff5yzSaOgJF1EREREZMVyOGkUlKSLiIiIiKxYdTWssgqUlOTkcErSRURERERWpKoqZ5NGQUm6iIiIiEj9Fi+G99/PWT06KEkXEREREanfRx/BvHk5q0cHJekiIiIiIvWrrvZrjaSLiIiIiOSJ6mpo3x769cvZIZWki4iIiIjUJzVptEXuUmcl6SIiIiIidUlg0igoSRcRERERqdu4cTB/fk4njYKSdBERERGRuiUwaRSUpIuIiIiI1K2qCjp0gL59c3pYJekiIiIiInWprobNN8/ppFFQki4iIiIiUrtFi3zSaI7r0UFJuoiIiIhI7caNgwULcl6PDkrSRURERERql5o0qpF0EREREZE8kZo0uv76OT+0knQRERERkdpUV/soeo4njYKSdBERERGRX1q0CEaPTqTUBZSki4iIiIj80ocfJjZpFJSki4iIiIj8UlWVX2skXUREREQkT1RXw6qrQp8+iRxeSbqIiIiISE1VVYmsNJqiJF1EREREJN3ChTBmTGL16KAkXURERERkeR9+6Il6QvXooCRdRERERGR5CU8aBSXpIiIiIiLLS3jSKECrxI4sIiKSpBDg0Udp07p10pGISL6pqvJRdLPEQtBIuoiINE8PPAAHH0zfG25IOhIRyScLFiQ+aRSUpIuISHM0eTKcdBK0a0eX11+HsWOTjkhE8sXYsbBoUaL16BBjkm5m65jZK2Y2zsw+NLNTo+2rm9kIM5sQXXeKtpuZ3WRmE81sjJltnravI6P7TzCzI+OKWUREmoEQ4Nhj/Z9wZSWL27WDyy5LOioRyRfV1X5dxCPpi4HTQwj9gW2BE82sP3AO8HIIoS/wcvQ9wCCgb3Q5DrgNPKkHLgC2AbYGLkgl9iIiIg12xx0wbBhcfTVsvTVf77MPPPIITJiQdGQikg+qqqBTJ1h33UTDiC1JDyFMDSG8G309G/gI6AHsC9wX3e0+YL/o632B+4MbBaxmZt2APYERIYTvQwgzgRFAeVxxi4hIEfvsMzj9dBg4EI4/HoDJBx4IbdrAFVckHJyI5IXq6sQnjUKOuruYWW9gM+AtoGsIYWp00zSga/R1D2BS2sMmR9vq2l7zGMfhI/B07dqVysrKrMXfUHPmzEn0+M2JznVu6Xznjs51DJYuZdM//Yn2wDvHHsuCV18FYE6bNkweNIju993HWwMHsmCttZKNs4jpdZ07OteNYwsXstOYMUz+zW/4LOHzF3uSbmbtgSeA00IIP1rau5IQQjCzkI3jhBDuAO4A2HLLLcOAAQOysdtGqaysJMnjNyc617ml8507OtcxuO4679hw771sd+CBP2+urKxk7RtvhGefZbvXX4dbbkkwyOKm13Xu6Fw3UlUVLF5Mz/33p2fC5y/W7i5m1hpP0B8IITwZbf4mKmMhup4ebZ8CrJP28LWjbXVtFxERycxHH8F558E++8ARR/zy9nXWgaOOgrvugqlTf3m7iDQPeTJpFOLt7mLAXcBHIYTr0m56Gkh1aDkSeCpt+xFRl5dtgR+isphhwB5m1imaMLpHtE1ERGTFFi/2xLx9e580Wled6dlne8eXa6/NbXwikj9Sk0Z79046klhH0ncADgd2NbP3o8tewBXAQDObAOwefQ/wPPAZMBEYCpwAEEL4HrgYeCe6XBRtExERWbErrvB/vLffDl271n2/Pn3g0EPhttvgu+9yF5+I5I/qah9FT3jSKMRYkx5CeAOo6yfcrZb7B+DEOvZ1N3B39qITEZFm4f334e9/h0MOgV//esX3P/dcX4n0hhvgkkvij09E8sf8+fDBB3DGGUlHAmjFURERKVYLFsDhh0OXLjBkSGaP6d8fDjgAbr4ZZs2KNz4RyS8ffODlcXlQjw5K0kVEpFhdeKEv733nnbD66pk/7vzz4ccfM0/sRaQ4pCaNbrFFsnFElKSLiEjxGTkSrroKjj0W9tqrYY/ddFPYe28veZkzJ574RCT/VFXBGmtAr15JRwIoSRcRkWIzdy4ceaS3VWxsp5bzz4cZM+Af/8hubCKSv/JkpdEUJekiIlJczjkHJkyAe+6Bjh0bt4/ttoPddoNrroF587Ibn4jkn/nzvTwuT0pdQEm6iIgUk//+1yd9nnoqlJU1bV9/+QtMmwZ3q7mYSNEbMyavJo2CknQRESkWP/4Iv/sd9OsHl13W9P3tsgtsvz1ceSUsXNj0/YlI/qqq8muNpIuIiGTZn/8MkyfDfffByis3fX9mPpo+aRL8619N35+I5K/qaujcGXr2TDqSnylJFxGRwvfcc3DXXXD22bDtttnbb3k5bL45XH65fxQuIsWpqiqvJo2CknQRESl0M2Z4q8WNN4YLLsjuvlOj6RMnwqOPZnffIpIf5s2DDz/Mq3p0UJIuIiKF7qSTPFG//35YaaXs73/ffWHDDeHSS2Hp0uzvX0SSNWYMLFmSV/XooCRdREQK2aOPwsMP+wj6JpvEc4wWLeC882DcOHjqqXiOISLJSU0a1Ui6iIhIFkybBiecAFtv7bXocTrwQFh/fbjkEggh3mOJSG5VV0OXLrD22klHshwl6SIiUnhCgN//Hn76ybu5tGoV7/FatYJzz4V334UXX4z3WCKSW3k4aRSUpIuISCG67z549lnvulJamptj/va33p7t4os1mi5SLObO9VK2PCt1ASXpIiJSaL76ylcU3WUXOOWU3B23TRsvqxk5Eiorc3dcEYnP6NF5OWkUlKSLiEghWboUjj7ar++5xyd15tLRR8Naa3mnFxEpfNXVfq2RdBERkSa47TZ4+WW47jpYd93cH79tWzjjDI9h5MjcH19EsquqCtZcE3r0SDqSX1CSLiIihWHCBDjrLF8F9Nhjk4vjD3+ANdbQaLpIMaiu9lH0PJs0CkrSs6+qinXvvNPrm0REJDuWLIGjjvK68DvvTPYfavv28Kc/wXPPwXvvJReHiDRNatJoHtajg5L07Bs9ml4PPOATm0REJDuuvRbefBOGDMmPj6VPPBE6dtRoukghe/99n9+Sh/XooCQ9+0pK/Prjj5ONQ0SkWIwdC3/9KxxwABx6aNLRuNVWg5NPhief9JE4ESk8qUmjGklvJlL9epWki4hkx2mnwaqr+qTRfKobPe00aNfOe7WLSOGpqoKuXaF796QjqZWS9Gzr3JlFHTvC+PFJRyIiUvgWLIDXX/d69C5dko5meZ07w/HHw4MPwqefJh2NiDRUHk8aBSXpsZjbs6dG0kVEsuHdd2HhQthuu6Qjqd3pp0Pr1nDFFUlHIiIN8dNP8NFHeVvqAkrSYzF3nXWUpIuIZEOqF/m22yYbR126dYNjjoH77oNJk5KORkQyleeTRkFJeizm9uwJ33wDs2YlHYqISGEbORJ69fJkOF+ddRaEAFdfnXQkIpKpqiq/1kh68zK3Z0//QnXpIiJNM2pU/pa6pPTqBUccAUOHwrRpSUcjIpmorvY3/3k6aRSUpMfi5yRdJS8iIo03ebJf8j1JBzjnHK+dv+66pCMRkUxUV+f1KDooSY/F/G7dfCKRknQRkcZL1aMXQpLety8cfDDceivMmJF0NCJSnzlzfNJoHtejg5L0WISWLWH99ZWki4g0xciR0LYtbLJJ0pFk5txzvWPETTclHYmI1Of9930eiUbSm6nSUtWki4g0xahR/k+0TZukI8nMRhvB/vt7kv7DD0lHIyJ1KYBJo6AkPT6lpTBxIixalHQkIiKFZ8ECrxkthFKXdOef7529br016UhEpC7V1T5hNJ+7RqEkPT4lJZ6gf/550pGIiBSe997L70WM6rLFFjBokE8g/emnpKMRkdpUVeX9KDooSY9Paalfqy5dRKTh8n0Ro/qcfz589x3ccUfSkYhITbNnezlynk8aBSXp8Skp8WvVpYuINNzIkdCzZ173MK7TDjvAgAFwzTUwf37S0YhIuvfeK4hJo6AkPT6rrQZrraWRdBGRxiiERYzqc/758PXX8OCDSUciIumqq/1aSXozV1KiJF1EpKGmTIFJkwo7Sd9tN+/2MmSIj9qJSH6oroYePXwgNc8pSY9TaamSdBGRhiqkRYzqYgYnneQfrb/5ZtLRiEhKVVVB1KNDjEm6md1tZtPNbGzatgvNbIqZvR9d9kq77Vwzm2hm481sz7Tt5dG2iWZ2TlzxxqK0FL7/3icQiYhIZkaOhJVWgk03TTqSpjnsMFh1Vbj55qQjERGAH3+ETz4piFIXiHck/V6gvJbt14cQNo0uzwOYWX/gYGDD6DG3mllLM2sJ3AIMAvoDh0T3LQzq8CIi0nCFtohRXdq3h6OPhiee8Pp0KTyLF8M77yQdhWRLAU0ahRiT9BDCa8D3Gd59X+DhEMKCEMLnwERg6+gyMYTwWQhhIfBwdN/CkOrwoiRdRCQzCxcW5iJGdTnxRFiyBP7xj6Qjkca4+27Yemv48MOkI5FsKKBJo5BMTfpJZjYmKofpFG3rAUxKu8/kaFtd2wtDz57Qtq2SdBGRTL33nq82WixJep8+sNdenqQvXJh0NNJQzz/v1y+9lGwckh1VVbD22tC1a9KRZKRVjo93G3AxEKLra4Gjs7FjMzsOOA6ga9euVFZWZmO3jTJnzpyfj79l9+7Mf/NNxiYYTzFLP9cSP53v3Gmu53rtxx9nfeDNEFiYo58/7nPdaeed2eS55xj3978zfeDA2I5TCArpdW2LF7PDiBG0Ar59/HE+3GSTpENqkEI617my9Rtv8FPv3nxYKOclhBDbBegNjF3RbcC5wLlptw0Dtosuw9K2L3e/ui5bbLFFSNIrr7yy7JsDDwxh/fUTi6XYLXeuJXY637kTy7l+++0Qtt8+hJkzs7/vbDnwwBDWWSenh4z9db1kSQh9+4aw7bbxHqcAFNTfkNdeCwFC6N49hE6d/HksIAV1rnNh1ix/Pi++OOu7BqpCDHl0TstdzKxb2rf7A6nOL08DB5vZSma2LtAXeBt4B+hrZuuaWRt8cunTuYy5yUpK4LPP/ONbEZEkXXWVtwNMfYSfjwp9EaPatGjh7RhHjfKP26UwDB8OLVvCeefBzJkwenTSEUlTvPeeXxdI+0WItwXjQ8BIoMTMJpvZMcBVZr41OnoAACAASURBVPaBmY0ByoA/AYQQPgQeBcYBLwInhhCWhBAWAyfhI+sfAY9G9y0cpaWwdClMnJh0JCLSnE2fDk895V+/+GKysdTl66/hq6+KL0kHOOoo7/aidoyFY/hw2HZb2DfqV1EoJRJSuwKbNArxdnc5JITQLYTQOoSwdgjhrhDC4SGEX4UQNg4h7BNCmJp2/0tDCH1CCCUhhBfStj8fQugX3XZpXPHGJtWGcfz4ZOMQkebt/vth0SIfRRo2zAcP8k0xLGJUl44d4cgj4eGH/Q1TczNyJOy1F2sWygTMGTO89eIee/hEw/XXh1deSToqaYqqKm/o0aVL0pFkTCuOxq1fP79WhxcRSUoIcOedsP32cOqpniSmPvrNJyNHem/0Ql/EqC4nnugdXoYOTTqS3Pn8czjoIH/tvfACaz/+eNIRZebll/33Zo89/PuyMnjtNW+nKYWpurqgRtFBSXr82rf3d+FK0kUkKa+/7p/m/f73y5KOfCx5SS1itNJKSUcSjw02gN13h9tu80VyitmsWXDWWf5p8jPPwAUXwPnn03H8eJg6dcWPT9rw4bDaasvql8vK4Icf8vPNrazYDz/AhAlK0qUWpaVK0kUkOUOHernFb34Da67piccLL6z4cbm0cKF/HF2MpS7pTj4ZpkyB//wn6UjisWgR3HIL9O0L11wDhx7qydGFF/qIOuT3xGXwEfRhw/wNVauoU/WAAX6tuvTC9O67fl1Ak0ZhBUm6mbUws+1zFUzRKi31USxvIykikjszZ8Ljj8Nhh8Eqq/i28nIvLZk5M9nY0r3/fnEtYlSXvfeG3r2LbwJpCPDss7Dxxt7J5le/8vKCe+6BHtEahBttxPyuXX1kPZ99/DFMnrzsUyeAbt28W5vq0gvTiBF+XUwj6SGEpcAtOYqleJWWwo8/wrRpSUciIs3NAw/A/Ple6pJSXu4TR19+Obm4airmSaPpWraEE07w+uYxY5KOJjtGj4aBA2HwYH9dPfWUv7Y222z5+5kxY7vtPGGaPz+ZWDMxfLhfpyfp4CUvr79e/KVKxebzz+H66+HXv4bOnZOOpkEyKXd52cwOMDOLPZpiVVLi1yp5EZFcCsFLXbbYYvmEaZttvN42n0peRo70+TupUddidswx0K4dDBmSdCRN8/XX/rNstpnXat90E4wdC/vsA3WkDDO22w7mzs3vspFhw/z/dq9ey28vK4PZs5e18pPCcOqp/ub4+uuTjqTBMknS/wA8Biw0sx/NbLaZ/RhzXMUl1YZRSbqI5NI77/ho7bHHLr+9VSsf+XzxxfwpwyvGRYzqsvrqXn70r3/B998nHU3D/fQTXHSR153/85/w5z/7WiAnnwytW9f70Fmbbgorr5y/JS8LFvgbiJqj6AC77OLX+fwGQ5b3zDPLJi6vvXbS0TTYCpP0EEKHEEKLqN95x+j7jrkIrmj06OG1oOqVLiK5NHSoJ0SHHvrL2wYN8pHQDz7IfVw1TZ0KX37ZfJJ08IR23jy4++6kI8nc0qVw333eWviCC2CvveCjj3yCaKdOme2iTRt/g/jss/nzBjHd//7nz0ttSXrXrtC/v+rSC8XcuXDKKf6cnXZa0tE0SkbdXcxsHzO7JrpUxB1U0TFThxcRya3Zs+Ghh7yjRsdaxlX23NOv86EVY3OpR0+38caw887eCaUQem+/8op3xjjqKB+RfP11eOwx6NOn4fsaPNhXlh07NuthNtmwYf5pQKqbS01lZfDGG97FRvLb5ZfDF1/479gKPuHJVytM0s3sCuBUYFx0OdXMLo87sKJTUqIkXURy5+GHvSwhfcJouu7dPVHMlyS9TZtfTjQsdied5ElEPrckHD8e9t0Xdt3VV+F88EF/vnbcsfH73Gsvv3722ezEmE3Dh8MOO/gaJ7UpK/Pfq6qq3MYlDTNhAlx1lZeV1fWGqwBkMpK+FzAwhHB3COFuoBzYO96wilBpqY8czJ2bdCQi0hzceSdsuCFsu23d9xk0yEcFZ8/OXVy1GTUKNt+8eBcxqst++3k5ZD62Y5wxw0sFNtrIR9Evu8wHmg45BFo0cYmVbt18VD7f6tK/+cZbgdZW6pKSqktXyUv+CsHLydq29VKsApbpb9pqaV+vGkcgRa+01F84EyYkHYmIFLsxY+Dtt30Uvb7GXOXl/rH9f/+bu9hqai6LGNWmdWs4/nhvSZgvn7QuWADXXgvrr+9lAscc4/+3zj3XO9JkS0WFvzn79tvs7bOpXnrJr1OlYLXp3Nl7wCtJz19PPullSxdfDGutlXQ0TZJJkn458J6Z3Wtm9wHVwKXxhlWE1OFFRHJl6FAvH/ntb+u/3/bbQ4cOybZiHD3ae2Y3xyQd/I1Umzb50Y5x0SJ/43bGGf4JzJgxcPvtPmEy2wYP9oGrfGoDOmyYJ+Gbblr//crKfILpggW5iUsyN2eOTxLdZBNfj6DAZdLd5SFgW+BJ4AlguxDCI3EHVnTWX99HtJSki0ic5s3z1n4HHABrrFH/fdu0gd12S7YVY3OcNJpuzTV9cu999/mid0k691xvL3jnnZ48b7hhfMfabDOfF5EvdekheD36wIErLucZMMB/z955JyehSQNcfLGvFnvrrd5qtsDV+Uo0s81TF6AbMDm6dI+2SUO0a+dLQasNo4jE6fHHYdasuieM1lRe7u0Pk/rbNHKk12UXYA/jrDn5ZB8BvO++5GJ4/HEvcznxRC9xiZsZ7L23v0FcuDD+463IBx94TXp99egpu+zi8avkJb+MGwfXXQe/+51/SlgE6nu7eG09l8KuxE+K2jCKSNyGDvVP7jLtaFBe7tdJlR00p0WM6rLVVr4K7JAh3os81z7+2BObbbf1JCdXKip80vLrr+fumHUZNsyvM0nSV1/dyymUpOePELxbUocOcOWVSUeTNXUm6SGEsnouu+YyyKJRWuqjVUn8ERaR4jd+vCc8xx5b/4TRdL16wQYbJNOKcdo0b0HY3JN08NH0Tz7xSaS5NGcO/N//+ae9jz3mJVC5svvu3oEjH0pehg/3Tjbdu2d2/7IyePNNn08hyXv44WVdiLp0STqarMl0MaONzOxAMzsidYk7sKJUUuItGCdPTjoSESlGd97pdZhHHtmwxw0aBK++mvsWsc29Hj3db37jEzRzOYE0BH9DN368Jzm5LjlaeWXvv/7MM8muPjp3rr+5zWQUPWXAAJ84+tZbsYUlGfrxRzj9dG/rmWmZX4HIZDGjC4Cbo0sZcBWwT8xxFadUhxfVpYtIti1c6DXNgwc3vO1YebknHJWVsYRWp5EjvQ1hc1vEqDZt2sBxx8Fzz8Fnn+XmmDfdBI88Apde6slyEioq4NNPk/2/+Npr/vqvr/ViTTvv7BNMVfKSvAsu8E/lbr0VWrZMOpqsymQk/dfAbsC0EMLvgE1Qr/TGURtGEYnLU095z+nGjCTttJOPaua65CW1iFHbtrk9br764x89ybjllviP9b//eavFffeFs8+O/3h12TtaGzHJkpfhw30hrZ12yvwxq63mby6VpCdrzBhfDOy443xuR5HJJEmfH0JYCiw2s47AdGCdeMMqUmuu6b/YStJFJNuGDoWePRv2kX1K27ZeY5vLyaOLFjXfRYzq0r27t868+25fej4u06Z5eU3v3v7pS6bzF+LQs6dPwkw6Sd9554Yv1lRW5m80582LJy6p39Kl3gu9UyevRS9C9bVgvMXMdgTeNrPVgKH4QkbvAiNzFF9xMfO6dCXpIpJNn3/uEw6PPrrxH/eWl8PEiX7JhdGjPblRkr68k0/2FpoPPBDP/hct8r7ss2bBE0/AqnnwwXhFBbzxBsycmftjT54MH37YsFKXlAEDvMxspFKiRPzzn/6J0JVXesedIlTfSPonwNVABXAe8BYwEDgyKnuRxkh1eBERyZa77/b62KOPbvw+Bg3y61yVvGjSaO22395XvLz55ngmU557rtdg33EHbLxx9vffGBUVsGRJMh2GUt10GvMJ1E47+Ztilbzk3syZcOaZ/vfjqKOSjiY29bVgvDGEsB2wMzADuBt4EdjfzPrmKL7iU1oKU6Z4b1gRkaZavNiT9PJyWKcJlYh9+nh/9Vwm6d27N+9FjGpj5qPpY8d6x51sSi1YdMIJ8NvfZnffTbH11t42L4mSl+HDoVs3b7/YUB07whZbKElPwl/+AjNm+GTRFa0QW8BW+JOFEL4MIVwZQtgMOATYD1C9RmOpw4uIZNMLL8DXX3srvaYqL/eEIxe9n1OLGCVZD52vDjkE1ljDR9OzJbVg0Tbb5HbBoky0aOETSF94wd905srSpT6SvscejX8dlpXB22/HO4dAllddDbfd5osXbbpp0tHEKpMWjK3MbLCZPQC8AIwH/i/2yIpVSYlfqy5dRLJh6FDvr11R0fR9DRq0rGd0nL75xuvoVepSu3bt/E3Xf/4DX33V9P2lFixq29YXLFpppabvM9sqKryE4c03c3fMd9/10djGlLqkDBjgdf65jLs5S00WXXNNuOiipKOJXX0TRwea2d3AZOD3wHNAnxDCwSGEp3IVYNHp08dr2DSSLiJNNWWK99X+3e+833hTDRjgCVzcJS+qR1+x44/369tvb9p+ai5Y1JSSqDgNHOiv4VyWvAwf7te77974fey4oy8gppKX3LjzTv/k4ppr8mPSc8zqG0k/F3gT2CCEsE8I4cEQgj7Paao2bTxR10i6iDTVvff6yFI2Sl3Ae6Xvskv8rRhTixhtvnm8xylkvXrBPvv4JyVNKT9KLVh0ySWw227Ziy/bOnb0N4m5TtI328xHZRurfXvvz60kPX7ffecTn3fZBQ47LOlocqK+iaO7hhDuDCEk0BOpyJWWKkkXkaZZuhTuustXiuzTJ3v7LS+Hjz6CL7/M3j5rGjXKkyMtYlS/k0/2xOThhxv3+NSCRfvsk+yCRZmqqPDX3qefxn+s2bP9/DSm9WJNZWXwzjteViTxOfdc+PFHX+yrmcxlKd4psfmspAQmTPCWUyIijfHyy17X3ZgVRutTXu7Xw4Zld78pixZ5QqNSlxUrK4MNN2xcO8bUgkW9evmCRYXQASOXq49WVvok1abUo6cMGOD/z994o+n7ktqNGuWlLqed5r8TzUQB/NYWodJSWLAg3pEqESluQ4f6Ah777Zfd/ZaWemIXV8nLmDFaxChTZt7B4t13PUnJ1OLFcPDBvmDRk0/6SteFoE8f2GCD3CTpw4d7edf22zd9Xzvs4OVbKnmJx5IlPlm0Rw/429+SjianlKQnIdWGUSUvItIY337rnT+OOCL7JSNmPpr+8su+mmK2adJow/z2tz5BriHtGM8913us/+Mf+bNgUaYGD/bYf/wx3uMMG+afVGSj083KK3try8rKpu9Lfum22+C997x1aIcOSUeTU0rSk6A2jCLSFPfd52Uj2S51SRk0yGt242grN3KkLx6Tr11G8k379t6957HHYOrUFd//iSe888Xxx8Phh8cfX7ZVVPhrO9V5JQ6ff+4lp9kodUkZMMD7d8f95qK5+eYbX7ho9929fKuZUZKehDXWgM6dlaSLSMOF4LWZ228P/fvHc4xdd/W2cnG0YtQiRg134on+kf8dd9R/v/HjPaHfemu4/vrcxJZt220HnTrFW/IyYoRfZzNJLyvz5yjuNQaam7PO8rUbhgxpln8zlKQnpbRUvdJFpOHeeMP/dsQ1ig7+kfKOO2Y/SZ8+HT77TKUuDbX++v7pxu23112ClFqwaKWV4PHH83PBoky0auU/6/PPx9dcYdgw6Nlz2afa2bDddt5iWXXp2fP663D//XDmmdl9rgqIkvSkqA2jiDTG0KHeUzruj34HDYLRo+Hrr7O3T9WjN95JJ3nHliee+OVtIfibto8/hoceKvxSosGDfd7FO+9kf9+LF/t8iz32yO7IbLt2/rrOl7r0e+6h9733wocfJh1J4yxa5JNFe/WC889POprEKElPSkmJjyp9/33SkYhIoZg502uTDz0UVlkl3mPF0Ypx5EgfKdUiRg23557Qt2/tE0hvvtl7qV98cdNWz8wXe+7pK3M/80z29/3OO/DDD9ktdUkZMMAnOM6alf19N8SkSfCHP9D7vvtgo428ZeGFF8K4ccnG1RA33wxjx8KNN/rE3GZKSXpSUh1eVPIiIpl64AFffTLOUpeUX/0KunfPbivG1CJG7dplb5/NRYsWXps+cqRPUEz53//g9NN99Pmcc5KLL5s6dfJyqzjq0ocP93MZx+qrZWW+yNhrr2V/3w1xzTUQAtW33OIL/3TpAhdd5Mn6Rhv51/n8Sf6UKXDBBd43f599ko4mUUrSk6IkXUQaIgQvddl889yMRKdaMY4Y4SUCTbV4sRYxaqqjjvJPUIYM8e+/+QYOPNBLAu6/vzAWLMpURYX31P/qq+zud9gw2GorX2Mg27bd1luiJlmXPn26/5347W+Z3b+/l4xUVnriO2SIN6648ELvR7/xxv7pS77lIaef7uUuN97YLCeLpovtN9rM7jaz6WY2Nm3b6mY2wswmRNedou1mZjeZ2UQzG2Nmm6c95sjo/hPM7Mi44s253r19kkk+v5sVkfxRVeVJSy5G0VPKy/2j+7ffbvq+xozxLg1K0htv1VXhyCO97nzaNDjoIC+BeuKJwlmwKFODB/t1NkfTZ82Ct96Kp9QFfLLu9tsnW5d+/fX+aVvNT1W6dfNPYl59FSZPhptu8tfMBRf4oOEmm8Cll8InnyQTd8rLL8Mjj3iv/z59ko0lD8T5tvteoLzGtnOAl0MIfYGXo+8BBgF9o8txwG3gST1wAbANsDVwQSqxL3itWvmMfSXpIpKJoUO9NvPQQ3N3zIEDvTY4GyUvmjSaHSee6CtW77STJ1y33+4JVrHp18//R2YzSf/vf70cJa4kHbwuffToZOabzZzp5S2/+U393VC6d4eTT/aynEmTfMS6QwfvR15SAptuCpdd5r3k47Z0qY/+v/cePPecv7779IGzz47/2AUgtiQ9hPAaUPNVui9wX/T1fcB+advvD24UsJqZdQP2BEaEEL4PIcwERvDLxL9wqcOLiGRizhwfPT3oIO/skiurreYf4WejFePIkbDWWt76Thqvf3+vp5440RcsOuKIpCOKh5mXvPz3v/DTT9nZ57Bh/vuzzTbZ2V9tysq8NO3VV+M7Rl2GDPFFyM47L/PH9OgBp5zirV0nTfKR+FVW8Y4q/fp5ad3ll/vrraF++slH5isrfT7NVVfBaaf5m4jtt/cyrbZtoWtXP05FBXz6qb/RyPZKygWqVY6P1zWEkFoybRrQNfq6BzAp7X6To211bS8OpaXw9NNee9W6ddLRiEi+evhhT9SPPTb3xx40yEfYpk+HNdds/H60iFH2XHst/POfXp5QzCoq4IYbvASiqRMIQ/Akfddd4/1/u/XW/olXZSXsv398x6lpzhw/VxUVjf9kZe21PYk+7TRP2B9/HB591JP+887zRPrAA+GAA3zy99dfe637lCm1f/3DD788RocOPpLfowfsssuyr3v08K/XXdeTdgFyn6T/LIQQzCxka39mdhxeKkPXrl2pTLAmbM6cORkdv2sIbLB4MW899BDzNLrUKJmea8kOne/cST/Xm197LS179eKdBQtyXu/avnNntgQ+uvFGvhk4sFH7aD1zJjt8+imf7r47k/Lw9VOQr+uKimUlRAWkIefali5lh1VWYfodd/BJEz9Bajd5Mtt8+SWf7L8/X8f8XG/cvz9tnn2Wqhwm6Ws/+ijrf/897w4axI/Rz9fk1/Vmm8Fmm7HSN9/Q5dVXWbOyko7nnFNrF6HQogUL1liDhZ07s6BzZxaUlCz7unPnn79eUl87xfnz4aOP/CIuhBDbBegNjE37fjzQLfq6GzA++vofwCE17wccAvwjbfty96vrssUWW4QkvfLKK5nd8a23QoAQ/vOfWOMpZhmfa8kKne/c+flcjx7tfyeuvz6ZQJYsCWHNNUM47LDG7+Opp/xneP317MWVRXpd506Dz/VvfhNCt27+OmyKm2/21+CnnzZtP5m49FI/1vTp8R8rhBDmzQthrbVC2HXX5TbH8rr+4osQhgwJ4bbbQnj66RCqqkKYOjWExYuzf6wCAlSFGPLoXPdrehpIdWg5EngqbfsRUZeXbYEfgpfFDAP2MLNO0YTRPaJtxSE1sUN16SJSlzvv9E5Qhx+ezPFbtPDFZYYN80lejZFaxGiLLbIbmxS/igqYOtUnFjbF8OE+IXG99bITV33Kyvw6V3Xp99zj3X5ysTJnr14+ufOPf/QOPFts4XNNWraM/9jNUJwtGB8CRgIlZjbZzI4BrgAGmtkEYPfoe4Dngc+AicBQ4ASAEML3wMXAO9HlomhbcVh1VW+LlG89SkUkP8yb57XHBxzg/Y2TUl4O3323/CI6DTFqlHeM0CJG0lCDBvk8hqZ0eVm40HuXx9nVJd2WW/rky1yUUC1aBFde6RO8U28OpGjEVpMeQjikjpt+scxX9FHBiXXs527g7iyGll/U4UVE6vLEE97bOZe90Wuzxx6eKL3wgi8E0xCLF3uf9WOOiSc2KW5duviE42ee8Z7ejTFypE+s3HPP7MZWl9atvUVmLhY1evBB+PJL7+yiSdlFp4iWJytQJSWepIeszaEVkWIxdKh/RL/LLsnG0bmzJ+eNacX4wQdaxEiapqLCP8X5+uvGPX74cC/HyOVI84ABMG6crwoblyVLvD3iJpvA3nvHdxxJjJL0pJWW+gIE336bdCQikkfaffWVLzZy7LH5sdx7ebmv1tjQRVq0iJE0VUWFXz//fOMeP3y4v/5yucZALurSn3zSy2XPO0+j6EUqD/7yN3OlpX6tunQRSdPt+ed9suVRRyUdihs0yCeOjhjRsMeNHOl9j3v1iicuKX4bbeSLYD3zTMMfm5pLkatSl5TNN/ee4HGVvITgffJLSnzOihQlJelJSyXpqksXkZSFC1lr2DDvnrDWWklH47baCjp1anjJixYxkqYy89+Fl17yydQN8dJLntDmatJoSqtW8dalP/88jB7tPcvVWaVoKUlP2jrreMcDJekikvL007TJhwmj6Vq29ETnxRczb8X47be+nLhKXaSpKip8bkNDO6YMH+5vLpNo/1lW5p+SN7aWvi6pUfReveCww7K7b8krStKT1qIF9OunJF1Elhk6lPlrrpn70b8VGTTI+zGPGZPZ/UeN8msl6dJUAwbAyis3rBVjCJ6k7757MqPNcdWlV1Z6GdlZZ3knGSlaStLzQWmpatJFxI0bByNGMG3QoPz7GDtV1/vCC5ndX4sYSba0bQsDB3pdeqbd0MaNgylTcl+PnrLppr4eSrZLXi691Mvgjj46u/uVvKMkPR+UlsLnn8P8+UlHIiJJCgFOPhlWXZUp++2XdDS/tNZasNlmmdeljxrl7eFWXjneuKR5GDwYJk3ytp6ZGD7crwcOjC+m+rRsCTvvnN0k/a234OWX4fTT/Y2LFDUl6fmgpMRrPCdOTDoSEUnSY4/Bf/8Ll17KotVWSzqa2pWXw5tvwg8/1H+/1CJGKnWRbNlrL7/OtORl+HAfBOvZM76YVqSszP+3T56cnf1deimsvjr88Y/Z2Z/kNSXp+UAdXkRkzhz485/9I/I//CHpaOpWXu4J+Msv13+/sWPhp5+UpEv2dOsGW26ZWZI+f77XgidV6pKSqktv6ITX2owZ4+U+p54K7ds3fX+S95Sk54N+/fxadekizdell3r97C235F8terrUojArKnnRIkYSh4oKL6OaPr3++73xhrdrTHry9cYbe3eZbJS8XHaZ914/+eSm70sKgpL0fLDKKv5xnEbSRZqn8ePh2mvhyCNh++2TjqZ+rVt7t4wXXqh/At/IkbDmmtC7d85Ck2Zg8GB/3a1o8vLw4dCmDeyyS27iqkuLFh5DU5P0Tz6BRx+FE07wpF+aBSXp+aKkREm6SHMUApxyiq+XcOWVSUeTmUGDvMZ23Li676NFjCQOm20G3buvuORl2DDYcUcfBEtaWZk3h/jyy8bv44orYKWV4E9/yl5ckveUpOeL0lJP0jNtLSUixeHf//ZRv4sugq5dk44mM6k637pKXr77DiZMUKmLZJ8Z7L23J+ELF9Z+n6lTvX476VKXlKbWpX/5Jfzzn764WaH8jZCsUJKeL0pLfeLY1KlJRyIiuTJ3ro+M/epXcOKJSUeTuXXWgQ03rLvkQIsYSZwqKmD2bHjttdpvf+klv86XJH3DDaFz58aXvFx9tb85OfPM7MYleU9Jer5QhxeR5ufyy+Grr2DIEF/0p5AMGgSvv+6DCzWNHOmTX7WIkcRh9929R3hdJS/Dhvl8iE02yW1cdUmvS2/op+XTpsGdd8IRR/ibY2lWlKTni5ISv1aSLtI8TJwIV10Fhx7qC54UmvJyLzeobXQwtYhRPtQDS/FZeWXYddfaVx9duhRGjPAFjFrkUYpTVuZvyL/4omGPu+46WLQIzjknlrAkv+XRK7iZ697d+54qSRdpHk47zbtPXH110pE0TmpSXs269CVLtIiRxK+iAj777Jf/M8eM8faM+VLqkpKqS29Iycv338Ntt8FBB8H668cTl+Q1Jen5wsxLXtQrXaT4PfMMPPccXHihv0EvRCut5KOZNVsxjh3rJTBK0iVOe+/t1zVLXoYP9+uBA3Mbz4pssIGX4DQkSb/pJv9dOu+8+OKSvKYkPZ+kOryISPGaN89XDNxgA2+9WMjKy7213IQJy7ZpESPJhZ49vaSqZpI+bJgvINStWzJx1cUMBgzIvC599mxP0vfdFzbaKPbwJD8pSc8nJSVes/bTT0lHIiJxufpqT2yHDPGFgQpZeblfp5e8jBwJXbrAuusmE5M0HxUV8L//eVkI+P/ON97Iv1KXlLIyX1X4009XfN/bboOZM+H88+OPS/KWkvR8kurw8sknycYhIvH4/HPv6HLggV4qUujWWw/69Vu+FaMWMZJcqajwORCpDgcX5AAAHBRJREFUN4mvveaTmfM5SYcVl7zMm+cTRgcOhK22ij8uyVtK0vNJKklXXbpIcfrTn7zjxDXXJB1J9gwa5Iu0zJsHM2b4IINKXSQXttrKP7VJlbwMG+atGXfaKdm46tKvH6y11oqT9Lvugm++0Si6KEnPK+uv7//AVZcuUnxeeAGeegr++tfi6ndcXg7z5/sophYxklxq2dInkL7wAixe7JNGd9nFE/V8ZOaj6ZWVddelL1zorVl32KEwW7NKVilJzydt20Lv3krSRYrNggU+SbRfPx9NLyappOiFF5YtYrTllklHJc1FRQXMmgWPPAIffZS/pS4pZWW+snhdZa3/+hdMmuSj6CoZa/YKbIm7ZkAdXkSKz7XX+uJFL77orQuLSbt23rXixRdh7bW9s4YWMZJcGTjQJ2CffbZ/v+eeycazIul16alFDFOWLIErroDNN182KVuaNY2k55vSUn+HvXRp0pGISDZ89RVccgnsv3/+JxCNVV7uc2lef12lLpJbHTv6m8QpU3zNgf79k46ofn36QI8etdelP/aYtzM97zyNogugJD3/lJb6BKxJk5KORESy4fTT/fr665ONI06DBvn1woVK0iX3Kir8eo898j+5rasufelSuPRSXz9h//0TC0/yi5L0fJP6+EslLyKF76WX4PHHfWSsV6+ko4lP377L+qIrSZdc23dfL7v69a+TjiQzZWUwfbrX0Kc884yv1nvuud5AQgQl6fkn1YZRSbpIYVu4EE46yT/ePuOMpKOJl5knSOut5xeRXOrVyxf+2XvvpCPJTM1+6SH4KPq668IhhyQXl+QdJen5pksX6NRJvdJFCt2NN/rv8Y035m9LuGy67DIYMyb/yw2kOBXShOzevaFnz2VJ+ksvwTvvwDnnQCv185BllKTnGzN1eBEpdFOmwN//DoMHF87oXlO1aqWuLiKZSNWlv/rqslr0Hj3gyCOTjkzyjJL0fFRSoiRdpJCdcYYvrnLDDUlHIiL5qKwMvvsO7rjDk/UzziisTwMkJ5Sk56PSUl/s4Mcfk45ERBqqshIeftj7Nqs+W0RqM2CAX//pT9C5M/z+94mGI/lJSXo+Sk0eVV26SGFZtMgni/bu7fWlIiK16dXLJ4rOn++JukrFpBZK0vOROryIFKYhQ+DDD70nert2SUcjIvmsvNwbRZx4YtKRSJ5Skp6P1lvPJ2EpSS9O8+bBggVJRyHZNm0aXHCB/+Pdd9+koxGRfHfVVfDBB7DqqklHInlKSXo+at3aeysrSS8+IcDOO8MBByQdiWTbWWf5m6+bblIbQhFZsfbtvauLSB3UkDNflZaqJr0YDRsGVVX+9XvvwWabJRuPZMcbb8A//+mrBfbtm3Q0IiJSBDSSnq9KS2HCBG/jJsXjqquge3cfQbnmmqSjkWxYvNhrStdZB84/P+loRESkSCSSpJvZF2b2gZm9b2ZV0bbVzWyEmU2IrjtF283MbjKziWY2xsw2TyLmnCsp8WXFv/gi6UgkW955x1eYO/10b7f1yCPw5ZdJRyVNdfvtvtLmddepQ4OIiGRNkiPpZSGETUMIW0bfnwO8HELoC7wcfQ8wCOgbXY4Dbst5pElQh5fic/XVPkHo97+H007zbVrsprBNnw5/+QvsvrvmGYiISFblU7nLvsB90df3Afulbb8/uFHAambWLYkAc6qkxK9Vl14cJk6EJ56AE06ADh2gZ084+GAYOhRmzkw6Ommsc86Bn37SZFEREcm6pJL0AAw3s2ozOy7a1jWEMDX6ehrQNfq6BzAp7bGTo23FbfXVYc01NZJeLK67zttqnnLKsm1nnOEJ3u23JxeXNN6oUXDPPb4QyQYbJB2NiIgUmaS6u+wYQphiZmsCI8xsuUw0hBDMLDRkh1GyfxxA165dqayszFqwDTVnzpysHH/TtdaCt97i/QR/lnyXrXMdp9YzZ7LtXXfxzcCBfPLxx8u98dp4iy1Y5ZprGLXFFoQ2bRKMMjOFcL7j0nrWLNp/+imrfPop7SdOpNO770Lnzrw9YABLYjgnzflc55rOde7oXOeOznXhSyRJDyFMia6nm9m/ga2Bb8ysWwhhalTOMj26+xRgnbSHrx1tq7nPO4A7ALbccsswYMCAGH+C+lVWVpKV42+zDfz739nZV5HK2rmO09/+BosW0f3aa+meKmNKuewy2HNPdpk0CY45Jpn4GqAgzndTLVkCn3wCo0cvf/n662X36d4dtt4azjuPnXbcMZYwmsW5zhM617mjc507OteFL+dJupmtArQIIcyOvt4DuAh4GjgSuCK6fip6yNPASWb2MLAN8ENaWUxxKy2F776DGTNgjTWSjkYaY84cXyp+v/2WzTNIN3AgbLKJt2P83e+gRT5NE2kGfvjBO7OMHg3vv+/XY8fC/Pl+e+vWXsqy227+PG26KWy8MXTpkmzcIiJS9JIYSe8K/Nt8klUr4MEQwotm9g7wqJkdA3wJHBjd/3lgL2AiMBf4Xe5DTkiqw8v48bD99snGIo1z990+MfSss2q/3cxr0w8/HJ57DgYPzm18zcXSpd7ONJWIpy7pLU7XWMOT8BNO8IR8k008QS+AMiQRESk+OU/SQwifAZvUsn0GsFst2wNwYg5Cyz+pkdePP1aSXogWLYJrr4WddoJtt637fgcdBOed5y0alaRn1+OPe5vLMWNg9mzf1qIF9Ovn5WTHHbcsIe/eXR1aREQkbyQ1cVQy0bu3j+KpDWNheuwx+OoruOWW+u/XurX3TT/9dHjrLU8epek+/RSOOMLbXR5xxLJylQ03hJVXTjo6ERGReilJz2ctW/qIn9owFp4Q4KqroH9/2GuvFd//97+Hiy7y0fTHH48/vmIXAhx/vLe9fPll6FH8XVtFRKS4aJZavistVZJeiEaM8JrnM8/MbDJohw7wxz/Ck0/6wkfSNA8+6M/BZZcpQRcRkYKkJD3flZT4x/YLFyYdiTTEVVd5jfOhh2b+mFNO8ZHf666LL67mYMYMX2Bom218NF1ERKQAKUnPd6Wl3rf5s8+SjkQyVV3tJRanndawziDdu8Nvf+urWH77bXzxFbszz/SOOnfc4SVjIiIiBUhJer5LtWFUyUvhuPpq6NjRO4c01BlneI/uFU02ldq98oq/yTnjDO9nLiIiUqCUpOe79DaMkv8++8y7uvzxj7Dqqg1/fP/+sPfevgDS3LnZj6+YzZ8Pf/gDrLeer/IqIiJSwJSk57sOHbwMQkl6YbjuOi+xOPXUxu/jzDO9rvree7MWVrNw2WUwYQLcfju0a5d0NCIiIk2iJL0QlJaqV3oh+PZbX2H08MP9jVVj7bwzbLWVJ/xLlmQvvmI2bhxccYXX9A8cmHQ0IiIiTaYkvRCk2jCGkHQkUp9bboF587weuinMfDT900/hP//JTmzFbOlSr//v0EGdcUREpGgoSS8EpaUwaxZMn550JFKXuXO9jnyffWCDDZq+v//7P6+tvvpqvTlbkTvvhP/9D669Frp0SToaERGRrFCSXgg0eTT/3XOP15GfdVZ29teyJfz5z/DWW/DGG9nZZzGaOtXPeVkZ/9/enQdJVZ57HP8+7AIioMjIIqiFqLiAYuSKEQ1eg4gSK5jA1YsQCS7RgAsGNSZlkipliYorGoxES0VEkxhExQvivamUBFEHWcKVTcBS3BBEE2H0uX+8bzvtOMPi7T7n9PTvU3Wqz3nPme6n33mn5+lz3vc9XHBB2tGIiIgUjJL0UpCbhlH90rOpqiqcxT3xROjbt3DPO3Ik7LtvOJsutRs7NszqMnVq6CYkIiJSTyhJLwWdOkHz5jqTnlVPPAFr1xbuLHpO8+bwk5/AX/4CK1YU9rnrg6efhpkz4ec/h0MPTTsaERGRglKSXgoaNAhdXpSkZ487TJwYfj9nnVX457/sMmjWLJypl2rbtsGll4Z55Qv95UhERCQDlKSXCiXp2TR/PrzySpiNpUER/pzatYMRI+Chh0L/awl+8QtYvx7uuw+aNEk7GhERkYJTkl4qDjsM1q0L/W8lOyZOhIqKMD93sVx5JezYAXfcUbzXKCWLF8OUKeGuroUcAyAiIpIhStJLxWGHha4Vb7yRdiSS89prMHduGLzYtGnxXqdbNzjnHLjnHvj44+K9Timoqgpzou+/P9x0U9rRiIiIFI2S9FKhaRizZ9KkcAOdiy4q/muNGxfmyr///uK/VpbdfnvoXnTHHdC6ddrRiIiIFI2S9FKRm71CSXo2rFsHjz0WEvQkksU+feCkk+DWW0PXl3K0bh3ccAMMGgTf/37a0YiIiBSVkvRS0bw5dOmiudKz4tZbw0DRMWOSe81x48JgyccfT+41s8I9TEdpBnfdpTnRRUSk3lOSXkqOOAKefRaeeSbtSMrbBx+EW9Gfd16Ywz4pgwaFsQmTJoWktZw8/jjMmQO/+Q0ceGDa0YiIiBSdkvRSMmFCGDA3cCAMHx6SRUne3XfDp5/C1Vcn+7oNGsBVV4UBq/PmJfvaadq8GX76UzjuOLj88rSjERERSYSS9FJy1FHw6qvhDouPPhrOrM+alXZU5eWf/wyDFwcNgh49kn/988+H9u3D2fRyMX48vP8+/O530LBh2tGIiIgkQkl6qWnaFH79a3j55dDV4txzwyA63egmGdOnh4QxrbtcNmsWzirPnQuVlenEkKS//jXcsGjsWOjVK+1oREREEqMkvVQdcwwsXAg33wxPPx3Oqk+fXn59lZP0+ecweXL1TCtpueQSaNEixFKfffZZmBO9Sxe48ca0oxEREUmUkvRS1qgR/Oxn4YzqkUfCyJFwxhnw5ptpR1Y/PfkkrFkTzqKnObtImzYwahTMmAEbNqQXR7FNnAgrVoSbOLVokXY0IiIiiVKSXh907w4vvgh33hm6B/ToEda/+CLtyOoP9zBw99BD4eyz044GrrgixHTbbWlHUhwrV4aZXH74w/DFU0REpMwoSa8vGjQI80gvWwZ9+4ZZMPr107zqhbJgASxeHGZXycLgxS5d4Ac/CP21P/oo7WgKyx0uvjjcG6C+fgkRERHZBSXp9U2XLmEu9QcegKVLQ9/1CROgqirtyErbxIlh+svhw9OOpNq4cbBtG9x7b9qRFNb06eFL0cSJUFGRdjQiIiKpUJJeH5nBiBGhP++ZZ4Yp7E44oTxmAymGJUvCF58xY8LsKlnRqxf07w9TpoRBlvXBu++GqxXf/jZceGHa0YiIiKRGSXp9VlEBTzwR7ta4cSP07g033FB/ErqkTJoUBi5ecknakXzduHFh+s1HHkk7ksK48srqqwMN9PEkIiLlS/8Fy8GQIbB8OQwbFgbj9eoFL72UdlSlYf36MIvK6NFhVpWsOf10OProMB1jqQ8Ufu45ePhhuO46OPzwtKMRERFJlZL0crHvvvDgg2FO9Y8/hhNPDDOEfPJJ2pFlW27g4tix6cZRFzO4+urwJeyZZ9KO5pv79NNwpaJ7d7j22rSjERERSZ2S9HIzcGCYAebii0MCevTRMH9+2lFl0+bNYfaUYcPgwAPTjqZuQ4eGu89OmpR2JN/cjTfC2rWhm0vTpmlHIyIikrpGaQcgKWjVCu6+O8xBPWpUGHw4ahScf364q2ZVVVjy12vb3p1jPv88dMPo0AEOOSQsBx8Me++ddi3s2j33hCsN48alHcnONW4czvRffTUsWgTHH592RLtn69YwKHfRIvjtb8NA0X790o5KREQkE5Skl7N+/cKML7/8JdxyC0ybVrjnbtw43BE1N6f4tm1f3d+uXXXCnkvec9sHHJDuHT0B/vWvMGvKGWfAUUelG8vu+PGP4Ve/CmfTZ85MO5qvcg93Rq2shNdeC0tlJaxeXX3M4YeHKRdFREQEUJIuzZuHxG706JBI5RLrRo2+utQs29kxtc3KsWVLSMrWrAmPueVvfwsDM/MHPe61V0jW8xP43HrXrt+sO8T27eHM7datoU9+br2u7TVrwnSA11zzjas2Ua1ahS5MkyfD7NnQrVuY171162S/8GzfHqb+zCXiuaR88+bqY7p1C4OXR46Enj3DXP4dO6b/xUxERCRDlKRL0K1bWIpln33g2GPDUtP27fDmm19P4NesgXnzwqDCHDPo3PnLpL3rjh0wZ86uk+7dmXbSLHTDadUqLCNHllb3izFjQjems86qLmvcOCTr7duHx/z1mo/t2oXjd9eHH4ZEPD8ZX74cduwI+5s1C2Mezj03JOI9e4arEqXQ1UlERCRlJZOkm9kAYArQEJjm7jenHJIUSpMmdX9JcIdNm2o/Cz97Nl03bQrJYC6xziXZnTt/vayu7VxZixalPTd3hw6wcmVIlDdtClcCaj4uWxYe6/rS0rZt3Yl8y5Z0ffbZ0DWqsjJMT5lTURES8QEDQjLes2f4fea6O4mIiMgeKYkk3cwaAncB/w5sBBaZ2VPuvjzdyKTozEICWFEBfft+bfeCefM4pX//FALLqA4dwrIz7uHqQl2JfG59yZLw+NFHX/5olwYNwjSJffvCpZdWd1epqCjyGxMRESkvJZGkA98CVrn7GgAzmwEMBpSklzudqd1zZtVXEHani9P27SFx37KF/9mwgZMHDCh+jCIiImWuVK7tdwQ25G1vjGUiUmxNmoR52Hv04ItmzdKORkREpCyYu6cdwy6Z2RBggLuPitv/CZzg7pflHTMaGA3Qvn3742bMmJFKrADbtm2jZcuWqb1+OVFdJ0v1nRzVdXJU18lRXSdHdZ2cU089dbG79y7085ZKd5e3gM55251i2Zfc/T7gPoDevXv7KaecklhwNS1YsIA0X7+cqK6TpfpOjuo6Oarr5Kiuk6O6Ln2l0t1lEdDNzA4ysybAUOCplGMSERERESmKkjiT7u5VZnYZ8BxhCsbfu/uylMMSERERESmKkkjSAdx9DjAn7ThERERERIqtVLq7iIiIiIiUDSXpIiIiIiIZoyRdRERERCRjlKSLiIiIiGSMknQRERERkYwpiTuO7ikzew94M8UQ9gPeT/H1y4nqOlmq7+SorpOjuk6O6jo5quvkdHf3vQv9pCUzBeOecPd2ab6+mb1cjNvDyteprpOl+k6O6jo5quvkqK6To7pOjpm9XIznVXcXEREREZGMUZIuIiIiIpIxStKL4760Aygjqutkqb6To7pOjuo6Oarr5Kiuk1OUuq6XA0dFREREREqZzqSLiIiIiGSMkvQCM7MBZrbSzFaZ2fi04ylFZtbZzF4ws+VmtszMxsTytmb2vJm9ER/bxHIzs9tjnS8xs2PznuuCePwbZnZBWu8py8ysoZm9amaz4/ZBZrYw1udjZtYkljeN26vi/q55z3FtLF9pZt9N551kn5m1NrNZZvYPM1thZv+mdl0cZnZF/PxYamaPmlkzte3CMLPfm9m7ZrY0r6xg7djMjjOz1+PP3G5mluw7zI466npS/AxZYmZ/NLPWeftqba915SZ1/U2Uo9rqOm/fVWbmZrZf3E6mXbu7lgItQENgNXAw0ASoBI5IO65SW4ADgGPj+t7A/wJHABOB8bF8PDAhrg8EngEM6AMsjOVtgTXxsU1cb5P2+8vaAlwJPALMjtszgaFxfSpwSVy/FJga14cCj8X1I2JbbwocFP8GGqb9vrK4AH8ARsX1JkBrteui1HNHYC2wV9yeCYxQ2y5Y/Z4MHAsszSsrWDsG/h6PtfizZ6T9njNW16cDjeL6hLy6rrW9spPcpK6/iXJcaqvrWN4ZeI5w/539Ylki7Vpn0gvrW8Aqd1/j7tuBGcDglGMqOe7+tru/Etc/BlYQ/ukOJiQ5xMfvxfXBwIMevAS0NrMDgO8Cz7v7h+6+GXgeGJDgW8k8M+sEnAlMi9sGfAeYFQ+pWc+5+p8F9I/HDwZmuPtn7r4WWEX4W5A8ZrYP4Z/A/QDuvt3dP0LtulgaAXuZWSOgOfA2atsF4e7/DXxYo7gg7Tjua+XuL3nIbB7Me66yU1tdu/tcd6+Kmy8BneJ6Xe211txkF5/3ZaeOdg1wK3ANkD+IM5F2rSS9sDoCG/K2N8Yy+YbiZedewEKgvbu/HXe9A7SP63XVu34fu3Yb4cPni7i9L/BR3j+A/Dr7sj7j/i3xeNXz7jkIeA94wEL3omlm1gK164Jz97eAycB6QnK+BViM2nYxFaodd4zrNculdj8inJWFPa/rnX3eC2Bmg4G33L2yxq5E2rWSdMksM2sJPAGMdfet+fviN1FNTfT/YGaDgHfdfXHasZSJRoRLqfe4ey/gE0K3gC+pXRdG7A89mPDFqAPQAl1tSIzacTLM7HqgCng47VjqIzNrDlwH/CKtGJSkF9ZbhL5LOZ1imewhM2tMSNAfdvcnY/GmeMmI+PhuLK+r3vX72Lm+wNlmto5w+fM7wBTCZbtG8Zj8OvuyPuP+fYAPUD3vro3ARndfGLdnEZJ2tevCOw1Y6+7vufsO4ElCe1fbLp5CteO3qO6+kV8uecxsBDAIOC9+KYI9r+sPqPtvQuAQwhf9yvh/shPwiplVkFC7VpJeWIuAbnG0dBPCAKSnUo6p5MR+cvcDK9z9lrxdTwG5kdIXAH/OKx8eR1v3AbbEy67PAaebWZt4Zu30WCaAu1/r7p3cvSuhrc539/OAF4Ah8bCa9Zyr/yHxeI/lQy3MkHEQ0I0wQEbyuPs7wAYz6x6L+gPLUbsuhvVAHzNrHj9PcnWttl08BWnHcd9WM+sTf3fD855LCDO1ELopnu3un+btqqu91pqbxDZe199E2XP31919f3fvGv9PbiRMavEOSbXr3R31qmW3RwcPJMxGshq4Pu14SnEBTiJcKl0CvBaXgYT+c/OAN4D/AtrG4w24K9b560DvvOf6EWHwzCpgZNrvLasLcArVs7scTPhgXwU8DjSN5c3i9qq4/+C8n78+1v9Kyngmht2o557Ay7Ft/4kw+l/tujh1fSPwD2Ap8BBhxgu17cLU7aOEvv47CInLhYVsx0Dv+HtbDdxJvPFiOS511PUqQr/n3P/HqXnH19peqSM3qetvohyX2uq6xv51VM/ukki71h1HRUREREQyRt1dREREREQyRkm6iIiIiEjGKEkXEREREckYJekiIiIiIhmjJF1EREREJGOUpIuI1HNmVmFmM8xstZktNrM5Znaymc3axc8tMLPeScUpIiLVGu36EBERKVXxxhl/BP7g7kNj2TFAK3cfstMfFhGR1OhMuohI/XYqsMPdp+YK3L2ScPfTpQBm1tDMJpvZUjNbYmaX13wSMxtmZq/HYyYkF76ISHnSmXQRkfrtSGDxLo4ZDXQFerp7lZm1zd9pZh2ACcBxwGZgrpl9z93/VIR4RUQEnUkXERE4DbjX3asA3P3DGvuPBxa4+3vxmIeBkxOOUUSkrChJFxGp35YRzoCLiEgJUZIuIlK/zQeamtnoXIGZHQ10zjvmeeAiM2sU97f96lPwd6Cfme1nZg2BYcCLxQ1bRKS8KUkXEanH3N2Bc4DT4hSMy4CbgHfyDpsGrAeWmFkl8B81nuNtYDzwAlAJLHb3PycRv4hIubLw+S0iIiIiIlmhM+kiIiIiIhmjJF1EREREJGOUpIuIiIiIZIySdBERERGRjFGSLiIiIiKSMUrSRUREREQyRkm6iIiIiEjGKEkXEREREcmY/wONsSlzRhzeMwAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"IjAkgViOJI-e","executionInfo":{"status":"ok","timestamp":1659112314871,"user_tz":180,"elapsed":1005,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"a09a5817-66e6-47ed-cbd4-a74ec5525fbb"},"source":["#@title Probar el Agente DQN Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","MostarDetalleJugada = False #@param {type:\"boolean\"}\n","\n","if DQNpolicy is not None:\n","  SimularEntorno(eval2P_env, [DQNpolicy, random_policy], [\"DQN\", \"AzarPolicy\"], \"Probando el Agente DQN entrenado contra azar\", cantidad_probar, MostarDetalleJugada)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","**  Probando el Agente DQN entrenado contra azar  **\n","    (DQN usa 'O', AzarPolicy usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |- O - |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |- O X |\n","      2 |X - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |O - O |\n","      1 |X O X |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |- O X |\n","      2 |- - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |O - O |\n","      1 |- O X |\n","      2 |O - X |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 6: \n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |- O X |\n","      2 |X - O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 7: \n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |X O X |\n","      2 |- X O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 8: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |O O X |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 9: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |X O - |\n","      2 |O - X |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 10: \n","         0 1 2 \n","        --------\n","      0 |O X - |\n","      1 |X O - |\n","      2 |- X O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","= Total Partidos Ganados: \n","\t DQN con 'O': 8.0 *\n","\t AzarPolicy con 'X': 2.0 \n"]}]},{"cell_type":"markdown","source":["## Torneo "],"metadata":{"id":"W11A-Wb_JI-f"}},{"cell_type":"code","source":["#@title Cargar o Guardar los Agentes Q-Learning y DQN entrenados\n","\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-TaTeTi\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","  import os\n","  from google.colab import drive\n","  from tf_agents.policies import TFPolicy, policy_saver\n","  # determina lugar donde se guarda el modelo\n","  policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  qlCSV = policy_dir + \"/QM-QLearning.csv\"\n","  # Montar Drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  if (DQNpolicy is not None) and isinstance(DQNpolicy, TFPolicy):\n","    # guarda la politica del agente DQN entrenado\n","    tf_policy_saver = policy_saver.PolicySaver(DQNpolicy)\n","    tf_policy_saver.save(policy_dir)\n","    print(\"\\nPolítica DQN guardada en \", policy_dir)\n","  if ql_policy is not None:\n","    if not os.path.exists(policy_dir):\n","         os.makedirs(policy_dir)\n","    ql_policy.saveQ(qlCSV)\n","    print(\"\\nPolítica Q-Learning guardada en \", qlCSV)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  DQNpolicy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica DQN recuperada de \", policy_dir)\n","  ql_policy.loadQ(qlCSV)\n","  print(\"\\nPolítica Q-Learning recuperada de \", qlCSV)\n"],"metadata":{"cellView":"form","id":"XTTpaVBfBOGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659112323424,"user_tz":180,"elapsed":8566,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"4188dae2-5b73-423e-ae8e-8de47e5d85fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n","WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_55_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-TaTeTi/assets\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n","  \"imported and registered.\" % type_spec_class_name)\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-TaTeTi/assets\n"]},{"output_type":"stream","name":"stdout","text":["\n","Política DQN guardada en  /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-TaTeTi\n","\n","Política Q-Learning guardada en  /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-TaTeTi/QM-QLearning.csv\n"]}]},{"cell_type":"code","source":["#@title Cargar Agente AlphaZero ya entrenado (opcional)\n","#@markdown (partes de código copiadas de AlphaZero)\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"AlphaZero-TaTeTi\" #@param {type:\"string\"}\n","accion_realizar = \"Cargar Modelo\" #@param [\"-\", \"Cargar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","\n","  import math\n","  from tensorflow.keras.models import load_model\n","\n","  ## comienzo copiado de AlphaZero\n","\n","  ### Definir clases para RNA del Agente\n","  class NNet():\n","      def __init__(self, game, args):\n","          # game params\n","          self.board_x, self.board_y = game.getBoardSize()\n","          self.action_size = game.getActionSize()\n","          self.args = args\n","\n","          if self.args is None:\n","            self.model = None\n","          else:\n","            # Neural Net\n","            # AlphaZero uses a deep neural network (pi,v)=f(s)\n","            # which takes the board position \"s\" as an input \n","            # and outputs a vector of move probabilities \"p=Pr(a|s)\" for each action \"a\",\n","            # and a scalar value \"v\" estiamting the expected outcome \"z\" from position \"s\",        \n","            # Here it learns these move probabilities and value estimates entirely from self-play, \n","            # which are then used guide its search.\n","            self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n","\n","            x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n","            h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n","            h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n","            h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n","            h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n","            h_conv4_flat = Flatten()(h_conv4)       \n","            s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n","            s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n","            # \"pi\" is the MCTS informed policy vector for the given board,         \n","            self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n","            # \"v\" is its value. \n","            self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n","\n","            self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n","            self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))\n","            # muestra el modelo creado\n","            self.model.summary()\n","\n","\n","  class NNetWrapper():\n","      def __init__(self, game, args):\n","          self.nnet = NNet(game, args)\n","          self.board_x, self.board_y = game.getBoardSize()\n","          self.action_size = game.getActionSize()\n","          self.trainHistory = []\n","\n","      def train(self, examples):\n","        # nota se hace una pequeña modificación para guardar información para generar gráfico\n","          \"\"\"\n","          examples: list of examples, each example is of form (board, pi, v)\n","          \"\"\"\n","          input_boards, target_pis, target_vs = list(zip(*examples))\n","          input_boards = np.asarray(input_boards)\n","          target_pis = np.asarray(target_pis)\n","          target_vs = np.asarray(target_vs)\n","          # entrena y guada histórico para el gráfico\n","          history = self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n","          self.trainHistory.append( history )\n","\n","      def genTrainGraphs(self, infoGraficar='loss', titulo=\"Gráfico del Entrenamiento\"):\n","        # nota se agrega este método para gráficar resultados del entrenamiento\n","        plt.figure(figsize=(15,8))       \n","        legs = []\n","        for i in range(len(self.trainHistory)):\n","          plt.plot(self.trainHistory[i].history[infoGraficar])\n","          legs.append( \"Iter \"+str(i+1))\n","        plt.title(titulo)\n","        plt.legend(legs, loc='best')\n","        plt.ylabel('')\n","        plt.xlabel('Epoch')\n","        plt.show()\n","\n","\n","      def predict(self, board):\n","          \"\"\"\n","          board: np array with board\n","          Returns:\n","              pi: a policy vector for the current board- a numpy array of length\n","                  game.getActionSize\n","              v: a float in [-1,1] that gives the value of the current board\n","          \"\"\"\n","          # timing\n","          ##start = time.time()\n","\n","          # preparing input\n","          board = board[np.newaxis, :, :]\n","\n","          # run\n","          pi, v = self.nnet.model.predict(board, verbose = 0)\n","\n","          ##print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n","          ##print(\"PI \", pi, \"--\", v)\n","          return pi[0], v[0]\n","      \n","      def save_checkpoint(self, folder='checkpoint', filename='checkpoint.h5'):\n","        # nota se hace una pequeña modificación para que funcione corretamente\n","          filepath = os.path.join(folder, filename)\n","          if not os.path.exists(folder):\n","              print(\"  No existe el directorio de checkpoints! Se crea {}\".format(folder))\n","              os.mkdir(folder)\n","          #else:\n","          #    print(\"Checkpoint Directory exists! \")\n","          self.nnet.model.save_weights(filepath)\n","          print(\"  Checkpoint grabado en '{}'\".format(filepath))\n","\n","      def load_checkpoint(self, folder='checkpoint', filename='checkpoint.h5'):\n","        # nota se hace una pequeña modificación para que funcione corretamente\n","          filepath = os.path.join(folder, filename)          \n","          if not os.path.exists(folder):\n","            print(\"  No se encuentra un checkpoint en '{}'\".format(filepath))\n","            #  raise(\"No model in path '{}'\".format(filepath))\n","          else:\n","            self.nnet.model.load_weights(filepath)\n","            print(\"  Checkpoint cargado de '{}'\".format(filepath))\n","\n","\n","      def save_model(self, folder='model', filename='model.h5'):\n","        # nota nuevo método para guardar todo el modelo\n","          filepath = os.path.join(folder, filename)\n","          if not os.path.exists(folder):\n","              print(\"  No existe el directorio para guardar el modelo! Se crea {}\".format(folder))\n","              os.mkdir(folder)\n","          #else:\n","          #    print(\"Checkpoint Directory exists! \")\n","          self.nnet.model.save(filepath)\n","          print(\"  Checkpoint grabado en '{}'\".format(filepath))\n","\n","      def load_model(self, folder='model', filename='model.h5'):\n","        # nota nuevo método para cargar todo el modelo\n","          filepath = os.path.join(folder, filename)          \n","          if not os.path.exists(folder):\n","            print(\"  No se encuentra el modelo guardado en '{}'\".format(filepath))\n","            #  raise(\"No model in path '{}'\".format(filepath))\n","          else:\n","            self.nnet.model = load_model(filepath)\n","            print(\"  Modelo cargado de '{}'\".format(filepath))\n","            # muestra el modelo creado\n","            self.nnet.model.summary()          \n","\n","\n","  print(\"Clases de la RNA definidas.\")\n","\n","  ## Definir clase Monte Carlo tree search (MCTS)\n","\n","  # we don’t directly train our policy network to make “good” moves.\n","  # Instead we train it to mimic the output of the Monte Carlo Tree Search.\n","  # As we play games, the policy network suggests moves to MCTS.\n","  # MCTS uses these suggestions (or priors) to explore the game tree \n","  # and returns a better set of probabilities for a given state. \n","  # We record the state and the probabilities produced by the MCTS.\n","  # --> Despite the world’s focus on the neural networks involved in AlphaZero,\n","  #  the true magic of AlphaZero actually comes from Monte Carlo Tree Search. \n","  # It’s here that AlphaZero simulates moves \n","  # and looks ahead to explore a range of promising moves.\n","\n","  EPS = 1e-8\n","\n","  class MCTS():\n","      \"\"\"\n","      This class handles the MCTS tree.\n","      \"\"\"\n","\n","      def __init__(self, game, nnet, args):\n","          self.game = game\n","          self.nnet = nnet\n","          self.args = args\n","          self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n","          self.Nsa = {}  # stores #times edge s,a was visited\n","          self.Ns = {}  # stores #times board s was visited\n","          self.Ps = {}  # stores initial policy (returned by neural net)\n","\n","          self.Es = {}  # stores game.getGameEnded ended for board s\n","          self.Vs = {}  # stores game.getValidMoves for board s\n","\n","      def getActionProb(self, canonicalBoard, temp=1):\n","          \"\"\"\n","          This function performs numMCTSSims simulations of MCTS starting from\n","          canonicalBoard.\n","          Returns:\n","              probs: a policy vector where the probability of the ith action is\n","                    proportional to Nsa[(s,a)]**(1./temp)\n","          \"\"\"\n","          for i in range(self.args.numMCTSSims):\n","              self.search(canonicalBoard)\n","\n","          s = self.game.stringRepresentation(canonicalBoard)\n","          counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n","\n","          if temp == 0:\n","              bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n","              bestA = np.random.choice(bestAs)\n","              probs = [0] * len(counts)\n","              probs[bestA] = 1\n","              return probs\n","\n","          counts = [x ** (1. / temp) for x in counts]\n","          counts_sum = float(sum(counts))\n","          probs = [x / counts_sum for x in counts]\n","          return probs\n","\n","      def search(self, canonicalBoard):\n","          \"\"\"\n","          This function performs one iteration of MCTS. It is recursively called\n","          till a leaf node is found. The action chosen at each node is one that\n","          has the maximum upper confidence bound as in the paper.\n","          Once a leaf node is found, the neural network is called to return an\n","          initial policy P and a value v for the state. This value is propagated\n","          up the search path. In case the leaf node is a terminal state, the\n","          outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n","          updated.\n","          NOTE: the return values are the negative of the value of the current\n","          state. This is done since v is in [-1,1] and if v is the value of a\n","          state for the current player, then its value is -v for the other player.\n","          Returns:\n","              v: the negative of the value of the current canonicalBoard\n","          \"\"\"\n","\n","          s = self.game.stringRepresentation(canonicalBoard)\n","\n","          if s not in self.Es:\n","              self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n","          if self.Es[s] != 0:\n","              # terminal node\n","              return -self.Es[s]\n","\n","          if s not in self.Ps:\n","              # leaf node\n","              self.Ps[s], v = self.nnet.predict(canonicalBoard)\n","              valids = self.game.getValidMoves(canonicalBoard, 1)\n","              self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n","              sum_Ps_s = np.sum(self.Ps[s])\n","              if sum_Ps_s > 0:\n","                  self.Ps[s] /= sum_Ps_s  # renormalize\n","              else:\n","                  # if all valid moves were masked make all valid moves equally probable\n","\n","                  # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n","                  # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n","                  print(\"All valid moves were masked, doing a workaround.\")\n","                  self.Ps[s] = self.Ps[s] + valids\n","                  self.Ps[s] /= np.sum(self.Ps[s])\n","\n","              self.Vs[s] = valids\n","              self.Ns[s] = 0\n","              return -v\n","\n","          valids = self.Vs[s]\n","          cur_best = -float('inf')\n","          best_act = -1\n","\n","          # pick the action with the highest upper confidence bound\n","          for a in range(self.game.getActionSize()):\n","              if valids[a]:\n","                  if (s, a) in self.Qsa:\n","                      u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n","                              1 + self.Nsa[(s, a)])\n","                  else:\n","                      u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n","\n","                  if u > cur_best:\n","                      cur_best = u\n","                      best_act = a\n","\n","          a = best_act\n","          next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n","          next_s = self.game.getCanonicalForm(next_s, next_player)\n","\n","          v = self.search(next_s)\n","\n","          if (s, a) in self.Qsa:\n","              self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n","              self.Nsa[(s, a)] += 1\n","\n","          else:\n","              self.Qsa[(s, a)] = v\n","              self.Nsa[(s, a)] = 1\n","\n","          self.Ns[s] += 1\n","          return -v\n","\n","\n","  class dotdict(dict):\n","      def __getattr__(self, name):\n","          return self[name]\n","\n","  print(\"Clase MCTS definida\")\n","\n","  ## fin copiado de AlphaZero\n","\n","  # define método para crear jugador AlphaZero  \n","  class AlphaZeroPlayer():\n","    def __init__(self, game):\n","        mctsArgs = dotdict({'numMCTSSims': 50, 'cpuct': 1.0})\n","        self.mctsNN = MCTS(game, AZnnet, mctsArgs)\n","\n","    def play(self, board):\n","        #aux = lambda x: np.argmax(self.mctsNN.getActionProb(x, temp=0))\n","        a = np.argmax(self.mctsNN.getActionProb(board, temp=0))\n","        return a\n","\n","  # Montar Drive\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","\n","if accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  juegoAux = ProblemGame()\n","  AZnnet = NNetWrapper(juegoAux, None)\n","  AZnnet.load_model(directorio_modelo, nombre_modelo_grabar)\n","  # define un entorno especial para jugar contra AlphaZero\n","  evalAZ_py_env = ProblemGameEnv(AlphaZeroPlayer)\n","  evalAZ_env = tf_py_environment.TFPyEnvironment(evalAZ_py_env)  \n","  print(\"\\nEntorno para jugar contra AlphaZero definida\")\n","else:\n","  evalAZ_env = None\n","  print(\"No se carga modelo AlphaZero.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"b66kKMS32bxB","executionInfo":{"status":"ok","timestamp":1659112334011,"user_tz":180,"elapsed":10604,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"5dc58c37-b908-4455-a178-e3f4e7f7e6e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases de la RNA definidas.\n","Clase MCTS definida\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","  Modelo cargado de '/content/gdrive/MyDrive/IA/demoRL/ModelosOK/AlphaZero-TaTeTi'\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 3, 3)]       0           []                               \n","                                                                                                  \n"," reshape (Reshape)              (None, 3, 3, 1)      0           ['input_1[0][0]']                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 3, 3, 512)    5120        ['reshape[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 3, 3, 512)   2048        ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," activation (Activation)        (None, 3, 3, 512)    0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 3, 3, 512)    2359808     ['activation[0][0]']             \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 3, 3, 512)   2048        ['conv2d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (None, 3, 3, 512)    0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 3, 3, 512)    2359808     ['activation_1[0][0]']           \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 3, 3, 512)   2048        ['conv2d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 3, 3, 512)    0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 1, 1, 512)    2359808     ['activation_2[0][0]']           \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 1, 1, 512)   2048        ['conv2d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_3 (Activation)      (None, 1, 1, 512)    0           ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," flatten (Flatten)              (None, 512)          0           ['activation_3[0][0]']           \n","                                                                                                  \n"," dense (Dense)                  (None, 1024)         525312      ['flatten[0][0]']                \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 1024)        4096        ['dense[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 1024)         0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," dropout (Dropout)              (None, 1024)         0           ['activation_4[0][0]']           \n","                                                                                                  \n"," dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 512)         2048        ['dense_1[0][0]']                \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 512)          0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 512)          0           ['activation_5[0][0]']           \n","                                                                                                  \n"," pi (Dense)                     (None, 10)           5130        ['dropout_1[0][0]']              \n","                                                                                                  \n"," v (Dense)                      (None, 1)            513         ['dropout_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,154,635\n","Trainable params: 8,147,467\n","Non-trainable params: 7,168\n","__________________________________________________________________________________________________\n","\n","Entorno para jugar contra AlphaZero definida\n"]}]},{"cell_type":"code","source":["#@title Define Agente para Humano (opcional)\n","crearAgenteHumano = True #@param {type:\"boolean\"}\n","\n","HumanoPolicy = None\n","\n","if crearAgenteHumano:\n","\n","    from tf_agents.policies.fixed_policy import FixedPolicy\n","    from tf_agents.trajectories import policy_step\n","    from tf_agents.utils import nest_utils\n","    from tf_agents.specs import tensor_spec\n","    from tf_agents.typing import types\n","    from typing import Optional, Text\n","\n","\n","    # Clase Policy Wrapper para usuario Humano\n","    # (se hereda de FixedPolicy porque es una simple para tener como base)\n","    class Humano_TF_Policy(FixedPolicy):\n","        \n","      def __init__(self,\n","                  time_step_spec: ts.TimeStep,            \n","                  action_spec: types.NestedTensorSpec,                              \n","                  policy_info: types.NestedTensorSpec = (),\n","                  info_spec: types.NestedTensorSpec = (),\n","                  name: Optional[Text] = None):    \n","        \n","          # llama al padre\n","          super(FixedPolicy, self).__init__(time_step_spec, action_spec, clip=False,\n","                                          info_spec=info_spec,\n","                                          name=name,\n","                                          emit_log_probability=False)             \n","          # guarda valores auxiliares\n","          self._policy_info = policy_info\n","          self._time_step_spec = tensor_spec.from_spec(time_step_spec)\n","          self._action_spec = tensor_spec.from_spec(action_spec)\n","          self.n = time_step_spec.observation.shape\n","\n","      # función auxiliar para determina jugada\n","      def play(self, valid):\n","          while True: \n","              print(\"Indique la coordenada donde desea jugar: \")\n","              e = input()\n","              x, y = -1, -1\n","              if e.find(' ') > 0:\n","                x, y = [int(x) for x in e.split(' ')]\n","              else:\n","                if e[0].isnumeric() and e[1].isnumeric():\n","                  x = int(e[0])\n","                  y = int(e[1])             \n","              if (x<0) or (y<0) or (x>=len(valid)) or (y>=len(valid[0])):\n","                  print('Coordenada inválida!')\n","              elif valid[x,y]==0:\n","                  a = self.n[0] * x + y if x!= -1 else self.n[1] ** 2\n","                  break\n","              else:\n","                  print('Coordenada ya utilizada!')\n","          return a\n","\n","      # devuelve la accion que se debe aplicar usando selección del usuario\n","      # (basado en clase HumanPlayer de AlphaZero)\n","      def _action(self, time_step, policy_state, seed):\n","          # obtiene estado actual\n","          ob = time_step.observation.numpy()[0]\n","          # solicita la accion para jugar\n","          # que controla contra estado del tablero\n","          accionAplicar = self.play(ob)\n","          # formatea el valor a devolver usando la action_spec y time_step_spec\n","          def convert(action, spec):\n","            return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n","          self._action_value = tf.nest.map_structure(convert, accionAplicar,\n","                                                      self._action_spec)\n","          outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n","          action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n","                                      self._action_value)\n","          # devuelve la accion\n","          return policy_step.PolicyStep(action, policy_state, self._policy_info)\n","\n","    print(\"Clase Humano_TF_Policy creada.\")\n","\n","    # instancia política de Humano\n","    HumanoPolicy = Humano_TF_Policy(\n","                                  time_step_spec = train_env.time_step_spec(),\n","                                  action_spec = train_env.action_spec()\n","                                   )\n","    # hace una prueba\n","    ##res = SimularEntorno(eval2P_env, [HumanoPolicy, random_policy], [\"Humano\", \"AzarPolicy\"], \"Probando el Agente Humano contra azar\", 1, True)"],"metadata":{"cellView":"form","id":"JYB6K2sg3Fhq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659112334012,"user_tz":180,"elapsed":42,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"6e7b4b0b-dde6-4f96-c204-de3df29062aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase Humano_TF_Policy creada.\n"]}]},{"cell_type":"code","source":["#@title Hacer competir Agentes entrenados de a pares\n","agente1 = \"Humano\" #@param [\"Humano\", \"DQN\", \"Q-Learning\", \"AzarPolicy\"]\n","agente2 = \"DQN\" #@param [\"AlphaZero\", \"DQN\", \"Q-Learning\"]\n","cantidad_probar = 10 #@param {type:\"integer\"}\n","mostarDetalleJugada = False #@param {type:\"boolean\"}\n","\n","# función auxiliar para hacer competir\n","def hacerCompetir(agente1, agente2, cantidad, mostarDetalle=False):\n","    \n","  # inicializa las variables auxiliares\n","  envTorneo = None\n","  polAgs = []\n","\n","  # determina agente 1\n","  if agente1 == \"DQN\":\n","    if DQNpolicy is not None:\n","      polAgs.append( DQNpolicy )\n","  elif agente1 == \"Q-Learning\":\n","    if ql_policy._QtableEntrenada:\n","      polAgs.append( ql_policy )\n","  elif agente1 == \"AzarPolicy\":\n","      polAgs.append( random_policy )\n","  elif agente1 == \"Humano\":  \n","      polAgs.append( HumanoPolicy )\n","      # fuerza mostrar detalle para humano\n","      mostarDetalle = True\n","      cantidad = 1\n","\n","  # determina agente 2\n","  if agente2 == \"AlphaZero\":\n","    if evalAZ_env is not None:\n","      envTorneo = evalAZ_env\n","  else:\n","      envTorneo = eval2P_env\n","      if agente2 == \"DQN\":\n","        if DQNpolicy is not None:\n","          polAgs.append( DQNpolicy )\n","      elif agente2 == \"Q-Learning\":\n","        if ql_policy._QtableEntrenada:\n","          polAgs.append( ql_policy )\n","\n","  # simula\n","  if len(polAgs) < 1 :\n","    print(\"No se encuentra disponible el agente 1 para jugar!\")\n","    return [0, 0]\n","  elif (envTorneo is None) and (len(polAgs) < 2):\n","    print(\"No se encuentra disponible el agente 2 para jugar!\")\n","    return [0, 0]\n","  else:\n","    # juega\n","    return SimularEntorno(envTorneo, polAgs, [agente1, agente2], \"Probando el agente \" + agente1 +\" contra \"+ agente2 + \"\", cantidad, mostarDetalle)\n","\n","preparadoHacerCompetir = True\n","\n","# hace jugar\n","res = hacerCompetir(agente1, agente2, cantidad_probar, mostarDetalleJugada)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iClzkPgUrGEd","executionInfo":{"status":"ok","timestamp":1659113016677,"user_tz":180,"elapsed":682702,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"14b0b1d3-6b1f-40a0-a126-a607d16bbb96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","**  Probando el agente Humano contra DQN  **\n","    (Humano usa 'O', DQN usa 'X') \n"," Ini: \n"," \n","Indique la coordenada donde desea jugar: \n","11\n"," #1:\tO juega pos (1, 1)\n","\tX juega pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |- - - |\n","      1 |- O X |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","01\n"," #2:\tO juega pos (0, 1)\n","\tX intenta acción inválida con pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |- O - |\n","      1 |- O X |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","21\n"," #3:\tO juega pos (2, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |- O - |\n","      1 |- O X |\n","      2 |- O - |\n","        --------\n"," \n"," Fin: \n","    -> GANA O y PIERDE X\n"]}]},{"cell_type":"code","metadata":{"id":"qlwElBTZrVcX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659113140711,"user_tz":180,"elapsed":124063,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"4ba3a7a8-5ec2-448b-fc43-3ed897e9ac09","cellView":"form"},"source":["#@title Hacer competir Agentes entrenados haciendo 'Todos contra Todos' \n","incluirHumano = True #@param {type:\"boolean\"}\n","cantPartidasJugarAuto = 5 #@param {type:\"integer\"}\n","if cantPartidasJugarAuto < 1:\n","  cantPartidasJugarAuto = 1\n","\n","if preparadoHacerCompetir:\n","  # resultados\n","  dicResultados = {}\n","  # determina posibles agente 1 \n","  posiblesAg1 = []\n","  posiblesAg1.append( \"AzarPolicy\" )\n","  if DQNpolicy is not None:\n","    posiblesAg1.append( \"DQN\" )\n","  if ql_policy._QtableEntrenada:  \n","    posiblesAg1.append( \"Q-Learning\" )\n","  if incluirHumano and (HumanoPolicy is not None):\n","    posiblesAg1.append( \"Humano\" )\n","\n","  # determina posibles agente 2\n","  posiblesAg2 = []\n","  if DQNpolicy is not None:\n","    posiblesAg2.append( \"DQN\" )\n","  if ql_policy._QtableEntrenada:  \n","    posiblesAg2.append( \"Q-Learning\" )\n","  if evalAZ_env is not None:\n","    posiblesAg2.append( \"AlphaZero\" )\n","\n","  # arma fixtures\n","  for ag1 in posiblesAg1:\n","    for ag2 in posiblesAg2:\n","      if ag1 == ag2:\n","        # no hace jugar consigo mismo\n","        continue\n","      if (ag1 == \"Humano\") or (ag2 == \"Humano\"):\n","        # humano siempre juega 1 partido contra cada uno\n","        cantJugAgs = 1\n","      else:\n","        cantJugAgs = cantPartidasJugarAuto        \n","      # hace jugar \n","      res = hacerCompetir(ag1, ag2, cantJugAgs, False)\n","      # guarda los resultados ag1\n","      if ag1 not in dicResultados:\n","        dicResultados[ag1] = [cantJugAgs, res[0]]\n","      else:\n","        dicResultados[ag1][0] = dicResultados[ag1][0] + cantJugAgs\n","        dicResultados[ag1][1] = dicResultados[ag1][1] + res[0]\n","      # guarda los resultados ag2     \n","      if ag2 not in dicResultados:\n","        dicResultados[ag2] = [cantJugAgs, res[1]]\n","      else:\n","        dicResultados[ag2][0] = dicResultados[ag2][0] + cantJugAgs\n","        dicResultados[ag2][1] = dicResultados[ag2][1] + res[1]\n","\n","# Muestra tabla de resultados\n","print(\"\\n\\n\")\n","print(\"====================================================\")\n","print(\"> Tabla de Resultados: \")\n","for ag in dicResultados.keys():\n","  cantJug = round(dicResultados[ag][0], 1)\n","  cantGan = round(dicResultados[ag][1], 1)\n","  if cantJug > 0:\n","    porc = round(cantGan*100.0/cantJug, 2)\n","  else:\n","    porc = 0.0\n","  print(\"\\t \" + ag + \": ganados \" + str(cantGan) + \" de \" + str(cantJug) + \" --> \" + str(porc) + \"%\")\n","print(\"====================================================\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","**  Probando el agente AzarPolicy contra DQN  **\n","    (AzarPolicy usa 'O', DQN usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |- X - |\n","      2 |- - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |- X - |\n","      2 |O - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |O X X |\n","      1 |X X O |\n","      2 |O O O |\n","        --------\n","    -> GANA O y PIERDE X\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |O X - |\n","      2 |O - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |- X - |\n","      2 |X - O |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","= Total Partidos Ganados: \n","\t AzarPolicy con 'O': 1.0 \n","\t DQN con 'X': 4.0 *\n","\n","**  Probando el agente AzarPolicy contra Q-Learning  **\n","    (AzarPolicy usa 'O', Q-Learning usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |- X O |\n","      2 |- - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |X X O |\n","      1 |- X - |\n","      2 |O X - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |- X O |\n","      2 |O - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |X X O |\n","      1 |O X X |\n","      2 |O X O |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |- - X |\n","      2 |O O X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","= Total Partidos Ganados: \n","\t AzarPolicy con 'O': 0.0 \n","\t Q-Learning con 'X': 5.0 *\n","\n","**  Probando el agente AzarPolicy contra AlphaZero  **\n","    (AzarPolicy usa 'O', AlphaZero usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |O X - |\n","      2 |O - X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |X X X |\n","      2 |- - - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |- O X |\n","      1 |X X O |\n","      2 |X O - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |O - - |\n","      1 |X X X |\n","      2 |- - - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |- - O |\n","      1 |X X X |\n","      2 |- - - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","= Total Partidos Ganados: \n","\t AzarPolicy con 'O': 0.0 \n","\t AlphaZero con 'X': 5.0 *\n","\n","**  Probando el agente DQN contra Q-Learning  **\n","    (DQN usa 'O', Q-Learning usa 'X') \n","\n","> Episodio 1: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n","\n","> Episodio 2: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n","\n","> Episodio 3: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n","\n","> Episodio 4: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n","\n","> Episodio 5: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |X O O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n","\n","= Total Partidos Ganados: \n","\t DQN con 'O': -2.5 *\n","\t Q-Learning con 'X': -2.5 *\n","\n","**  Probando el agente DQN contra AlphaZero  **\n","    (DQN usa 'O', AlphaZero usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |X O O |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |X O O |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |X O O |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |O - X |\n","      1 |X O O |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |O O - |\n","      2 |X X X |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","= Total Partidos Ganados: \n","\t DQN con 'O': 0.0 \n","\t AlphaZero con 'X': 5.0 *\n","\n","**  Probando el agente Q-Learning contra DQN  **\n","    (Q-Learning usa 'O', DQN usa 'X') \n","\n","> Episodio 1: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |O O X |\n","      1 |- X O |\n","      2 |O - O |\n","        --------\n","\n","> Episodio 2: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |O O X |\n","      1 |- X O |\n","      2 |O - O |\n","        --------\n","\n","> Episodio 3: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |O O X |\n","      1 |- X O |\n","      2 |O - O |\n","        --------\n","\n","> Episodio 4: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |O O X |\n","      1 |- X O |\n","      2 |O - O |\n","        --------\n","\n","> Episodio 5: \n","Se finaliza por hacer más de 100!!!\n","         0 1 2 \n","        --------\n","      0 |O O X |\n","      1 |- X O |\n","      2 |O - O |\n","        --------\n","\n","= Total Partidos Ganados: \n","\t Q-Learning con 'O': -2.5 *\n","\t DQN con 'X': -2.5 *\n","\n","**  Probando el agente Q-Learning contra AlphaZero  **\n","    (Q-Learning usa 'O', AlphaZero usa 'X') \n","\n","> Episodio 1: \n","         0 1 2 \n","        --------\n","      0 |X O - |\n","      1 |X O - |\n","      2 |X X - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 2: \n","         0 1 2 \n","        --------\n","      0 |O X X |\n","      1 |- X O |\n","      2 |O X O |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 3: \n","         0 1 2 \n","        --------\n","      0 |X O - |\n","      1 |X O - |\n","      2 |X X - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 4: \n","         0 1 2 \n","        --------\n","      0 |X O - |\n","      1 |X O - |\n","      2 |X X - |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","> Episodio 5: \n","         0 1 2 \n","        --------\n","      0 |O X X |\n","      1 |- X O |\n","      2 |O X O |\n","        --------\n","    -> GANA X y PIERDE O\n","\n","= Total Partidos Ganados: \n","\t Q-Learning con 'O': 0.0 \n","\t AlphaZero con 'X': 5.0 *\n","\n","**  Probando el agente Humano contra DQN  **\n","    (Humano usa 'O', DQN usa 'X') \n"," Ini: \n"," \n","Indique la coordenada donde desea jugar: \n","11\n"," #1:\tO juega pos (1, 1)\n","\tX juega pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |- - - |\n","      1 |- O X |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","02\n"," #2:\tO juega pos (0, 2)\n","\tX juega pos (0, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |- O X |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","20\n"," #3:\tO juega pos (2, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |- O X |\n","      2 |O - - |\n","        --------\n"," \n"," Fin: \n","    -> GANA O y PIERDE X\n","\n","**  Probando el agente Humano contra Q-Learning  **\n","    (Humano usa 'O', Q-Learning usa 'X') \n"," Ini: \n"," \n","Indique la coordenada donde desea jugar: \n","11\n"," #1:\tO juega pos (1, 1)\n","\tX juega pos (0, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - - |\n","      1 |- O - |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","02\n"," #2:\tO juega pos (0, 2)\n","\tX juega pos (2, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |- O - |\n","      2 |X - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","10\n"," #3:\tO juega pos (1, 0)\n","\tX intenta acción inválida con pos (1, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |O O - |\n","      2 |X - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","12\n"," #4:\tO juega pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |X - O |\n","      1 |O O O |\n","      2 |X - - |\n","        --------\n"," \n"," Fin: \n","    -> GANA O y PIERDE X\n","\n","**  Probando el agente Humano contra AlphaZero  **\n","    (Humano usa 'O', AlphaZero usa 'X') \n"," Ini: \n"," \n","Indique la coordenada donde desea jugar: \n","11\n"," #1:\tO juega pos (1, 1)\n","\tX juega pos (0, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |- - X |\n","      1 |- O - |\n","      2 |- - - |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","22\n"," #2:\tO juega pos (2, 2)\n","\tX juega pos (0, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X - X |\n","      1 |- O - |\n","      2 |- - O |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","01\n"," #3:\tO juega pos (0, 1)\n","\tX juega pos (2, 1)\n","\n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |- O - |\n","      2 |- X O |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","11\n","Coordenada ya utilizada!\n","Indique la coordenada donde desea jugar: \n","10\n"," #4:\tO juega pos (1, 0)\n","\tX juega pos (1, 2)\n","\n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |O O X |\n","      2 |- X O |\n","        --------\n"," \n","Indique la coordenada donde desea jugar: \n","20\n"," #5:\tO juega pos (2, 0)\n","\n","         0 1 2 \n","        --------\n","      0 |X O X |\n","      1 |O O X |\n","      2 |O X O |\n","        --------\n"," \n"," Fin: \n","    -> EMPATAN O y X\n","\n","\n","\n","====================================================\n","> Tabla de Resultados: \n","\t AzarPolicy: ganados 1.0 de 15 --> 6.67%\n","\t DQN: ganados -1.0 de 21 --> -4.76%\n","\t Q-Learning: ganados 0.0 de 21 --> 0.0%\n","\t AlphaZero: ganados 15.5 de 16 --> 96.88%\n","\t Humano: ganados 2.5 de 3 --> 83.33%\n","====================================================\n"]}]}]}