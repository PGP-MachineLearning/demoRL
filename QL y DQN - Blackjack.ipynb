{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QL & DQN - Blackjack.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMRKZnvzXC++ffumP5kNeFF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"976f3dde9b8a4392ba79f0f27476bf36":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Dame Otra Carta","disabled":false,"icon":"","layout":"IPY_MODEL_a37b35cc25564aa0ab44485b5dfbdf4e","style":"IPY_MODEL_d92fd232e68c4a758efceabd7fd23818","tooltip":""}},"a37b35cc25564aa0ab44485b5dfbdf4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92fd232e68c4a758efceabd7fd23818":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"52ff142f411a4a4394b570694f804155":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Plantarse","disabled":false,"icon":"","layout":"IPY_MODEL_22a72ffb8b53441d8150b0e893b007b6","style":"IPY_MODEL_c7892e049679440f961bde0f0f7e05dc","tooltip":""}},"22a72ffb8b53441d8150b0e893b007b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7892e049679440f961bde0f0f7e05dc":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"e8843eb7d58349d59d76582186baced5":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Reiniciar Partida","disabled":false,"icon":"","layout":"IPY_MODEL_235d8993871a409fb9e2b1d64c192fa6","style":"IPY_MODEL_87b2863cd95d465f8ed10a8be741ccae","tooltip":""}},"235d8993871a409fb9e2b1d64c192fa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87b2863cd95d465f8ed10a8be741ccae":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"122781ec73164df693c4c1d416d77dcd":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_d14632828d254a258b1a480ffe6da868","msg_id":"","outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Nueva Partida:\n","\n","* Juega Agente QL #1: pide carta As -> SUMA=11\n","* Juega Agente DQN #1: pide carta 5 -> SUMA=5\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #1: pide carta Q -> SUMA=10\n"," \n","* Juega Agente QL #2: decide no pedir más cartas -> SUMA=11\n","* Juega Agente DQN #2: pide carta 3 -> SUMA=8\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #2: pide carta 7 -> SUMA=17\n"," \n","* Juega Agente DQN #3: pide carta 5 -> SUMA=13\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #3: decide no pedir más cartas -> SUMA=17\n"," \n","* Juega Agente DQN #4: decide no pedir más cartas -> SUMA=13\n","\n","> Resultados: \n","\t Agente QL: 11\n","\t Agente DQN: 13\n","\t Humano: 17\n","\t = GANA: Humano\n","\n","\n","\n","\n"]}]}},"d14632828d254a258b1a480ffe6da868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para jugar al Blackjack (21) usando primero Q-Learning y luego una red DQN\n","\n"," Basado inicialmente en los tutoriales de Tensor Flow: https://www.tensorflow.org/agents/tutorials/2_environments_tutorial"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014757753,"user_tz":180,"elapsed":91159,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"8c3105dc-d342-4f7b-cd2f-719e23c76a6d"},"source":["#@title Instalar Paquete de TF-Agents\n","##!pip install -q tf-agents\n","\n","# usar esta versión para evitar error \n","!pip install tf-agents[reverb]\n","!git clone https://github.com/tensorflow/agents.git\n","!cd agents\n","!git checkout v0.13.0\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf-agents[reverb]\n","  Downloading tf_agents-0.13.0-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.2.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.1.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[K     |████████████████████████████████| 18.3 MB 107 kB/s \n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n","Collecting dm-reverb~=0.8.0\n","  Downloading dm_reverb-0.8.0-cp37-cp37m-manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 25.3 MB/s \n","\u001b[?25hCollecting rlds\n","  Downloading rlds-0.1.4-py3-none-manylinux2010_x86_64.whl (37 kB)\n","Collecting tensorflow~=2.9.0\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 5.8 kB/s \n","\u001b[?25hRequirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (1.3.9)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (0.1.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.7.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (14.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.47.0)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.26.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.6.3)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (57.4.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 72.8 MB/s \n","\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 49.0 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (21.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-agents[reverb]) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-agents[reverb]) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.35.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.9)\n","Installing collected packages: gast, tensorflow-estimator, tensorboard, pygame, keras, flatbuffers, tf-agents, tensorflow, rlds, dm-reverb\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","Successfully installed dm-reverb-0.8.0 flatbuffers-1.12 gast-0.4.0 keras-2.9.0 pygame-2.1.0 rlds-0.1.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tf-agents-0.13.0\n","Cloning into 'agents'...\n","remote: Enumerating objects: 19500, done.\u001b[K\n","remote: Counting objects: 100% (3422/3422), done.\u001b[K\n","remote: Compressing objects: 100% (1102/1102), done.\u001b[K\n","remote: Total 19500 (delta 2407), reused 3141 (delta 2312), pack-reused 16078\u001b[K\n","Receiving objects: 100% (19500/19500), 11.86 MiB | 9.17 MiB/s, done.\n","Resolving deltas: 100% (14748/14748), done.\n","fatal: not a git repository (or any of the parent directories): .git\n","TF-Agentes instalado.\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"wJl4YsniURev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014760368,"user_tz":180,"elapsed":2730,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"d302ddb8-ea75-4800-febc-02b9c5e6d237"},"source":["#@title Cargar Librerías\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_py_environment\n","\n","from tf_agents.environments import utils\n","from tf_agents.specs import array_spec\n","\n","from tf_agents.policies import random_tf_policy\n","\n","from tf_agents.trajectories import time_step as ts\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.networks import q_network\n","from tf_agents.utils import common\n","\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","\n","tf.compat.v1.enable_v2_behavior()\n","\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Librerías cargadas.\n"]}]},{"cell_type":"markdown","source":["## Clases sobre el Problema a resolver"],"metadata":{"id":"1KXNPjrJSmh3"}},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014760953,"user_tz":180,"elapsed":592,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"a3d2b3b4-fa85-4294-f6a4-bbaa0f84be23"},"source":["#@title Definir Entorno del Problema \n","\n","# Un entorno que represente el juego podría verse así:\n","##Acciones: Tenemos dos acciones. \n","##             Acción 0: obtener una nueva carta \n","##             Acción 1: terminar la ronda actual.\n","##Observaciones: Suma de las cartas de la ronda actual.\n","##Recompensa: El objetivo es acercarse lo más posible a 21 sin pasarse, \n","##            por lo que podemos lograrlo usando la siguiente recompensa al final de la ronda: \n","##            suma_de_tarjetas - 21 si suma_de_tarjetas <= 21, de lo contrario -21\n","\n","posiblesAccionesDescrip = [\"pide carta\", \"decide no pedir más cartas\"]\n","posiblesEstadosDescrip = ['SUMA={:}'.format(x) for x in range(0, 33)]\n","posiblesCartas = ['-', 'As', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']\n","class CardGameEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(2,), dtype=np.int32, minimum=0, name='observation')\n","    self._state = 0\n","    self._episode_ended = False\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    self._state = 0\n","    self._episode_ended = False\n","    return ts.restart(np.array([self._state, 0], dtype=np.int32))\n","\n","  def _step(self, action):\n","    # aplica una acción sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n","\n","    # Aplica la acción\n","    if action == 1:\n","      # indica no seguir jugando\n","      self._episode_ended = True\n","      new_card = 0\n","    elif action == 0:\n","      # solicita una carta\n","      new_card = randint(1, 12)\n","      if new_card == 1:\n","        # el as puede valer 1 o 11\n","        if (self._state + 11) < 21:\n","          self._state = self._state + 11\n","        else:\n","          self._state = self._state + 1  \n","      elif new_card >= 10:\n","        # las figuras valen 10\n","        self._state = self._state + 10\n","      else:\n","        # el resto vale su valor\n","        self._state = self._state + new_card\n","      # se llega a un valor de 21, no se puede pedir más\n","      if self._state >= 21:\n","        self._episode_ended = True\n","    else:\n","      raise ValueError('La acción debe ser 0 o 1.')\n","\n","    # finaliza \n","    if self._episode_ended:\n","      # si finaliza\n","      # determina el reward (siempre se maximiza)\n","      # en este caso el máximo es 21!\n","      if self._state > 21:\n","        # se paso de 21, devuelve cantidad que se paso (negativo)\n","        reward = 21 - self._state \n","      else:\n","        # no se paso de 21 (devuelve a donde llegó)\n","        reward = self._state \n","      return ts.termination(np.array([self._state, new_card], dtype=np.int32), reward)\n","    else:\n","      # si no finaliza\n","      return ts.transition(\n","          np.array([self._state, new_card], dtype=np.int32), reward=0.0, discount=0.9)\n","\n","\n","print(\"Entorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y evaluación\n","train_py_env = CardGameEnv()\n","eval_py_env = CardGameEnv()\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"Entornos de entrenamiento y prueba definidos. \")\n","\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(eval_env, policy, titulo, num_episodes=1, mostrarDetalleStep=False):\n","    if num_episodes <=0:\n","      num_episodes = 1    \n","    # inicializa acumulador auxiliar \n","    cumulative_reward_total = 0.0    \n","    print(\"\\n** \", titulo, \"**\")                   \n","    for i in range(num_episodes):\n","      if num_episodes>1:\n","        print(\"\\n> Episodio \", i+1, \": \")\n","      # muesta estado inicial\n","      time_step = eval_env.reset()  \n","      ob = time_step.observation.numpy()[0]\n","      if mostrarDetalleStep:\n","        print(\" Ini: estado\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")      \n","      else:\n","        print(\" Ini: estado\", posiblesEstadosDescrip[ob[0]])      \n","      j = 1\n","      while not time_step.is_last():\n","        # la política determina la acción a realizar\n","        action_step = policy.action(time_step)\n","        time_step = eval_env.step(action_step.action)\n","        # recupera la observación y muestra el nuevo estado \n","        ob = time_step.observation.numpy()[0]\n","        if ob[1] == 0:\n","          descAccion = posiblesAccionesDescrip[1]\n","        else:\n","          descAccion = posiblesAccionesDescrip[0] + \" \" + posiblesCartas[ob[1]]\n","        if mostrarDetalleStep:\n","          print(\" #\", j, \":\", descAccion, \"-> estado:\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")\n","        else:\n","          print(\" #\" + str(j) + \":\", descAccion, \"-> estado:\", posiblesEstadosDescrip[ob[0]])\n","        j = j + 1\n","        \n","      # muestra estado final\n","      ob = time_step.observation.numpy()[0]\n","      r = time_step.reward.numpy()\n","      if mostrarDetalleStep:\n","        print(\" Fin -> estado\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")\n","      else:\n","        print(\" Fin -> estado\", posiblesEstadosDescrip[ob[0]])\n","      print(' Recompensa final = ', r[0])\n","      cumulative_reward_total += r[0]\n","    if num_episodes > 1:\n","      promedioEpisodios = round(cumulative_reward_total/num_episodes,3)\n","      print(\"\\n= Recompensa Promedio Total: \", promedioEpisodios, \"\\n\")\n","      return promedioEpisodios\n","    else:\n","      return cumulative_reward_total\n","\n","\n","# función auxiliar para comparar\n","def compararRtdosPolicy(cantidad_probar, eval_env, policy1, policy2, descPol1=\"Agente 1\", descPol2=\"Agente 2\"):\n","  prom1 = 0\n","  prom2 = 0\n","  for i in range(cantidad_probar):\n","    print(\"\\n> Prueba \", i+1, \":\")\n","    # Probar 1\n","    valor1 = SimularEntorno(eval_env, policy1, \"Resultados de \" + descPol1, False) \n","    prom1 = prom1 + valor1\n","    # Probar 2\n","    valor2 = SimularEntorno(eval_env, policy2, \"Resultados de \" + descPol2, False) \n","    prom2 = prom2 + valor2\n","    # Decide Ganador\n","    strMostrar = \"\\n--> \" + descPol1 + \" (%s) genera\" % valor1\n","    if valor1 > valor2:\n","      strMostrar = strMostrar + \" MEJOR \"\n","    elif valor1 < valor2:\n","      strMostrar = strMostrar + \" PEOR \"\n","    else:\n","      strMostrar = strMostrar + \" IGUAL \"\n","    strMostrar = strMostrar + \"resultado que \" + descPol2 + \" (%s).\" % valor2\n","    print(strMostrar)\n","  # Decide Ganador General\n","  if cantidad_probar > 0:\n","    prom1 = prom1 / cantidad_probar\n","    prom2 = prom2 / cantidad_probar\n","    print(\"\\n================================================================================================\\n\")\n","    strMostrar = \" * En Promedio \" + descPol1 + \" (%s) genera\" % prom1\n","    if prom1 > prom2:\n","      strMostrar = strMostrar + \" MEJORES \"\n","    elif prom1 < prom2:\n","      strMostrar = strMostrar + \" PEORES \"\n","    else:\n","      strMostrar = strMostrar + \" IGUALES \"\n","    strMostrar = strMostrar + \"resultados que \" + descPol2 + \" (%s).\" % prom2\n","    print(strMostrar)\n","    print(\"\\n================================================================================================\\n\")\n","\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","MostarDetalleSteps = False #@param {type:\"boolean\"}\n","\n","if Probar_Entorno:\n","  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema al azar\", 1, MostarDetalleSteps)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entorno del Problema definido.\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema al azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n"]}]},{"cell_type":"markdown","source":["##Q-Learning"],"metadata":{"id":"-Jl6zZnToIh5"}},{"cell_type":"code","source":["#@title Define clase auxiliar Policy para Q-Learning\n","\n","## funciones auxiliares basadas en:\n","##  https://rubikscode.net/2019/06/24/introduction-to-q-learning-with-python-and-open-ai-gym/\n","\n","from tf_agents.policies.fixed_policy import FixedPolicy\n","from tf_agents.trajectories import policy_step\n","from tf_agents.utils import nest_utils\n","from tf_agents.specs import tensor_spec\n","from tf_agents.typing import types\n","from typing import Optional, Text\n","\n","## funciones auxiliares para manejar TS->Obs\n","## (se puede definir otras si es necesario)\n","\n","# devuelve el primer valor de OBS \n","# como número de estado\n","def obtenerEstadoDeObs(obs):  \n","  return obs[0]\n","\n","# devuelve valor unico en base a vector OBS \n","# para determinar fila de estado\n","def generarNroEstadoVector(obs):\n","  ntarget = np.array(obs).flatten()\n","  valorTotal = 0        \n","  for i in range(1, len(ntarget)):\n","    if ntarget[i] == -1:\n","      valorTotal = valorTotal + 2**i    \n","    elif ntarget[i] == 1:\n","      valorTotal = valorTotal + 3**i      \n","  return valorTotal\n","\n","# devuelve la cantidad de estados máxima\n","# que se pueden generar usando un ejemplo \n","# del vector de OBS \n","def calcMaxNroEstadoVector(ejemploObs):\n","  ntarget = np.array(ejemploObs).flatten()  \n","  cant = len(ntarget)\n","  return ( 3**cant + 2**cant + 1 )\n","\n","\n","# Clase para Q-Learning\n","# (se hereda de FixedPolicy porque es una simple para tener como base)\n","class QL_TF_Policy(FixedPolicy):\n","    \n","  def __init__(self,\n","               posiblesEstadosList, \n","               posiblesAccionesList,\n","               time_step_spec: ts.TimeStep,            \n","               action_spec: types.NestedTensorSpec,                              \n","               funcDevuelveNroEstado = obtenerEstadoDeObs,   \n","               policy_info: types.NestedTensorSpec = (),\n","               info_spec: types.NestedTensorSpec = (),\n","               name: Optional[Text] = None):    \n","    \n","      # llama al padre\n","      super(FixedPolicy, self).__init__(time_step_spec, action_spec, clip=False,\n","                                      info_spec=info_spec,\n","                                      name=name,\n","                                      emit_log_probability=False)             \n","      # guarda valores auxiliares\n","      self._policy_info = policy_info\n","      self._time_step_spec = tensor_spec.from_spec(time_step_spec)\n","      self._action_spec = tensor_spec.from_spec(action_spec)   \n","      self._funcDevuelveNroEstado = funcDevuelveNroEstado\n","      # inicializa parametros de matriz\n","      if (posiblesEstadosList is None) or (len(posiblesEstadosList)<2):\n","       raise ValueError('No se ha definida la lista de posibles estdos!' )\n","      self._posiblesEstadosList = posiblesEstadosList\n","      self._cantEstados = len(posiblesEstadosList)\n","      if (posiblesAccionesList is None) or (len(posiblesAccionesList)<2):\n","       raise ValueError('No se ha definida la lista de posibles acciones!' )\n","      self._posiblesAccionesList = posiblesAccionesList\n","      self._cantAcciones = len(posiblesAccionesList)\n","      self.ResetQ()\n","  \n","  # funcion auxiliar para inicializar la matriz\n","  def ResetQ(self):     \n","      self._Qtable = np.zeros([self._cantEstados, self._cantAcciones])\n","      self._QtableEntrenada = False \n","\n","  # función auxiliar de entrenamiento\n","  def TrainQ(self, env, train_policy, alpha = 0.1, gamma = 0.6, epsilon = 0.1, cant_ciclos_entrenamiento = 100000, log_cada_ciclos = 1000, mostrarDetalleAcciones=False):\n","      ##print(self._Qtable.shape)\n","      ##print(self._usarOBSVector)\n","      # ejecuta el entrenamiento\n","      for step in range(1, cant_ciclos_entrenamiento+1):\n","          # Resetea el enviroment\n","          time_step = env.reset()  \n","          ob = time_step.observation.numpy()[0]\n","          state = self._funcDevuelveNroEstado(ob)\n","          secuenciaAcciones = \"\"\n","          j = 1\n","          # Simula      \n","          while not time_step.is_last():\n","              # Considera lo aprendido o toma al azar depende de azar y epsilon\n","              if random.uniform(0, 1) < epsilon:\n","                  # toma de Matriz-Q\n","                  accionAplicar = np.argmax( self._Qtable[state] )              \n","              else:\n","                  # toma al azar\n","                  action_step = train_policy.action( time_step ) \n","                  accionAplicar = action_step.action.numpy()[0]\n","              if j > 1:\n","                secuenciaAcciones = secuenciaAcciones + \" + \"\n","              secuenciaAcciones = secuenciaAcciones + self._posiblesAccionesList[ accionAplicar ]\t\n","              j = j + 1\n","              # Aplica la Accion  \n","              time_step = env.step( accionAplicar )\n","              ob = time_step.observation.numpy()[0]\n","              next_state = self._funcDevuelveNroEstado(ob)\n","              r = time_step.reward.numpy()[0] \n","              # Recalcula Q\n","              q_value = self._Qtable[state, accionAplicar]\n","              max_value = np.max(self._Qtable[next_state])\n","              new_q_value = (1 - alpha) * q_value + alpha * (r + gamma * max_value)                       \n","              # Actualiza Matriz-Q\n","              self._Qtable[state, accionAplicar] = new_q_value\n","              state = next_state        \n","          # muestra estado\n","          if log_cada_ciclos > 0:\n","            if (step == 1) or (step % log_cada_ciclos == 0):\n","              if mostrarDetalleAcciones:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) = {3} '.format(step, r, (j-1), secuenciaAcciones))  \n","              else:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) '.format(step, r, (j-1)))  \n","      # Devuelve Matriz-Q\n","      self._QtableEntrenada = True\n","      return self._Qtable   \n","\n","  # obtiene Q-Table como DataFrame\n","  def getQ_DF(self):\n","        cm = self._Qtable \n","        cmtx = pd.DataFrame(\n","            cm, \n","            index=self._posiblesEstadosList, \n","            columns=self._posiblesAccionesList\n","          )\n","        return cmtx\n","\n","  # graba Q-Table como CSV\n","  def saveQ(self, fileName):\n","        # obtiene data frame de matriz Q\n","        df = self.getQ_DF()\n","        # graba matriz Q como CSV\n","        df.to_csv(fileName, index=True)\n","        return \n","\n","  # recupera Q-Table de  CSV\n","  def loadQ(self, fileName):\n","        # lee matriz Q de CSV\n","        df = pd.read_csv(fileName)         \n","        # carga elementos en memoria\n","          # saca nombre de estados\n","        self._posiblesEstadosList = list(df.pop(df.columns[0]))  \n","          # acciones\n","        self._posiblesAccionesList = list(df.columns.tolist())\n","          # borra el primer elemento de titulo\n","        ##self._posiblesAccionesList = np.delete(self._posiblesAccionesList, 0, axis=0)      \n","         # matriz Q\n","        self._Qtable = list(df.to_numpy())\n","        # re-inicializa las cantidades\n","        self._cantEstados = len(self._posiblesEstadosList)\n","        self._cantAcciones = len(self._posiblesAccionesList)\n","        self._QtableEntrenada = True\n","        # muestra resultados\n","        self.MostrarQ(\"Matriz-Q recuperada:\")\n","        return \n","\n","  # función auxiliar para mostrar matri Q\n","  def MostrarQ(self, titulo=\"Matriz-Q entrenada:\"):\n","        # muestra Q table\n","        cmtx = self.getQ_DF()\n","        print('\\n ' + titulo + ' ')        \n","        # agrega para poder mostrar la matrix de confusión completa\n","        pd.options.display.max_rows = 100\n","        pd.options.display.max_columns = 100\n","        print(cmtx)\n","        print(\"\\n\")\n","        return \n","\n","  # devuelve la accion que se debe aplicar usando la matrix Q entrenada\n","  def _action(self, time_step, policy_state, seed):    \n","      # determina la accion a realizar\n","      # obtiene estado actual\n","      ob = time_step.observation.numpy()[0]\n","      state = self._funcDevuelveNroEstado(ob)\n","      # toma de Matriz-Q      \n","      accionAplicar = np.argmax( self._Qtable[state] )\n","      # formatea el valor a devolver usando la action_spec y time_step_spec\n","      def convert(action, spec):\n","        return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n","      self._action_value = tf.nest.map_structure(convert, accionAplicar,\n","                                                  self._action_spec)\n","      outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n","      action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n","                                   self._action_value)\n","      # devuelve la accion\n","      return policy_step.PolicyStep(action, policy_state, self._policy_info)\n","\n","\n","print(\"Clase QL_TF_Policy creada.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"vNLy2mNyi2xy","executionInfo":{"status":"ok","timestamp":1659014760954,"user_tz":180,"elapsed":16,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"89004011-34f0-4a8d-e7f9-a930bbc1cce7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase QL_TF_Policy creada.\n"]}]},{"cell_type":"code","source":["#@title Entrenar con algoritmo Q-Learning\n","\n","# parámetros\n","entrenar_QL = True #@param {type:\"boolean\"}\n","alpha = 0.1 #@param {type:\"number\"}\n","gamma = 0.6 #@param {type:\"number\"}\n","epsilon = 0.6 #@param {type:\"number\"}\n","cant_ciclos_entrenamiento_finalizar = 5000 #@param {type:\"integer\"}\n","mostar_recompensa_cada = 500  #@param {type:\"integer\"}\n","if alpha <= 0.0:\n","   alpha = 0.001\n","if gamma <= 0.0:\n","    gamma = 0.001\n","if epsilon <= 0.0:\n","   epsilon = 0.001    \n","if cant_ciclos_entrenamiento_finalizar <= 10:\n","   cant_ciclos_entrenamiento_finalizar = 10    \n","if mostar_recompensa_cada < 1:\n","  mostar_recompensa_cada = 1\n","\n","# instancia política de Q Learning\n","ql_policy = QL_TF_Policy(posiblesEstadosList = posiblesEstadosDescrip,\n","                         posiblesAccionesList = posiblesAccionesDescrip,                         \n","                         time_step_spec = train_env.time_step_spec(),\n","                         action_spec = train_env.action_spec(),\n","                         funcDevuelveNroEstado = obtenerEstadoDeObs\n","                         )\n","\n","if entrenar_QL:\n","  # hace el entrenamiento\n","  print(\"** Comienza el Entrenamiento:\\n\")\n","  ql_policy.TrainQ(env = train_env, \n","                   train_policy = random_policy, \n","                   alpha = alpha, gamma = gamma, epsilon = epsilon, \n","                   cant_ciclos_entrenamiento = cant_ciclos_entrenamiento_finalizar, \n","                   log_cada_ciclos = mostar_recompensa_cada,\n","                   mostrarDetalleAcciones = True)\n","  print(\"\\n** Entrenamiento Finalizado **\")\n","  # muestra matriz\n","  ql_policy.MostrarQ()\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente Q-Learning.\")  "],"metadata":{"cellView":"form","id":"lQZNJ9kVoDLG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017047322,"user_tz":180,"elapsed":16840,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"4bbc40ee-6eb5-43ba-b3db-c52a9f348508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** Comienza el Entrenamiento:\n","\n","step = 1: Recompensa = -9.0 - Acciones(3) = pide carta + pide carta + pide carta \n","step = 500: Recompensa = 18.0 - Acciones(3) = pide carta + pide carta + decide no pedir más cartas \n","step = 1000: Recompensa = 9.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 1500: Recompensa = 10.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 2000: Recompensa = 17.0 - Acciones(4) = pide carta + pide carta + pide carta + decide no pedir más cartas \n","step = 2500: Recompensa = 11.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 3000: Recompensa = 10.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 3500: Recompensa = 8.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 4000: Recompensa = 13.0 - Acciones(3) = pide carta + pide carta + decide no pedir más cartas \n","step = 4500: Recompensa = 9.0 - Acciones(2) = pide carta + decide no pedir más cartas \n","step = 5000: Recompensa = 20.0 - Acciones(4) = pide carta + pide carta + pide carta + decide no pedir más cartas \n","\n","** Entrenamiento Finalizado **\n","\n"," Matriz-Q entrenada: \n","         pide carta  decide no pedir más cartas\n","SUMA=0    11.941562                    7.539054\n","SUMA=1     0.000000                    0.000000\n","SUMA=2    14.513891                   10.801132\n","SUMA=3    16.483953                   12.540074\n","SUMA=4    17.897921                   14.196789\n","SUMA=5    18.742169                   15.767144\n","SUMA=6    19.501762                   17.615219\n","SUMA=7    19.948881                   18.881656\n","SUMA=8    22.847002                   21.276742\n","SUMA=9    24.547185                   22.739032\n","SUMA=10   22.751682                   25.000000\n","SUMA=11   23.048137                   27.499998\n","SUMA=12   19.628852                   29.994769\n","SUMA=13   16.766255                   32.483619\n","SUMA=14   16.146406                   34.968759\n","SUMA=15   15.259022                   37.414406\n","SUMA=16    9.047976                   39.934137\n","SUMA=17    8.835959                   42.297735\n","SUMA=18    2.842644                   44.452347\n","SUMA=19    1.666163                   45.450827\n","SUMA=20   -3.414759                   48.091603\n","SUMA=21    0.000000                    0.000000\n","SUMA=22    0.000000                    0.000000\n","SUMA=23    0.000000                    0.000000\n","SUMA=24    0.000000                    0.000000\n","SUMA=25    0.000000                    0.000000\n","SUMA=26    0.000000                    0.000000\n","SUMA=27    0.000000                    0.000000\n","SUMA=28    0.000000                    0.000000\n","SUMA=29    0.000000                    0.000000\n","SUMA=30    0.000000                    0.000000\n","SUMA=31    0.000000                    0.000000\n","SUMA=32    0.000000                    0.000000\n","\n","\n"]}]},{"cell_type":"code","source":["#@title Probar Q-Learning Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if ql_policy._QtableEntrenada:\n","  compararRtdosPolicy(cantidad_probar, eval_env, ql_policy, random_policy, \"Agente Q-Learning entrenado\", \"el Azar\")\n"],"metadata":{"cellView":"form","id":"KgQWppPm_nDd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017047968,"user_tz":180,"elapsed":673,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"fedd7390-78d2-4da9-f6e5-fb1a1835eb78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 8 -> estado: SUMA=8\n"," #2: pide carta 2 -> estado: SUMA=10\n"," #3: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta Q -> estado: SUMA=14\n"," #3: decide no pedir más cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente Q-Learning entrenado (14.0) genera MEJOR resultado que el Azar (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 6 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (13.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 4 -> estado: SUMA=9\n"," #3: pide carta 8 -> estado: SUMA=17\n"," #4: decide no pedir más cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta J -> estado: SUMA=20\n"," #3: decide no pedir más cartas -> estado: SUMA=20\n"," Fin -> estado SUMA=20\n"," Recompensa final =  20.0\n","\n","--> Agente Q-Learning entrenado (17.0) genera PEOR resultado que el Azar (20.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: decide no pedir más cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (11.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 6 -> estado: SUMA=9\n"," #3: pide carta 9 -> estado: SUMA=18\n"," #4: decide no pedir más cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 5 -> estado: SUMA=12\n"," #3: pide carta 3 -> estado: SUMA=15\n"," #4: decide no pedir más cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","--> Agente Q-Learning entrenado (18.0) genera MEJOR resultado que el Azar (15.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 4 -> estado: SUMA=9\n"," #3: pide carta 7 -> estado: SUMA=16\n"," #4: decide no pedir más cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: pide carta 7 -> estado: SUMA=17\n"," #3: pide carta 2 -> estado: SUMA=19\n"," #4: pide carta 9 -> estado: SUMA=28\n"," Fin -> estado SUMA=28\n"," Recompensa final =  -7.0\n","\n","--> Agente Q-Learning entrenado (16.0) genera MEJOR resultado que el Azar (-7.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta 6 -> estado: SUMA=15\n"," #3: decide no pedir más cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (15.0) genera MEJOR resultado que el Azar (0.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente Q-Learning entrenado (13.4) genera MEJORES resultados que el Azar (4.1).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"markdown","source":["##DQN"],"metadata":{"id":"2a6HbaIgoJL4"}},{"cell_type":"code","metadata":{"cellView":"form","id":"diEOEg3JaMHa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014767939,"user_tz":180,"elapsed":16,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"97579a0f-c8cf-4081-90f5-9bbbef4638ad"},"source":["#@title Definir el Agente tipo DQN\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","DQNpolicy = None\n","\n","if entrenar_DQN:\n","  tipo_agente = \"DQN\" #@param [\"DQN\", \"DQN Categorico (C51)\"]\n","  learning_rate = 1e-3  # @param {type:\"number\"}\n","  cant_neuronas_ocultas = \"100\" # @param {type:\"string\"}\n","  DQNCat_num_atoms = 51  # param {type:\"integer\"}\n","\n","  # controla cantidad de atoms para DQN Cat\n","  if DQNCat_num_atoms <= 1:\n","    DQNCat_num_atoms = 51\n","\n","  # Define cantidad de neuronas ocultas para RNA-Q\n","  hidden_layers = []\n","  for val in cant_neuronas_ocultas.split(','):\n","    if  int(val) < 1:\n","      hidden_layers.append( 10 )\n","    else:\n","      hidden_layers.append( int(val) )\n","  fc_layer_params = tuple(hidden_layers, )\n","\n","  if tipo_agente==\"DQN\":\n","\n","    #define las capas convolutional\n","    CNN_preprocessing_layers = None\n","\n","    # Define RNA-Q\n","    q_net = q_network.QNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.Variable(0)\n","\n","    # Define el agente de tipo Q\n","    ag = dqn_agent.DqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        q_network=q_net,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        train_step_counter=train_step_counter)\n","\n","    ag.initialize()\n","\n","    print(\"Agente DQN inicializado. \")\n","\n","  elif tipo_agente == \"DQN Categorico (C51)\":\n","    \n","    # Define RNA-Q Categórico\n","    categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        num_atoms=DQNCat_num_atoms,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.compat.v2.Variable(0)\n","    \n","    # parámetros especificos (por defecto)\n","    n_step_update = 2\n","    gamma = 0.99\n","\n","    # Define el agente de tipo Q Categórico\n","    ag = CategoricalDqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        categorical_q_network=categorical_q_net,\n","        optimizer=optimizer,\n","        n_step_update=n_step_update,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        gamma=gamma,\n","        train_step_counter=train_step_counter)\n","    \n","    ag.initialize()\n","    \n","    print(\"Agente DQN Categorico (C51) inicializado. \")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Agente DQN inicializado. \n"]}]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014777240,"user_tz":180,"elapsed":9311,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"ad260f9c-7609-497a-ec61-e88f119bf445"},"source":["#@title Métricas para evaluación y Preparar datos para Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  # Definir Métricas para evaluación para Agente DQN\n","    \n","  # Se usa el promedio de la recompensa (la más común)\n","  # See also the metrics module for standard implementations of different metrics.\n","  # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n","\n","  def compute_avg_return(environment, policy, num_episodes=10):\n","    if num_episodes == 0:\n","      return 0.0 \n","    total_return = 0.0\n","    for _ in range(num_episodes):\n","\n","      time_step = environment.reset()\n","      episode_return = 0.0\n","\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = environment.step(action_step.action)\n","        episode_return += time_step.reward\n","      total_return += episode_return\n","\n","    avg_return = total_return / num_episodes\n","    return avg_return.numpy()[0]\n","\n","  initial_collect_steps = 1000  # @param {type:\"integer\"} \n","  collect_steps_per_iteration =   50# @param {type:\"integer\"}\n","  replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","  batch_size = 64  # @param {type:\"integer\"}\n","\n","  # Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\n","  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","      data_spec = ag.collect_data_spec,\n","      batch_size = train_env.batch_size,\n","      max_length = replay_buffer_max_length)\n","\n","  # Recolecta datos generados al azar\n","  # This loop is so common in RL, that we provide standard implementations. \n","  # For more details see the drivers module.\n","  # https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n","\n","  def collect_step(environment, policy, buffer):\n","    time_step = environment.current_time_step()\n","    action_step = policy.action(time_step)\n","    next_time_step = environment.step(action_step.action)\n","    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","    # Add trajectory to the replay buffer\n","    buffer.add_batch(traj)\n","\n","  def collect_data(env, policy, buffer, steps):\n","    for _ in range(steps):\n","      collect_step(env, policy, buffer)\n","\n","  collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n","\n","  print(\"\\nDatos recolectados.\")\n","\n","  # Muestra ejemplo de los datos recolectados\n","  ##iter(replay_buffer.as_dataset()).next()\n","\n","  # Preparar los datos recolectados con trajectories de shape [Bx2x...]\n","  dataset = replay_buffer.as_dataset(\n","      num_parallel_calls=3, \n","      sample_batch_size=batch_size, \n","      num_steps=2).prefetch(3)\n","  iterator = iter(dataset)\n","  # Muestra ejemplo \n","  ##iterator.next()\n","  print(\"\\nDataset creado.\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Datos recolectados.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n","\n","Dataset creado.\n"]}]},{"cell_type":"code","source":["#@title Entrenar al Agente DQN\n","\n","if entrenar_DQN:\n","\n","  cant_ciclos_entrenamiento_finalizar =  5000# @param {type:\"integer\"}\n","  log_cada_ciclos = 200  # @param {type:\"integer\"}\n","  mostar_recompensa_cada = 500  # @param {type:\"integer\"}\n","  cant_episodios_evaluacion =  10# @param {type:\"integer\"}\n","  minima_recompensa_promedio_finalizar = 21.0 # @param {type:\"number\"}\n","  \n","  #  Optimize by wrapping some of the code in a graph using TF function (Optional)\n","  ag.train = common.function(ag.train)\n","\n","  # Reset the train step\n","  ag.train_step_counter.assign(0)\n","\n","  # Evaluate the agent's policy once before training.\n","  avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","  ar_cicloL = []\n","  ar_cicloR = []\n","  ar_returns = []\n","  ar_loss = []\n","\n","  print(\"\\n** Comienza el Entrenamiento **\\n\")\n","  for _ in range(cant_ciclos_entrenamiento_finalizar):\n","\n","    # Collect a few steps using collect_policy and save to the replay buffer.\n","    collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\n","\n","    # Sample a batch of data from the buffer and update the agent's network.\n","    experience, unused_info = next(iterator)\n","    train_loss = ag.train(experience).loss\n","\n","    step = ag.train_step_counter.numpy()\n","\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\n","      print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \n","      ar_cicloL.append( step )\n","      ar_loss.append( train_loss )\n","    \n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\n","      avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","      ar_cicloR.append( step )\n","      ar_returns.append( avg_return )\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\n","\n","      if (avg_return >= minima_recompensa_promedio_finalizar):\n","        print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \n","        break\n","\n","  DQNpolicy = ag.policy\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"metadata":{"cellView":"form","id":"LQbSlCJW8BeN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017028394,"user_tz":180,"elapsed":2251169,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"f47f3add-b8c4-44ef-dbaa-961596f92f13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","** Comienza el Entrenamiento **\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.foldr(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","step = 1: loss = 24.622\n","step = 1: Promedio Recompensa = 0.0\n","step = 200: loss = 28.070\n","step = 400: loss = 31.736\n","step = 500: Promedio Recompensa = 14.1\n","step = 600: loss = 8.072\n","step = 800: loss = 4.521\n","step = 1000: loss = 5.269\n","step = 1000: Promedio Recompensa = 13.4\n","step = 1200: loss = 4.882\n","step = 1400: loss = 14.475\n","step = 1500: Promedio Recompensa = 14.2\n","step = 1600: loss = 6.526\n","step = 1800: loss = 5.982\n","step = 2000: loss = 1.922\n","step = 2000: Promedio Recompensa = 15.2\n","step = 2200: loss = 2.982\n","step = 2400: loss = 18.821\n","step = 2500: Promedio Recompensa = 16.5\n","step = 2600: loss = 5.374\n","step = 2800: loss = 10.127\n","step = 3000: loss = 11.777\n","step = 3000: Promedio Recompensa = 13.4\n","step = 3200: loss = 13.062\n","step = 3400: loss = 8.338\n","step = 3500: Promedio Recompensa = 14.6\n","step = 3600: loss = 0.852\n","step = 3800: loss = 3.453\n","step = 4000: loss = 7.652\n","step = 4000: Promedio Recompensa = 15.1\n","step = 4200: loss = 8.276\n","step = 4400: loss = 2.012\n","step = 4500: Promedio Recompensa = 14.0\n","step = 4600: loss = 3.113\n","step = 4800: loss = 1.977\n","step = 5000: loss = 1.611\n","step = 5000: Promedio Recompensa = 15.8\n","\n","** Entrenamiento Finalizado **\n","\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"9EBBl7mRkQYa","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1659017028407,"user_tz":180,"elapsed":38,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"e2f7e7e7-ee52-41fa-c81d-a2dd4296c3a9"},"source":["#@title Mostrar Gráficos del Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloR, ar_returns)\n","  plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloR))   \n","  plt.grid(True)\n","  plt.show()\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloL, ar_loss, color=\"red\" )\n","  plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloL))   \n","  plt.grid(True)\n","  plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017029603,"user_tz":180,"elapsed":1224,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"310ba82e-bd32-4d55-ff59-ad59b635521d"},"source":["#@title Probar el Agente DQN Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if DQNpolicy is not None:\n","  compararRtdosPolicy(cantidad_probar, eval_env, DQNpolicy, random_policy, \"Agente DQN entrenado\", \"el Azar\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 2 -> estado: SUMA=5\n"," #3: pide carta 10 -> estado: SUMA=15\n"," #4: decide no pedir más cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente DQN entrenado (15.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 4 -> estado: SUMA=7\n"," #3: pide carta 9 -> estado: SUMA=16\n"," #4: decide no pedir más cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN entrenado (16.0) genera MEJOR resultado que el Azar (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: pide carta 2 -> estado: SUMA=12\n"," #3: pide carta As -> estado: SUMA=13\n"," #4: pide carta 10 -> estado: SUMA=23\n"," Fin -> estado SUMA=23\n"," Recompensa final =  -2.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir más cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente DQN entrenado (-2.0) genera PEOR resultado que el Azar (0.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: pide carta 10 -> estado: SUMA=21\n"," Fin -> estado SUMA=21\n"," Recompensa final =  21.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: decide no pedir más cartas -> estado: SUMA=6\n"," Fin -> estado SUMA=6\n"," Recompensa final =  6.0\n","\n","--> Agente DQN entrenado (21.0) genera MEJOR resultado que el Azar (6.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 4 -> estado: SUMA=8\n"," #3: pide carta 8 -> estado: SUMA=16\n"," #4: decide no pedir más cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: decide no pedir más cartas -> estado: SUMA=9\n"," Fin -> estado SUMA=9\n"," Recompensa final =  9.0\n","\n","--> Agente DQN entrenado (16.0) genera MEJOR resultado que el Azar (9.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta Q -> estado: SUMA=19\n"," #3: decide no pedir más cartas -> estado: SUMA=19\n"," Fin -> estado SUMA=19\n"," Recompensa final =  19.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: decide no pedir más cartas -> estado: SUMA=5\n"," Fin -> estado SUMA=5\n"," Recompensa final =  5.0\n","\n","--> Agente DQN entrenado (19.0) genera MEJOR resultado que el Azar (5.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: pide carta As -> estado: SUMA=17\n"," #3: decide no pedir más cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN entrenado (17.0) genera MEJOR resultado que el Azar (10.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta As -> estado: SUMA=11\n"," #3: pide carta 3 -> estado: SUMA=14\n"," #4: decide no pedir más cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 4 -> estado: SUMA=11\n"," #3: decide no pedir más cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","--> Agente DQN entrenado (14.0) genera MEJOR resultado que el Azar (11.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: pide carta 5 -> estado: SUMA=11\n"," #3: decide no pedir más cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: decide no pedir más cartas -> estado: SUMA=7\n"," Fin -> estado SUMA=7\n"," Recompensa final =  7.0\n","\n","--> Agente DQN entrenado (11.0) genera MEJOR resultado que el Azar (7.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: pide carta 6 -> estado: SUMA=17\n"," #3: decide no pedir más cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN entrenado (17.0) genera MEJOR resultado que el Azar (10.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente DQN entrenado (14.4) genera MEJORES resultados que el Azar (7.1).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"markdown","source":["## Comparar Q-Learning y DQN"],"metadata":{"id":"dxPfnPsTpQmz"}},{"cell_type":"code","source":["\n","\n","\n","#@title Cargar o Guardar los Agentes Q-Learning y DQN entrenados\n","\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-Blackjack\" #@param {type:\"string\"}\n","accion_realizar = \"Grabar Modelo\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","  import os\n","  from google.colab import drive\n","  from tf_agents.policies import TFPolicy, policy_saver\n","  # determina lugar donde se guarda el modelo\n","  policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  qlCSV = policy_dir + \"/QM-QLearning.csv\"\n","  # Montar Drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  if (DQNpolicy is not None) and isinstance(DQNpolicy, TFPolicy):\n","    # guarda la politica del agente DQN entrenado\n","    tf_policy_saver = policy_saver.PolicySaver(DQNpolicy)\n","    tf_policy_saver.save(policy_dir)\n","    print(\"\\nPolítica DQN guardada en \", policy_dir)\n","  if ql_policy is not None:\n","    if not os.path.exists(policy_dir):\n","         os.makedirs(policy_dir)\n","    ql_policy.saveQ(qlCSV)\n","    print(\"\\nPolítica Q-Learning guardada en \", qlCSV)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  DQNpolicy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica DQN recuperada de \", policy_dir)\n","  ql_policy.loadQ(qlCSV)\n","  print(\"\\nPolítica Q-Learning recuperada de \", qlCSV)\n"],"metadata":{"cellView":"form","id":"XTTpaVBfBOGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017222485,"user_tz":180,"elapsed":3329,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"52596b7c-de76-45f2-96f1-059d06391d35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\n","Política Q-Learning guardada en  /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-Blackjack/QM-QLearning.csv\n"]}]},{"cell_type":"code","source":["#@title Probar Q-Learning Entrenado contra Agente DQN Entrenado\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if DQNpolicy is not None and ql_policy._QtableEntrenada:\n","  compararRtdosPolicy(cantidad_probar, eval_env, DQNpolicy, ql_policy, \"Agente DQN\", \"Agente Q-Learning\")\n"],"metadata":{"cellView":"form","id":"JP6KZI29H47G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017029607,"user_tz":180,"elapsed":37,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"b5a8aeed-14b7-4b7a-9c62-4c521f4b7e10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta 9 -> estado: SUMA=18\n"," #3: decide no pedir más cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: decide no pedir más cartas -> estado: SUMA=7\n"," Fin -> estado SUMA=7\n"," Recompensa final =  7.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (7.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: pide carta 8 -> estado: SUMA=18\n"," #3: decide no pedir más cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta 5 -> estado: SUMA=15\n"," #3: decide no pedir más cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: decide no pedir más cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN (15.0) genera MEJOR resultado que Agente Q-Learning (10.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta J -> estado: SUMA=19\n"," #3: decide no pedir más cartas -> estado: SUMA=19\n"," Fin -> estado SUMA=19\n"," Recompensa final =  19.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: decide no pedir más cartas -> estado: SUMA=6\n"," Fin -> estado SUMA=6\n"," Recompensa final =  6.0\n","\n","--> Agente DQN (19.0) genera MEJOR resultado que Agente Q-Learning (6.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 5 -> estado: SUMA=10\n"," #3: pide carta J -> estado: SUMA=20\n"," #4: decide no pedir más cartas -> estado: SUMA=20\n"," Fin -> estado SUMA=20\n"," Recompensa final =  20.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 10 -> estado: SUMA=14\n"," #3: decide no pedir más cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","--> Agente DQN (20.0) genera MEJOR resultado que Agente Q-Learning (14.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 8 -> estado: SUMA=12\n"," #3: decide no pedir más cartas -> estado: SUMA=12\n"," Fin -> estado SUMA=12\n"," Recompensa final =  12.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 9 -> estado: SUMA=12\n"," #3: decide no pedir más cartas -> estado: SUMA=12\n"," Fin -> estado SUMA=12\n"," Recompensa final =  12.0\n","\n","--> Agente DQN (12.0) genera IGUAL resultado que Agente Q-Learning (12.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 6 -> estado: SUMA=11\n"," #3: decide no pedir más cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 8 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (11.0) genera PEOR resultado que Agente Q-Learning (13.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta As -> estado: SUMA=18\n"," #3: decide no pedir más cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: decide no pedir más cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (11.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 2 -> estado: SUMA=2\n"," #2: pide carta 9 -> estado: SUMA=11\n"," #3: pide carta J -> estado: SUMA=21\n"," Fin -> estado SUMA=21\n"," Recompensa final =  21.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 8 -> estado: SUMA=8\n"," #2: decide no pedir más cartas -> estado: SUMA=8\n"," Fin -> estado SUMA=8\n"," Recompensa final =  8.0\n","\n","--> Agente DQN (21.0) genera MEJOR resultado que Agente Q-Learning (8.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: pide carta 3 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir más cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (13.0) genera IGUAL resultado que Agente Q-Learning (13.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente DQN (16.5) genera MEJORES resultados que Agente Q-Learning (10.7).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"code","metadata":{"id":"qlwElBTZrVcX","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["976f3dde9b8a4392ba79f0f27476bf36","a37b35cc25564aa0ab44485b5dfbdf4e","d92fd232e68c4a758efceabd7fd23818","52ff142f411a4a4394b570694f804155","22a72ffb8b53441d8150b0e893b007b6","c7892e049679440f961bde0f0f7e05dc","e8843eb7d58349d59d76582186baced5","235d8993871a409fb9e2b1d64c192fa6","87b2863cd95d465f8ed10a8be741ccae","122781ec73164df693c4c1d416d77dcd","d14632828d254a258b1a480ffe6da868"]},"executionInfo":{"status":"ok","timestamp":1659017030495,"user_tz":180,"elapsed":909,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"3909fac9-f344-41e2-906a-a8782848fda0"},"source":["#@title Probar Agentes entrenados contra Usuario Humano \n","#@markdown (notar que agentes no conocen las cartas que salieron al otro)\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","def resetear():\n","  global agentesDescrip, envAgentes, nroPasoAgentes, TSAgentes\n","  # Inicializar variables\n","  envAgentes = []\n","  nroPasoAgentes = []\n","  TSAgentes = []\n","  # Crea los entornos e inicializa variables\n","  for i in range(len(agentesDescrip)):\n","    env = tf_py_environment.TFPyEnvironment( CardGameEnv() )\n","    ts = env.reset()  \n","    #ob = time_step.observation.numpy()[0]    \n","    envAgentes.append( env )\n","    nroPasoAgentes.append( 1 )\n","    TSAgentes.append( ts )\n","\n","def jugar(prefijoJugador, nombreJugador, env, paso, action):\n","  if paso < 0:\n","    # no se hace porque ya termino la partida\n","    return paso\n","  else:\n","    # aplica la acción\n","    time_step = env.step(action)\n","    # recupera la observación y muestra el nuevo estado \n","    ob = time_step.observation.numpy()[0]\n","    if ob[1] == 0:\n","      descAccion = posiblesAccionesDescrip[1]\n","    else:\n","      descAccion =posiblesAccionesDescrip[0] + \" \" + posiblesCartas[ ob[1] ]\n","    print(prefijoJugador, \"Juega\", nombreJugador, \"#\" + str(paso) + \":\", descAccion, \"->\", posiblesEstadosDescrip[ob[0]] )\n","    # ve si termino o sigue\n","    if time_step.is_last():\n","      paso = -1\n","    else:\n","      paso = paso + 1\n","    return paso, time_step\n","\n","def jugarAgentes():\n","    global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","    algunoPuedeSeguirJugando = False\n","    # inicia jugando primero a los agentes\n","    for i in range(len(agentesPolicy)):\n","      # no es humano\n","      if i != idPosHumano:\n","        # puede seguir jugando\n","        if nroPasoAgentes[i] >= 0:\n","          aux_action_step  = agentesPolicy[i].action( TSAgentes[i] )\n","          auxPaso, auxTS = jugar(\"*\", agentesDescrip[i], envAgentes[i], nroPasoAgentes[i], aux_action_step.action)\n","          nroPasoAgentes[i] = auxPaso\n","          TSAgentes[i] = auxTS\n","          algunoPuedeSeguirJugando = algunoPuedeSeguirJugando or (auxPaso >= 0)\n","    return algunoPuedeSeguirJugando\n","\n","# define funciones para los botones\n","def on_buttonR_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca botón para reiniciar la partida\n","  with output:\n","    clear_output()\n","    print(\"\\n> Nueva Partida:\\n\")\n","    # resetea ambientes\n","    resetear()\n","    # inicia jugando primero a los agentes\n","    jugarAgentes()\n","    # fuerza que le de la primera carta al humano\n","    on_button0_clicked(button0)\n","  # muestra botones\n","  display(button0, button1, buttonR, output)\n","\n","def on_button0_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca botón de pedir nueva carta\n","  with output:\n","    # juega humano\n","    if nroPasoAgentes[idPosHumano] >= 0:\n","      auxPaso, auxTS = jugar(\"+\", agentesDescrip[idPosHumano], envAgentes[idPosHumano], nroPasoAgentes[idPosHumano], np.array(0, dtype=np.int32))\n","      nroPasoAgentes[idPosHumano] = auxPaso\n","      TSAgentes[idPosHumano] = auxTS\n","    print(\" \")\n","    # juega agentes \n","    puedenAGSeguir = True\n","    while puedenAGSeguir:\n","      puedenAGSeguir = jugarAgentes()\n","      if nroPasoAgentes[idPosHumano] >= 0:\n","        # si el humano no terminó, lo hace jugar una vez solamente\n","        break\n","    # si todos finalizaron, muestra el estado final\n","    if not(puedenAGSeguir) and (nroPasoAgentes[idPosHumano] < 0):\n","      mostrarEstadoFinal()\n","\n","def on_button1_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca botón de finalizar partida (plantarse)\n","  with output:\n","    # juega humano\n","    if nroPasoAgentes[idPosHumano] >= 0:\n","      auxPaso, auxTS = jugar(\"+\", agentesDescrip[idPosHumano], envAgentes[idPosHumano], nroPasoAgentes[idPosHumano], np.array(1, dtype=np.int32))\n","      nroPasoAgentes[idPosHumano] = auxPaso\n","      TSAgentes[idPosHumano] = auxTS\n","    print(\" \")\n","    # juega agentes hasta que terminen\n","    puedenAGSeguir = True\n","    while puedenAGSeguir:\n","      puedenAGSeguir = jugarAgentes()\n","    # muesta el estado final\n","    mostrarEstadoFinal()\n","    print(\"\\n\")\n","\n","def mostrarEstadoFinal():\n","    global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes        \n","    # muestra estado final\n","    print(\"\\n> Resultados: \")\n","    rewL = []\n","    for i in range(len(agentesDescrip)):\n","      r = int( TSAgentes[i].reward.numpy()[0] )\n","      rewL.append( r )\n","      resul = \"%s\" % r\n","      if (r > 21) or (r < 0):\n","        resul = resul + \"!\"\n","      print(\"\\t \" + agentesDescrip[i] + \": \" + resul)\n","    print(\"\\t = GANA: \"  + agentesDescrip[ np.argmax(rewL) ])  \n","    print(\"\\n\")\n","    return\n","\n","\n","# inicializa variables auxiliares\n","idPosHumano = 2\n","agentesDescrip = []\n","agentesPolicy = []\n","# ql\n","if ql_policy._QtableEntrenada:\n","  agentesDescrip.append( \"Agente QL\" )\n","  agentesPolicy.append( ql_policy )\n","# dqn\n","if DQNpolicy is not None:\n","  agentesDescrip.append( \"Agente DQN\" )\n","  agentesPolicy.append( DQNpolicy )\n","# humano\n","agentesDescrip.append( \"Humano\" )\n","agentesPolicy.append( None )\n","resetear()\n","\n","# define botones\n","button0 = widgets.Button(description=\"Dame Otra Carta\")\n","button1 = widgets.Button(description=\"Plantarse\")\n","buttonR = widgets.Button(description=\"Reiniciar Partida\")\n","output = widgets.Output()\n","buttonR.on_click(on_buttonR_clicked)\n","button1.on_click(on_button1_clicked)\n","button0.on_click(on_button0_clicked)    \n","\n","# comienza a jugar\n","on_buttonR_clicked(buttonR)\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}}]}]}