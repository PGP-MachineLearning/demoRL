{"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo con ambiente Gymnasium para jugar juegos de Atari usando una red DQN \n","\n","Nota: debido a que TF-Agents todavía no es compatbile con Gymnasium no se usa y se genera la red usando Keras puro\n","\n","Basado en los tutoriales: \n","\n","  https://farama.org/Announcing-The-Farama-Foundation \n","  \n","  https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t\n","  \n","  https://keras.io/examples/rl/deep_q_network_breakout/ \n","\n","  https://github.com/moduIo/Deep-Q-network/blob/master/DQN.ipynb\n","\n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0rB1g-ioscoR"},"outputs":[],"source":["#@title Instalar Paquete Gymnasium para acceder a juegos Atari\n","!pip install gymnasium[atari,accept-rom-license]     \n","print(\"Gymnasium para ATARI instalado.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wJl4YsniURev"},"outputs":[],"source":["#@title Cargar Librerías\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","from collections import deque\n","import csv\n","\n","print(\"Librerías cargadas.\")"]},{"cell_type":"markdown","metadata":{"id":"TilJ4dT4SdtK"},"source":["## Entorno para Juego de Atari"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6_hXvrt6UQW_"},"outputs":[],"source":["#@title Preparar funciones auxiliares para visualizar juegos Atari\n","\n","import gymnasium as gym\n","from gym import logger as gymlogger\n","from gym.wrappers import RecordVideo\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","import os\n","from IPython import display as ipythondisplay\n","\n","##from pyvirtualdisplay import Display\n","##display = Display(visible=0, size=(1400, 900))\n","##display.start()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","def show_env_video(env):\n","  # trata de encontrar el video que corresponde\n","  encuentraVideo = False\n","  mp4 = './EnvVideos/' + env.spec.kwargs[\"game\"] + '-episode-' + str(env.episode_id-1) + '.mp4'  \n","  mp4list = glob.glob(mp4)\n","  if len(mp4list) == 1:\n","    encuentraVideo = True\n","  else:\n","    mp4list = glob.glob('./EnvVideos/*.mp4')\n","    if len(mp4list) > 0:\n","      # toma el último video generado para el juego\n","      mp4list.sort(reverse=True, key=os.path.getmtime)\n","      mp4 = mp4list[0]\n","      encuentraVideo = True\n","  if encuentraVideo:\n","    print(\"Video: \", mp4)\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"No se encuentra video \" + mp4 + \" del juego!\")\n","    \n","def func_episode_trigger(ep):\n","  # siempre graba\n","  return True\n","\n","def wrap_env_recorder(env):\n","  env = RecordVideo(env, \n","                    video_folder = './EnvVideos', \n","                    episode_trigger = func_episode_trigger, \n","                    video_length = 0,\n","                    name_prefix = env.spec.kwargs[\"game\"])\n","  return env\n","\n","\n","print(\"\\nWrapper para generar video preparado.\")\n","\n","def simular_entorno(env, agente, mostrarRecompensa=True, num_episodes=1, mostrar_video=True):\n","  for i in range(num_episodes):\n","    if num_episodes > 1:\n","      print(\"Generando episodio \" + str(i+1) + \"...\")\n","    else:\n","      print(\"Generando...\")\n","    # inicia entorno\n","    observation, info = env.reset(seed=123)\n","    sumR = 0.00\n","    termino = False\n","    step = 0\n","    while not termino:\n","      # si tiene acción inicial la ejecuta\n","      if (step == 0) and (env_start_action is not None):\n","        action_step = env_start_action\n","      else:\n","        # sino usa la acción del agente directamente\n","        action_step = agente.action(observation, info)\n","      # aplica la acción\n","      observation, step_reward, termino, info = env.step(action_step)\n","      sumR += step_reward\n","      step += 1\n","    # muestra recompensa\n","    if mostrarRecompensa:\n","      rFinal = step_reward\n","      if num_episodes > 1:\n","        print(\"Recompensa Acumulada del episodio \" + str(i+1) + \": \", sumR)\n","        ##print(\"Recompensa Final del episodio \" + str(i+1) + \": \", rFinal)\n","      else:\n","        print(\"Recompensa Acumulada: \", sumR)\n","        ##print(\"Recompensa Final: \", rFinal)\n","    if mostrar_video:\n","      show_env_video(env)\n","  return \n","\n","print(\"\\nFunción para simular entorno definida.\")\n","\n","# Clase para Agente que juega al azar \n","class randomAgentClass():\n","\n","  def __init__(self, observation_space, action_space):\n","    self._action_space = action_space\n","  \n","  def action(self, observation=None, info=None): \n","      # devuelve valor al azar\n","      return self._action_space.sample()\n","\n","print(\"\\nClase randomAgentClass definida.\")     \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-jibiVExeuhR"},"outputs":[],"source":["#@title Selecciona el juego de Atari\n","seleccionaJuego = \"Breakout\" #@param [\"Pong\", \"Freeway\", \"Enduro\", \"Asteroids\", \"Breakout\", \"Space Invaders\"]\n","#@markdown Ver informacion en https://www.gymlibrary.dev/environments/atari/complete_list/\n","tipoObsTS = \"Grayscale Screen\" #@param [\"Game RAM\", \"Grayscale Screen\", \"RGB Screen\"]\n","entornoDeterministico = True #@param{type:\"boolean\"}\n","\n","# selecciona juego\n","env_start_action = None\n","if seleccionaJuego == \"Freeway\":\n","  gym_env_name = 'ALE/Freeway-v5'\n","elif seleccionaJuego == \"Enduro\":\n","  gym_env_name = 'ALE/Enduro-v5'\n","elif seleccionaJuego == \"Pong\":\n","  gym_env_name = 'ALE/Pong-v5'\n","elif seleccionaJuego == \"Asteroids\":\n","  gym_env_name = 'ALE/Asteroids-v5'\n","elif seleccionaJuego == \"Breakout\":\n","  gym_env_name = 'ALE/Breakout-v5'\n","  env_start_action = 1 # debe disparar la pelotita\n","elif seleccionaJuego == \"Space Invaders\":\n","  gym_env_name = 'ALE/SpaceInvaders-v5'\n","else:\n","  raise ValueError(\"No se puede defnir gym_env_name!!!\")\n","\n","# determina tipo de OBS\n","if tipoObsTS == \"Game RAM\":\n","  obsType = 'ram'\n","elif tipoObsTS == \"RGB Screen\":\n","  obsType = 'rgb'\n","elif tipoObsTS == \"Grayscale Screen\":\n","  obsType = 'grayscale'\n","else:\n","  raise ValueError(\"No se puede defnir obsType!!!\")  \n","\n","\n","# función para inicializar juego\n","def inicializar_gym_env(gym_env_name, obs_type, entornoDeterministico=False):\n","    if entornoDeterministico:\n","      # crea el entorno con parámetros deterministicos\n","      env = gym.make(gym_env_name, \n","                                obs_type = obsType, \n","                                frameskip = 1,\n","                                repeat_action_probability = False,\n","                                full_action_space = False,\n","                                render_mode = 'rgb_array') \n","    else:\n","       # crea el entorno con parámetros estocásticos\n","      env = gym.make(gym_env_name, \n","                                obs_type = obsType, \n","                                full_action_space = False,\n","                                render_mode = 'rgb_array')   \n","    env.metadata['render_fps'] = 30\n","    return env\n","\n","\n","\n"," # crea el entorno para entrenar (no graba video)\n","train_atari_env = inicializar_gym_env(gym_env_name, \n","                                      obsType, \n","                                      entornoDeterministico\n","                                      )\n","\n","# crea el entorno para probar (graba video )\n","eval_atari_env = wrap_env_recorder( \n","                    inicializar_gym_env(gym_env_name, \n","                                        obsType,\n","                                        entornoDeterministico\n","                                        )\n","                )\n","\n","    \n","# muesta información del entorno\n","print(\"\\n\")\n","print('- Entorno: ', gym_env_name)\n","print('\\n- Specification:')\n","for det in train_atari_env.spec.kwargs:\n","  print(\"  \", det, \"=\", train_atari_env.spec.kwargs[det])\n","#print(\"   max_episode_steps=\", train_atari_env._max_episode_steps)\n","print('\\n- Observation space:')\n","print(\"  \", train_atari_env.observation_space)\n","print('\\n-Action space:')\n","print(\"  \", train_atari_env.action_space)\n","print('\\n-Reward range:')\n","print(\"  \", train_atari_env.reward_range)\n","\n","# inicializa agente Random para el ambiente\n","randomAg = randomAgentClass(train_atari_env.observation_space, train_atari_env.action_space)\n","\n","# muestra pantalla ejemplo\n","print(\"\\n-Ejemplo pantalla: \")\n","train_atari_env.reset()\n","import PIL.Image\n","PIL.Image.fromarray(train_atari_env.render())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPf8a-KIj42z","cellView":"form"},"outputs":[],"source":["#@title Ejemplo de juego jugando al Azar\n","simular_entorno(eval_atari_env, randomAg, True, 1, True)"]},{"cell_type":"markdown","metadata":{"id":"3iip7y2pJI-b"},"source":["##DQN"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"diEOEg3JaMHa"},"outputs":[],"source":["#@title Definir el Agente tipo DQN\n","\n","#@markdown ### Parámetros generales:\n","agent_max_memoria = 1000 #@param {type:\"integer\"}\n","        # 100000\n","        \n","#@markdown ### Parámetros de las capas ConvNet:\n","convNet_usar_DeepMind_config = True #@param {type:\"boolean\"}\n","convNet_tamaño_kernel_N =  3 #@param {type:\"integer\"}\n","convNet_tamaño_pooling_M = 3 #@param {type:\"integer\"}\n","convNet_cantidad_capas_ocultas =  3#@param {type:\"integer\"}\n","\n","#@markdown ### Parámetros de las capas Lineales:\n","lineal_cant_neuronas_capas_ocultas = '512' #@param {type:\"string\"}\n","#@markdown (Nota: se puede indicar Cantidad de neuronas, D para DropOut, BN para BatchNormalization)\n","lineal_porc_capa_DropOut = 0.4 #@param {type:\"number\"}\n","\n","\n","# diccionario auxiliar para pasar configuracion QNetwork\n","config_q_network = {}\n","\n","# tamaño de los kernels y pooling (para simplificar son todas iguales)\n","if convNet_usar_DeepMind_config:\n","  # usa red definido en paper original de DeepMind\n","  config_q_network[\"cnn_deepmind_config\"] = True\n","else:\n","  # usa red con configuración definida por parámetros\n","  config_q_network[\"cnn_deepmind_config\"] = False\n","  if convNet_tamaño_kernel_N<1:\n","    convNet_tamaño_kernel_N = 1\n","  config_q_network[\"cnn_kernel_shape\"] = (convNet_tamaño_kernel_N)\n","  if convNet_tamaño_pooling_M<0:\n","    convNet_tamaño_pooling_M=0\n","  config_q_network[\"cnn_pooling_shape\"] = (convNet_tamaño_pooling_M)\n","\n","  # indica la configuración para la parte Encoder \n","  #   (cada elemento de las listas son la configuración de las capas Conv)\n","  if convNet_cantidad_capas_ocultas<1:\n","    convNet_cantidad_capas_ocultas = 1\n","  cnn_filters = []\n","  for i in range(convNet_cantidad_capas_ocultas, 0, -1):\n","    cnn_filters.append( 2**(i+2) )\n","  config_q_network[\"cnn_filters\"] = cnn_filters\n","\n","  # chequea configuración de drop out\n","  if lineal_porc_capa_DropOut <= 0:\n","    lineal_porc_capa_DropOut = 0.10\n","  elif lineal_porc_capa_DropOut > 0.9:\n","      lineal_porc_capa_DropOut = 0.9\n","  config_q_network[\"lineal_porc_capa_DropOut\"] = lineal_porc_capa_DropOut\n","\n","  # cantidad de neuronas ocultas \n","  hidden_layers = []\n","  for val in lineal_cant_neuronas_capas_ocultas.split(','):\n","    val = val.strip()\n","    if val == \"D\":\n","      hidden_layers.append( \"DropOut\" )  \n","    elif val == \"BN\":\n","      hidden_layers.append( \"BatchNormalization\" )  \n","    elif val.isnumeric():\n","      hidden_layers.append( val )\n","    else:\n","      print(\"Capa \", val, \"descartada!\")  \n","  config_q_network[\"hidden_layers\"] = hidden_layers\n","\n","# Define clase de Agente DQN \n","class dqnAgentClass():\n","\n","  def __init__(self, observation_space, action_space, config_q_network, agent_memory):\n","    self._action_space = action_space\n","    self._observation_space = observation_space\n","    self._num_actions = self._action_space.n\n","    # Hyperparameters\n","    self.gamma = 1.0            # Discount rate\n","    self.epsilon = 1.0          # Exploration rate\n","    self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n","    self.epsilon_decay = 0.995  # Decay rate for epsilon\n","    self.update_rate = 1000     # Number of steps until updating the target network\n","    # memory collection\n","    self.memory = deque(maxlen=max(agent_memory, 1000))\n","    # Construct DQN models      \n","    self._config_q_network = config_q_network\n","    self.create_q_models()\n","\n","\n","  def create_q_models(self):      \n","    if self._config_q_network[\"cnn_deepmind_config\"]:          \n","        # crea modelos usando arquitectura de paper de Deepmind\n","        self.q_model = self._create_q_model_deepmind(\"Q-Model\",\n","                                    self._observation_space.shape, \n","                                    self._action_space.n)\n","    else:\n","        # crea modelos usando arquitectura a partir de configuración de usuario\n","        self.q_model = self._create_q_model_custom(\"Q-Model\",\n","                                    self._observation_space.shape, \n","                                    self._action_space.n)\n","    \n","    # copia model para reward model_target\n","    self.target_model = tf.keras.models.clone_model(self.q_model)\n","    self.target_model._name = \"Reward-Model\"\n","    self.target_model.set_weights(self.q_model.get_weights())      \n","\n","    # determina optimizer para modelos\n","    _optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n","    self.q_model.compile(loss='mse', optimizer=_optimizer)\n","    ##self.target_model.compile(loss='mse', optimizer=_optimizer)\n","\n","    # muestra uno de los dos modelos creados (tiene la misma estructura)\n","    self.q_model.summary()\n","\n","  def _prepare_obsState(self, obsState, haceArray=True):      \n","    normObs = (obsState-128)/127.0\n","    if haceArray:\n","      return np.array( [ normObs ], dtype=\"float32\" )\n","    else:\n","      return normObs\n","\n","  def _determine_obs_matrixShape(self, inputShape):\n","      # determina la forma en matriz para capa reshape\n","      if len(inputShape)==1:\n","        cols = inputShape[0]\n","        if (cols%16)==0:\n","          matrixShape = [16, (cols//16)]\n","        elif (cols%3)==0:\n","          matrixShape = [3, (cols//3)]        \n","        elif (cols%2)==0:\n","          matrixShape = [2, (cols//2)]\n","        else:\n","          matrixShape = [1, cols]        \n","        return matrixShape\n","      else:\n","        return inputShape\n","\n","  def _create_q_model_deepmind(self, modelName, inputShape, num_actions):      \n","      # Network defined by the Deepmind paper\n","      \n","      # capa de entrada\n","      inputLay = tf.keras.layers.Input(shape=inputShape, name=\"input\")\n","      eachLay = inputLay\n","\n","      # agrega capas conv según corresponda\n","      if len(inputShape)==1:\n","        # ajusta la forma de entrada para poder usar capas conv1D           \n","          eachLay = tf.keras.layers.Reshape(self._determine_obs_matrixShape(inputShape), name=\"reshapeRAM\")(eachLay)\n","          # agrega solo 1 capa convolution para RAM\n","          eachLay = tf.keras.layers.Conv1D(32, 8, strides=4, activation=\"relu\", name=\"conv_1\")(eachLay)        \n","      elif len(inputShape)>2:\n","        # Convolutions on the frames on the RGB screen\n","        eachLay = tf.keras.layers.Conv2D(32, 8, strides=4, activation=\"relu\", name=\"conv_1\")(eachLay)\n","        eachLay = tf.keras.layers.Conv2D(64, 4, strides=2, activation=\"relu\", name=\"conv_2\")(eachLay)\n","        eachLay = tf.keras.layers.Conv2D(64, 3, strides=1, activation=\"relu\", name=\"conv_3\")(eachLay)\n","      else:         \n","        # Convolutions on the frames on the GrayScale screen\n","        eachLay = tf.keras.layers.Conv1D(32, 8, strides=4, activation=\"relu\", name=\"conv_1\")(eachLay)\n","        eachLay = tf.keras.layers.Conv1D(64, 4, strides=2, activation=\"relu\", name=\"conv_2\")(eachLay)\n","        eachLay = tf.keras.layers.Conv1D(64, 3, strides=1, activation=\"relu\", name=\"conv_3\")(eachLay)\n","\n","      # capas flatten y lineal\n","      eachLay = tf.keras.layers.Flatten(name=\"flat\")(eachLay)\n","      eachLay = tf.keras.layers.Dense(512, activation=\"relu\", name=\"lineal\")(eachLay)\n","\n","      # capa de salida\n","      outputLay = tf.keras.layers.Dense(num_actions, activation=\"linear\", name=\"output\")(eachLay)\n","      \n","      # devuelve el modelo creado            \n","      mod = tf.keras.Model(name=modelName+\"_DeepMind\", inputs=inputLay, outputs=outputLay)\n","      return mod\n","\n","  def _create_q_model_custom(self, modelName, inputShape, num_actions):      \n","      # Red definida por configuración\n","\n","      # capa de entrada\n","      inputLay = tf.keras.layers.Input(shape=inputShape, name=\"input\")\n","      eachLay = inputLay\n","      \n","      # agrega capas conv según corresponda\n","      if len(inputShape)==1:\n","        # ajusta la forma de entrada para poder usar capas conv1D           \n","          eachLay = tf.keras.layers.Reshape(self._determine_obs_matrixShape(inputShape), name=\"reshapeRAM\")(eachLay)\n","\n","      auxName = 'conv_'\n","      for i in range(len(self._config_q_network[\"cnn_filters\"])):  \n","\n","          # define el nombre de la capa oculta\n","          auxlayerName = 'conv_'+str(i+1)\n","\n","          # agrega las capas ocultas de tipo Conv2D \n","          if len(inputShape)>2:\n","            eachLay =  tf.keras.layers.Conv2D(self._config_q_network[\"cnn_filters\"][i], self._config_q_network[\"cnn_kernel_shape\"], activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","          else:\n","            eachLay =  tf.keras.layers.Conv1D(self._config_q_network[\"cnn_filters\"][i], self._config_q_network[\"cnn_kernel_shape\"], activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","          # determina nombre y shape de la capa conv2D\n","          last_conv_layer_name = 'c_'+auxlayerName\n","          if self._config_q_network[\"cnn_pooling_shape\"] > 0:\n","            # sino no agrega capa MaxPooling \n","            if len(inputShape)>2:\n","              eachLay =  tf.keras.layers.MaxPooling2D(self._config_q_network[\"cnn_pooling_shape\"], padding='same', name='p_'+auxlayerName)(eachLay)\n","            else:\n","              eachLay =  tf.keras.layers.MaxPooling1D(self._config_q_network[\"cnn_pooling_shape\"], padding='same', name='p_'+auxlayerName)(eachLay)\n","\n","      #  agrega capa Flatten \n","      eachLay = tf.keras.layers.Flatten(name='flat')(eachLay)\n","\n","      # agrega capas lineales\n","      auxName = 'lineal_'\n","      auxId = 1 \n","      for val_hid in self._config_q_network[\"hidden_layers\"]:  \n","\n","        if val_hid == \"DropOut\":\n","          auxlayerName = \"d_\"+str(auxId)\n","          auxId = auxId + 1\n","          eachLay =  tf.keras.layers.Dropout(self._config_q_network[\"lineal_porc_capa_DropOut\"], name=auxlayerName)(eachLay)\n","        elif val_hid == \"BatchNormalization\":\n","          auxlayerName = \"bn_\"+str(auxId)\n","          auxId = auxId + 1\n","          eachLay =  tf.keras.layers.BatchNormalization(name=auxlayerName)(eachLay)\n","        elif val_hid.isnumeric():\n","          # agrega la capa oculta\n","          auxlayerName = auxName+str(auxId)\n","          auxId = auxId + 1\n","          eachLay =  tf.keras.layers.Dense(int(val_hid), name=auxlayerName)(eachLay) # capas ocultas\n","\n","      # capa de salida\n","      outputLay = tf.keras.layers.Dense(num_actions, activation=\"linear\", name=\"output\")(eachLay)\n","      \n","      # devuelve el modelo creado            \n","      mod = tf.keras.Model(name=modelName+\"_custom\", inputs=inputLay, outputs=outputLay)\n","      #mod.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n","      return mod\n","\n","\n","  #\n","  # Chooses action based on epsilon-greedy policy\n","  #\n","  def train_act(self, state):\n","      # Use epsilon-greedy for exploration\n","      if np.random.rand() <= self.epsilon:\n","          # Random exploration\n","          return random.randrange(self._action_space.n)\n","      else:\n","          # Predict action Q-values using model\n","          input_data = self._prepare_obsState(state)\n","          #input_data = state\n","          act_values = self.q_model.predict(input_data, verbose=0)            \n","          # Returns action using policy\n","          return np.argmax(act_values[0])  \n","\n","  #\n","  # Stores experience in replay memory\n","  #\n","  def remember(self, state, action, reward, next_state, done):\n","      # prepara estados \n","      #  (así no hace falta prepararlo cada vez que hace replay)\n","      state = self._prepare_obsState(state, False)\n","      next_state =  self._prepare_obsState(next_state, False)\n","      # guarda en memoria\n","      self.memory.append((state, action, reward, next_state, done))\n","\n","  #\n","  # Trains the model using randomly selected experiences in the replay memory\n","  #\n","  def replay(self, batch_size): \n","      # determina ejemplos para re-entrenar\n","      rndindices = np.random.choice(range(len(self.memory)), size=batch_size)\n","\n","      # obtiene datos de ejemplos\n","      sample_next_states = []\n","      sample_actions = []\n","      sample_rewards = []\n","      sample_done = []\n","      sample_states = []\n","      for i in rndindices:\n","          # self.memory structure: (state, action, reward, next_state, done))\n","          auxMem = self.memory[i]\n","          sample_states.append( auxMem[0] )\n","          sample_actions.append(  auxMem[1] )\n","          sample_rewards.append(  auxMem[2] )\n","          sample_next_states.append( auxMem[3] )            \n","          sample_done.append( auxMem[4] )\n","      sample_states = np.array( sample_states )\n","      sample_actions = np.array( sample_actions )\n","      sample_rewards = np.array( sample_rewards )\n","      sample_next_states = np.array(sample_next_states)            \n","      sample_done = np.array( sample_done )\n","\n","      # ejecuta los modelos (una con todos los datos)\n","\n","      # determina prediccion modelo target para reward\n","      model_rewards = self.target_model.predict(sample_next_states, verbose=0)\n","      \n","      # Use the current model to output the Q-value predictions\n","      model_qvalues = self.q_model.predict(sample_states, verbose=0)\n","      \n","      train_qvalues = []\n","      for i in range(batch_size):\n","\n","          if not sample_done[i]:\n","              pred_reward = np.amax(model_rewards[i])\n","              target = sample_rewards[i] + self.gamma * pred_reward\n","          else:\n","              target = sample_rewards[i]\n","        \n","          # 2. Rewrite the chosen action value with the computed target\n","          model_qvalues[i][sample_actions[i]] = target\n","                      \n","          # 3. Use vectors in the objective computation\n","          train_qvalues.append( model_qvalues[i] )\n","      \n","      # reentrena       \n","      train_qvalues = np.array( train_qvalues )\n","      self.q_model.fit(sample_states, train_qvalues, epochs=1, verbose=0)\n","          \n","      # libera memoria\n","      del sample_next_states\n","      del sample_actions\n","      del sample_rewards\n","      del sample_done\n","      del sample_states\n","      del train_qvalues\n","\n","      # degrada epsilon\n","      if self.epsilon > self.epsilon_min:\n","          self.epsilon *= self.epsilon_decay\n","\n","  #\n","  # Sets the target model parameters to the current model parameters\n","  #\n","  def update_target_model(self):\n","      self.target_model.set_weights(self.q_model.get_weights())\n","          \n","  #\n","  # Loads a saved model\n","  #\n","  def load(self, name):\n","      #self.q_model.load_weights(name)\n","      if \".h5\" not in name:\n","        name = name + \".h5\"\n","      self.q_model = tf.keras.models.load_model( name )\n","      # copia model para reward model_target\n","      self.target_model = tf.keras.models.clone_model(self.q_model)\n","      self.target_model._name = \"Reward-Model\"\n","      self.target_model.set_weights(self.q_model.get_weights())      \n","      # carga epsilon\n","      with open(name + \".csv\", 'r') as f:\n","            reader = csv.reader(f)\n","            for row in reader:\n","              self.epsilon = float(row[1])\n","      f.close()      \n","\n","  #\n","  # Saves parameters of a trained model\n","  #\n","  def save(self, name):\n","      # graba modelo\n","      if \".h5\" not in name:\n","        name = name + \".h5\"\n","      self.q_model.save(name, save_format=\"h5\")\n","      # graba epsilon\n","      with open(name + \".csv\", 'w') as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"epsilon\", self.epsilon])      \n","      f.close()\n","\n","  def action(self, observation, info): \n","      # prepara datos entrada para modelo\n","      input_data = self._prepare_obsState(observation) ##np.array( [observation] )\n","      # devuelve valor determinado por el modelo QNetwork\n","      pred_modelo = self.q_model.predict(input_data, verbose=0)        \n","      # utiliza el ID acción con mayor valor Q\n","      accionModelo = np.argmax(pred_modelo[0])\n","      #print(predModelo, predModelo[0], accionModelo)\n","      return accionModelo\n","\n","print(\"\\nClase dqnClass definida.\")   \n","\n","# inicializa Agente DQN\n","dqnAg = dqnAgentClass(train_atari_env.observation_space, \n","                      train_atari_env.action_space, \n","                      config_q_network,\n","                      agent_max_memoria)\n","\n","print(\"\\nAgente DQN inicializado. \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRgOztpbYQf1","cellView":"form"},"outputs":[],"source":["#@title Entrenar al Agente DQN\n","import gc\n","import time\n","\n","#@markdown Parámetros del entrenamiento:\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","  # 300    \n","train_episodes = 10 #@param {type:\"integer\"}\n","train_batch_size = 16 #@param {type:\"integer\"}\n","min_recompensa_promedio_finalizar = 10.0 #@param {type:\"number\"}\n","mostrar_estado_cada_steps = 200\n","\n","#@markdown Parámetros para grabar/recuperar modelo DQN:\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"keras-Atari\" #@param {type:\"string\"}\n","recuperar_modelo_entrenado = True #@param {type:\"boolean\"}\n","grabar_modelo_mientras_entrena = True #@param {type:\"boolean\"}\n","\n","if recuperar_modelo_entrenado or grabar_modelo_mientras_entrena:\n","  import os\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  # determina lugar donde se guarda el modelo\n","  dqn_modelo_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  dqn_modelo_dir = dqn_modelo_dir + \"/\" + seleccionaJuego \n","  if \".h5\" not in dqn_modelo_dir:\n","    dqn_modelo_dir = dqn_modelo_dir + \".h5\"\n","  if recuperar_modelo_entrenado:\n","    if os.path.isfile(dqn_modelo_dir):\n","      # recupera un modelo anterior grabado\n","      dqnAg.load(dqn_modelo_dir)\n","      #dqnAg.q_model.summary()\n","      print(\"\\n++ Modelo recuperado de \" + dqn_modelo_dir + \" ++\")\n","    else:\n","      print(\"\\n++ No se encuentra modelo grabado en \" + dqn_modelo_dir + \"!!! ++\\n\")\n","\n","if entrenar_DQN:\n","\n","  # variables auxiliares\n","  total_step = 0   # Counter for total number of steps taken\n","  all_rewards = 0  # Used to compute avg reward over time\n","  done = False\n","  avg_reward = -1  \n","\n","  # usa el ambiente de training que no genera videos\n","  env = train_atari_env\n","\n","  print(\"\\n** Comienza Entrenamiento **\")\n","\n","  for e in range(1, train_episodes+1):\n","      total_reward = 0\n","      game_score = 0\n","      state, info = env.reset(seed=123)\n","          \n","      print(\"\\ntotal steps: {} * comienza episode {}/{} \" \n","            .format(total_step, e, train_episodes))\n","\n","      for step in range(1, 20001):\n","          #env.render()\n","          total_step += 1\n","          \n","          # Every update_rate timesteps we update the target network parameters\n","          if (total_step % dqnAg.update_rate) == 0:\n","              dqnAg.update_target_model()\n","\n","          if (step <= 1) and (env_start_action is not None):\n","              # si tiene acción inicial la ejecuta\n","              action = env_start_action\n","          else:\n","              # ejecuta la acción del agente\n","              # Transition Dynamics\n","              action = dqnAg.train_act(state)\n","\n","          # ejecuta la acción\n","          next_state, reward, done, truncated, _ = env.step(action)\n","          \n","          # termino realmente el juego\n","          # o se forzo la terminación por máximos pasos\n","          terminated = done or truncated\n","                    \n","          # Store sequence in replay memory\n","          dqnAg.remember(state, action, reward, next_state, done)\n","\n","          state = next_state\n","          game_score += reward\n","          reward -= 1  # Punish behavior which does not accumulate reward\n","          total_reward += reward\n","          \n","          if terminated:\n","              # si termino\n","              all_rewards += game_score\n","              avg_reward = all_rewards/(e+1)\n","              print(\"total step: {} / ep step: {} * fin episode: {}/{} -> game score: {}, total reward: {}, avg reward: {}\"\n","                    .format(total_step, step, e, train_episodes, game_score, total_reward, round(avg_reward,3)))                            \n","              break \n","          else:\n","              # no termino, se fija si muestra estado\n","              if (step % mostrar_estado_cada_steps)==0:\n","                print(\"total step: {} / ep step: {} * sigue episode: {}/{} -> game score: {} \"\n","                    .format(total_step, step, e, train_episodes, game_score))                            \n","              # fuerz liberar memoria\n","              gc.collect()\n","          \n","          # entrena modelos usando estados anteriores\n","          if len(dqnAg.memory) > train_batch_size:\n","              t1 = time.time()\n","              dqnAg.replay(train_batch_size)\n","              t2 = time.time()\n","              if (t2 - t1) > 2:\n","                 print(\"total step: {} / ep step: {} !! replay tarda mucho: {}\" \n","                    .format(total_step, step, round(t2-t1,3)))                            \n","\n","      if grabar_modelo_mientras_entrena:\n","          # graba modelo\n","          dqnAg.save(dqn_modelo_dir)\n","          print(\"++ Modelo grabado en \" + dqn_modelo_dir + \" ++\")\n","      \n","      # finaliza si alcanza recompensa promedio minima\n","      if avg_reward >= min_recompensa_promedio_finalizar:\n","          print(\"total step: {} * fin episode: {}/{} -- se alcanza mínima recompensa para finalizar entrenamiento!\"\n","            .format(total_step, e, train_episodes))\n","          break\n","\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")                        "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Fo0gnDfkW96a"},"outputs":[],"source":["#@title Probar el Agente DQN Entrenado\n","simular_entorno(eval_atari_env, dqnAg, True, 1, True)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}