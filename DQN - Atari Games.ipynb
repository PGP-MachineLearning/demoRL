{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN - Atari Games.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNCrw1/Ddq3g2YjrkwjAnxq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para jugar juegos de Atari usando una red DQN\n","\n"," Basado en los tutoriales de Tensor Flow: https://www.tensorflow.org/agents/tutorials/2_environments_tutorial\n","\n"," Clase Agente especial de:  https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_04_atari.ipynb "]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip"},"source":["#@title Instalar Paquete de TF-Agents\n","##!pip install -q tf-agents\n","\n","# usar esta versión para evitar error \n","!pip install tf-agents[reverb]\n","!git clone https://github.com/tensorflow/agents.git\n","!cd agents\n","!git checkout v0.13.0\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Instalar Paquetes para acceder a Atari Gym\n","## Opción 1: bajar ROMs ade http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n","#!wget http://www.atarimania.com/roms/Roms.rar\n","#!mkdir /content/ROM/\n","#!unrar e /content/Roms.rar /content/ROM/\n","#!python -m atari_py.import_roms /content/ROM/\n","\n","## Opción 2: más sencilla\n","%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]"],"metadata":{"cellView":"form","id":"0rB1g-ioscoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Instalar Paquetes para visualizar juegos de Atari \n","!sudo apt-get update\n","!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n","!pip install 'imageio==2.4.0'\n","!pip install pyvirtualdisplay\n","!pip install pyglet\n"],"metadata":{"cellView":"form","id":"WOuRKArWfH6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"wJl4YsniURev"},"source":["#@title Cargar Librerías\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_py_environment\n","\n","from tf_agents.environments import utils\n","from tf_agents.specs import array_spec\n","\n","from tf_agents.policies import TFPolicy\n","from tf_agents.policies import random_tf_policy\n","\n","from tf_agents.trajectories import time_step as ts\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents import CategoricalDqnAgent\n","from tf_agents.networks import q_network, categorical_q_network\n","from tf_agents.utils import common\n","\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","\n","tf.compat.v1.enable_v2_behavior()\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entorno para Juego de Atari"],"metadata":{"id":"TilJ4dT4SdtK"}},{"cell_type":"code","source":["#@title Selecciona el juego de Atari\n","seleccionaJuego = \"Pong\" #@param [\"Pong\", \"Freeway\", \"Enduro\", \"Asteroids\", \"Breakout\", \"Space Invaders\"]\n","#@markdown Ver informacion en https://www.gymlibrary.ml/environments/atari/complete_list/\n","tipoObsTS = \"Game RAM\" #@param [\"Game RAM\", \"RGB Screen\", \"Grayscale Screen\"]\n","\n","# selecciona juego\n","if seleccionaJuego == \"Freeway\":\n","  gym_env_name = 'ALE/Freeway-v5'\n","elif seleccionaJuego == \"Enduro\":\n","  gym_env_name = 'ALE/Enduro-v5'\n","elif seleccionaJuego == \"Pong\":\n","  gym_env_name = 'ALE/Pong-v5'\n","elif seleccionaJuego == \"Asteroids\":\n","  gym_env_name = 'ALE/Asteroids-v5'\n","elif seleccionaJuego == \"Breakout\":\n","  gym_env_name = 'ALE/Breakout-v5'\n","elif seleccionaJuego == \"Space Invaders\":\n","  gym_env_name = 'ALE/SpaceInvaders-v5'\n","else:\n","  raise ValueError(\"No se puede defnir gym_env_name!!!\")\n","\n","# determina tipo de OBS\n","if tipoObsTS == \"Game RAM\":\n","  obsType = 'ram'\n","elif tipoObsTS == \"RGB Screen\":\n","  obsType = 'rgb'\n","elif tipoObsTS == \"Grayscale Screen\":\n","  obsType = 'grayscale'\n","else:\n","  raise ValueError(\"No se puede defnir obsType!!!\")  \n","\n","# librerías especiales\n","import pyvirtualdisplay\n","from tf_agents.environments import suite_gym\n","import PIL.Image\n","import base64\n","import imageio\n","import IPython\n","import os\n","import time\n","\n","# Set up a virtual display for rendering OpenAI gym environments.\n","display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n","\n","# funciones auxiliares para mostrar \n","def embed_mp4(filename):\n","  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n","  video = open(filename,'rb').read()\n","  b64 = base64.b64encode(video)\n","  tag = '''\n","  <div align=\"middle\">\n","  <video width=\"640\" height=\"480\" controls>\n","    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n","  Your browser does not support the video tag.\n","  </video></div>'''.format(b64.decode())\n","  return IPython.display.HTML(tag)\n","\n","\n","def generar_animacion_policy(env, envpy, policy, filename, mostrarRecompensa=True, num_episodes=1, fps=32):\n","  filename = filename + \".mp4\"\n","  with imageio.get_writer(filename, fps=fps) as video:\n","    for i in range(num_episodes):\n","      if num_episodes > 1:\n","        print(\"Generando episodio \" + str(i+1) + \"...\")\n","      else:\n","        print(\"Generando...\")\n","      # inicia entorno\n","      time_step = env.reset()\n","      video.append_data(envpy.render())\n","      sumR = 0.00\n","      while not time_step.is_last():\n","        # hace jugar\n","        action_step = policy.action(time_step)\n","        time_step = env.step(action_step.action)\n","        sumR += time_step.reward.numpy()[0]\n","        video.append_data(envpy.render())\n","      # muestra recompensa\n","      if mostrarRecompensa:\n","        rFinal = time_step.reward.numpy()[0]\n","        if num_episodes > 1:\n","          print(\"Recompensa Acumulada del episodio \" + str(i+1) + \": \", sumR)\n","          print(\"Recompensa Final del episodio \" + str(i+1) + \": \", rFinal)\n","        else:\n","          print(\"Recompensa Acumulada: \", sumR)\n","          print(\"Recompensa Final: \", rFinal)\n","    video.close()\n","    ##while not os.path.exists(filename):\n","    ##  time.sleep(1)\n","  return embed_mp4(filename)\n","\n","# crea entornos\n","train_py_env = suite_gym.load(gym_env_name, gym_kwargs={'obs_type':obsType,'render_mode':'human'})\n","eval_py_env = suite_gym.load(gym_env_name, gym_kwargs={'obs_type':obsType,'render_mode':'human'})\n","train_py_env.metadata['render_fps'] = 30\n","eval_py_env.metadata['render_fps'] = 30\n","ori_train_py_env = None\n","ori_eval_py_env = None\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), \n","                                                train_env.action_spec())\n","\n","# muesta información del entorno\n","print(\"\\n\")\n","print('- Entorno: ', gym_env_name)\n","print(\"\\n\")\n","print('- Time Step Spec:')\n","print(\"  \", eval_env.time_step_spec())\n","print(\"\\n\")\n","print('-Action Spec:')\n","print(\"  \", eval_env.action_spec())\n","print(\"\\n\")\n","\n","# muestra pantalla ejemplo\n","eval_env.reset()\n","PIL.Image.fromarray(eval_py_env.render())"],"metadata":{"id":"-jibiVExeuhR","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Ejemplo de juego jugando al Azar\n","generar_animacion_policy(eval_env, eval_py_env, random_policy, \"random_policy\", True, 1)"],"metadata":{"id":"mPf8a-KIj42z","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##DQN"],"metadata":{"id":"3iip7y2pJI-b"}},{"cell_type":"code","metadata":{"cellView":"form","id":"diEOEg3JaMHa"},"source":["#@title Definir el Agente tipo DQN\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","DQNpolicy = None\n","\n","if entrenar_DQN:\n","  tipo_agente = \"AtariCategoricalQNetwork\" #@param [\"DQN\", \"DQN Categorico (C51)\", \"AtariCategoricalQNetwork\"]\n","  learning_rate = 2e-3  # @param {type:\"number\"}\n","  cant_neuronas_ocultas = \"500, 200, 10\" # @param {type:\"string\"}\n","  DQNCat_num_atoms = 51  # param {type:\"integer\"}\n","  # controla cantidad de atoms para DQN Cat\n","  if DQNCat_num_atoms <= 1:\n","    DQNCat_num_atoms = 51\n","\n","  if obsType == 'ram':\n","    # como es un vector no se usa CNN\n","    DQN_agregar_capa_cnn = False\n","  else:\n","    # como es una matriz/imagen se usa CNN\n","    DQN_agregar_capa_cnn = True\n","\n","  # Define cantidad de neuronas ocultas para RNA-Q\n","  hidden_layers = []\n","  for val in cant_neuronas_ocultas.split(','):\n","    if  int(val) < 1:\n","      hidden_layers.append( 10 )\n","    else:\n","      hidden_layers.append( int(val) )\n","  fc_layer_params = tuple(hidden_layers, )\n","\n","  #define las capas convolutional\n","  if DQN_agregar_capa_cnn:    \n","    CNN_preprocessing_layers = tf.keras.models.Sequential(\n","                                        [tf.keras.layers.LayerNormalization(axis=1),\n","                                        tf.keras.layers.Conv2D(2, 2, activation='relu', padding=\"same\"),\n","                                        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","                                        tf.keras.layers.Flatten()])\n","  else:\n","    CNN_preprocessing_layers = None\n","\n","\n","  if tipo_agente==\"DQN\":\n","\n","\n","    # Define RNA-Q\n","    q_net = q_network.QNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        preprocessing_layers=CNN_preprocessing_layers,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.Variable(0)\n","\n","    # Define el agente de tipo Q\n","    ag = dqn_agent.DqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        q_network=q_net,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        train_step_counter=train_step_counter)\n","\n","    ag.initialize()\n","\n","    print(\"Agente DQN inicializado. \")\n","\n","  elif tipo_agente == \"DQN Categorico (C51)\":\n","    \n","    # Define RNA-Q Categórico\n","    categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        num_atoms=DQNCat_num_atoms,\n","        preprocessing_layers=CNN_preprocessing_layers,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.compat.v2.Variable(0)\n","    \n","    # parámetros especificos (por defecto)\n","    n_step_update = 2\n","    gamma = 0.99\n","\n","    # Define el agente de tipo Q Categórico\n","    ag = CategoricalDqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        categorical_q_network=categorical_q_net,\n","        optimizer=optimizer,\n","        n_step_update=n_step_update,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        gamma=gamma,\n","        train_step_counter=train_step_counter)\n","    \n","    ag.initialize()\n","    \n","    print(\"Agente DQN Categorico (C51) inicializado. \")\n","\n","  elif tipo_agente == \"AtariCategoricalQNetwork\":\n","    \n","    # código copiado de https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_04_atari.ipynb\n","    from tf_agents.networks import q_network, network\n","    from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n","    from tf_agents.networks import categorical_q_network\n","    from tf_agents.specs import tensor_spec\n","    import tensorflow as tf\n","\n","    # AtariPreprocessing runs 4 frames at a time, max-pooling over the last 2\n","    # frames. We need to account for this when computing things like update\n","    # intervals.\n","    ATARI_FRAME_SKIP = 4\n","\n","    class AtariCategoricalQNetwork(network.Network):\n","        \"\"\"CategoricalQNetwork subclass that divides observations by 255.\"\"\"\n","\n","        def __init__(self, input_tensor_spec, action_spec, **kwargs):\n","            super(AtariCategoricalQNetwork, self).__init__(\n","                input_tensor_spec, state_spec=())\n","            input_tensor_spec = tf.TensorSpec(\n","                dtype=tf.float32, shape=input_tensor_spec.shape)\n","            self._categorical_q_network = \\\n","                categorical_q_network.CategoricalQNetwork(\n","                    input_tensor_spec, action_spec, **kwargs)\n","\n","        @property\n","        def num_atoms(self):\n","            return self._categorical_q_network.num_atoms\n","\n","        def call(self, observation, step_type=None, network_state=()):\n","            state = tf.cast(observation, tf.float32)\n","            # We divide the grayscale pixel values by 255 here rather than\n","            # storing normalized values beause uint8s are 4x cheaper to\n","            # store than float32s.\n","            # TODO(b/129805821): handle the division by 255 for\n","            # train_eval_atari.py in\n","            # a preprocessing layer instead.\n","            state = state / 255\n","            return self._categorical_q_network(\n","                state, step_type=step_type, network_state=network_state)\n","    \n","    fc_layer_params = (512,)\n","    \n","    #define las capas convolutional\n","    if DQN_agregar_capa_cnn:    \n","      conv_layer_params = ((32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1))\n","    else:\n","      conv_layer_params = None\n","\n","    q_net = AtariCategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        conv_layer_params=conv_layer_params,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.RMSPropOptimizer(\n","        learning_rate=learning_rate,\n","        decay=0.95,\n","        momentum=0.0,\n","        epsilon=0.00001,\n","        centered=True)\n","\n","    train_step_counter = tf.Variable(0)\n","\n","    observation_spec = tensor_spec.from_spec(train_env.observation_spec())\n","    time_step_spec = ts.time_step_spec(observation_spec)\n","\n","    action_spec = tensor_spec.from_spec(train_env.action_spec())\n","    target_update_period = 32000  # ALE frames\n","    update_period = 16  # ALE frames\n","    _update_period = update_period / ATARI_FRAME_SKIP\n","\n","\n","    ag = categorical_dqn_agent.CategoricalDqnAgent(\n","        time_step_spec,\n","        action_spec,\n","        categorical_q_network=q_net,\n","        optimizer=optimizer,\n","        # epsilon_greedy=epsilon,\n","        n_step_update=1.0,\n","        target_update_tau=1.0,\n","        target_update_period=(\n","            target_update_period / ATARI_FRAME_SKIP / _update_period),\n","        gamma=0.99,\n","        reward_scale_factor=1.0,\n","        gradient_clipping=None,\n","        debug_summaries=False,\n","        summarize_grads_and_vars=False)\n","\n","    ag.initialize()\n","\n","    print(\"Agente Atari Categorical Q-Network inicializado. \")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form"},"source":["#@title Métricas para evaluación y Preparar datos para Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  # Definir Métricas para evaluación para Agente DQN\n","    \n","  # Se usa el promedio de la recompensa (la más común)\n","  # See also the metrics module for standard implementations of different metrics.\n","  # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n","\n","  def compute_avg_return(environment, policy, num_episodes=10):\n","    if num_episodes == 0:\n","      return 0.0 \n","    total_return = 0.0\n","    for _ in range(num_episodes):\n","\n","      time_step = environment.reset()\n","      episode_return = 0.0\n","\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = environment.step(action_step.action)\n","        episode_return += time_step.reward\n","      total_return += episode_return\n","\n","    avg_return = total_return / num_episodes\n","    return avg_return.numpy()[0]\n","\n","  initial_collect_steps = 500  # @param {type:\"integer\"} \n","  collect_steps_per_iteration =   25# @param {type:\"integer\"}\n","  replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","  batch_size = 32  # @param {type:\"integer\"}\n","\n","  # Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\n","  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","      data_spec = ag.collect_data_spec,\n","      batch_size = train_env.batch_size,\n","      max_length = replay_buffer_max_length)\n","\n","  # Recolecta datos generados al azar\n","  # This loop is so common in RL, that we provide standard implementations. \n","  # For more details see the drivers module.\n","  # https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n","\n","  def collect_step(environment, policy, buffer):\n","    time_step = environment.current_time_step()\n","    action_step = policy.action(time_step)\n","    next_time_step = environment.step(action_step.action)\n","    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","    # Add trajectory to the replay buffer\n","    buffer.add_batch(traj)\n","\n","  def collect_data(env, policy, buffer, steps):\n","    for _ in range(steps):\n","      collect_step(env, policy, buffer)\n","\n","  collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n","\n","  print(\"\\nDatos recolectados.\")\n","\n","  # Muestra ejemplo de los datos recolectados\n","  ##iter(replay_buffer.as_dataset()).next()\n","\n","  # Preparar los datos recolectados con trajectories de shape [Bx2x...]\n","  dataset = replay_buffer.as_dataset(\n","      num_parallel_calls=3, \n","      sample_batch_size=batch_size, \n","      num_steps=2).prefetch(3)\n","  iterator = iter(dataset)\n","  # Muestra ejemplo \n","  ##iterator.next()\n","  print(\"\\nDataset creado.\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Entrenar al Agente DQN\n","\n","if entrenar_DQN:\n","\n","  cant_ciclos_entrenamiento_finalizar =  5000# @param {type:\"integer\"}\n","  log_cada_ciclos = 300  # @param {type:\"integer\"}\n","  mostar_recompensa_cada = 500  # @param {type:\"integer\"}\n","  cant_episodios_evaluacion =  10# @param {type:\"integer\"}\n","  minima_recompensa_promedio_finalizar = 1000 # @param {type:\"number\"}\n","  \n","  #  Optimize by wrapping some of the code in a graph using TF function (Optional)\n","  ag.train = common.function(ag.train)\n","\n","  # Reset the train step\n","  ag.train_step_counter.assign(0)\n","\n","  # Evaluate the agent's policy once before training.\n","  avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","  ar_cicloL = []\n","  ar_cicloR = []\n","  ar_returns = []\n","  ar_loss = []\n","\n","  print(\"\\n** Comienza el Entrenamiento **\\n\")\n","  for _ in range(cant_ciclos_entrenamiento_finalizar):\n","\n","    # Collect a few steps using collect_policy and save to the replay buffer.\n","    collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\n","\n","    # Sample a batch of data from the buffer and update the agent's network.\n","    experience, unused_info = next(iterator)\n","    train_loss = ag.train(experience).loss\n","\n","    step = ag.train_step_counter.numpy()\n","\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\n","      print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \n","      ar_cicloL.append( step )\n","      ar_loss.append( train_loss )\n","    \n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\n","      avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","      ar_cicloR.append( step )\n","      ar_returns.append( avg_return )\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\n","\n","      if (avg_return >= minima_recompensa_promedio_finalizar):\n","        print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \n","        break\n","\n","  DQNpolicy = ag.policy\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"metadata":{"cellView":"form","id":"LQbSlCJW8BeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"9EBBl7mRkQYa"},"source":["#@title Mostrar Gráficos del Entrenamiento del Agente DQN\n","\n","\n","if entrenar_DQN:\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloR, ar_returns)\n","  plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloR))   \n","  plt.grid(True)\n","  plt.show()\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloL, ar_loss, color=\"red\" )\n","  plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloL))   \n","  plt.grid(True)\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IjAkgViOJI-e"},"source":["#@title Probar el Agente DQN Entrenado \n","\n","generar_animacion_policy(eval_env, eval_py_env, DQNpolicy, \"DQNpolicy\", True, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Cargar o Guardar el Agente DQN entrenado\n","\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-Atari\" #@param {type:\"string\"}\n","accion_realizar = \"Grabar Modelo\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","  import os\n","  from google.colab import drive\n","  from tf_agents.policies import TFPolicy, policy_saver\n","  # determina lugar donde se guarda el modelo\n","  policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  policy_dir = os.path.join(policy_dir, gym_env_name)\n","  # Montar Drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  if (DQNpolicy is not None) and isinstance(DQNpolicy, TFPolicy):\n","    # guarda la politica del agente DQN entrenado\n","    tf_policy_saver = policy_saver.PolicySaver(DQNpolicy)\n","    tf_policy_saver.save(policy_dir)\n","    print(\"\\nPolítica DQN guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  DQNpolicy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica DQN recuperada de \", policy_dir)\n"],"metadata":{"cellView":"form","id":"XTTpaVBfBOGi"},"execution_count":null,"outputs":[]}]}