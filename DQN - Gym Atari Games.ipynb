{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents con ambiente Gym para jugar juegos de Atari usando una red DQN\n","\n"," Basado en los tutoriales: \n","\n","   https://www.tensorflow.org/agents/tutorials/2_environments_tutorial \n","\n","   https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8-AxnvAVyzQQ\n","   \n","   https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_04_atari.ipynb \n"]},{"cell_type":"code","metadata":{"id":"Qxbe02w0T0ip","cellView":"form"},"source":["#@title Instalar Paquete de TF-Agents\n","# usar esta versión para evitar errores\n","# recomendada en https://github.com/tensorflow/agents\n","!pip install tf-agents[reverb]\n","!git clone https://github.com/tensorflow/agents.git\n","!cd agents\n","#!git checkout v0.15.0  \n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Instalar Paquete Gym para acceder a juegos Atari\n","# nota: hay una version nueva Gymnasium pero todavía no es compatible con TF-Agents\n","#        https://gymnasium.farama.org/content/gym_compatibility/\n","!pip install gym[atari,accept-rom-license]     \n","print(\"Gym para ATARI instalado.\")"],"metadata":{"cellView":"form","id":"0rB1g-ioscoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"wJl4YsniURev"},"source":["#@title Cargar Librerías\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","import reverb\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.environments import utils\n","from tf_agents.specs import array_spec\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.trajectories import time_step as ts\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents import CategoricalDqnAgent\n","from tf_agents.networks import q_network, categorical_q_network\n","from tf_agents.utils import common\n","\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","\n","from tf_agents.networks import sequential\n","from tf_agents.specs import tensor_spec\n","from tf_agents.replay_buffers import reverb_replay_buffer\n","from tf_agents.replay_buffers import reverb_utils\n","from tf_agents.drivers import py_driver\n","from tf_agents.policies import py_tf_eager_policy\n","\n","tf.compat.v1.enable_v2_behavior()\n","\n","print(\"Librerías cargadas.\")\n","print(\"(nota: ignorar el error que tira por diferencias de versiones)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entorno para Juego de Atari"],"metadata":{"id":"TilJ4dT4SdtK"}},{"cell_type":"code","source":["#@title Preparar funciones auxiliares para visualizar juegos Atari\n","\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import RecordVideo\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","import os\n","from IPython import display as ipythondisplay\n","\n","##from pyvirtualdisplay import Display\n","##display = Display(visible=0, size=(1400, 900))\n","##display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","def show_env_video(env):\n","  # trata de encontrar el video que corresponde\n","  global seleccionaJuego\n","  encuentraVideo = False\n","  mp4list = glob.glob('./EnvVideos/*.mp4')\n","  if len(mp4list) > 0:\n","    # toma el último video generado para el juego\n","    mp4list.sort(reverse=True, key=os.path.getmtime)\n","    mp4 = mp4list[0]\n","    encuentraVideo = True\n","  if encuentraVideo:\n","    print(\"Video: \", mp4)\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"No se encuentra video \" + mp4 + \" del juego!\")\n","    \n","    \n","def func_episode_trigger(ep):\n","  # siempre graba\n","  return True\n","\n","def wrap_env_recorder(env):\n","  global seleccionaJuego\n","  env = RecordVideo(env, \n","                    video_folder = './EnvVideos', \n","                    episode_trigger = func_episode_trigger, \n","                    video_length = 0,\n","                    name_prefix = seleccionaJuego)\n","  return env\n","\n","print(\"\\nWrapper para generar video preparado.\")\n","\n","def simular_entorno(env, policy, mostrarRecompensa=True, num_episodes=1, mostrar_video=True):\n","  for i in range(num_episodes):\n","    if num_episodes > 1:\n","      print(\"Generando episodio \" + str(i+1) + \"...\")\n","    else:\n","      print(\"Generando...\")\n","    # inicia entorno\n","    time_step = env.reset()\n","    sumR = 0.00\n","    while not time_step.is_last():\n","      # hace jugar\n","      action_step = policy.action(time_step)\n","      time_step = env.step(action_step.action)\n","      sumR += time_step.reward.numpy()[0]\n","    # muestra recompensa\n","    if mostrarRecompensa:\n","      rFinal = time_step.reward.numpy()[0]\n","      if num_episodes > 1:\n","        print(\"Recompensa Acumulada del episodio \" + str(i+1) + \": \", sumR)\n","        #print(\"Recompensa Final del episodio \" + str(i+1) + \": \", rFinal)\n","      else:\n","        print(\"Recompensa Acumulada: \", sumR)\n","        #print(\"Recompensa Final: \", rFinal)\n","    if mostrar_video:\n","      show_env_video(env)\n","  return \n","\n","print(\"\\nFunción para simular entorno definida.\")"],"metadata":{"id":"L8u4OtWqurq6","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Seleccionar el juego de Atari\n","\n","seleccionaJuego = \"Breakout\" #@param [\"Pong\", \"Freeway\", \"Enduro\", \"Asteroids\", \"Breakout\", \"Space Invaders\"]\n","#@markdown Ver informacion en https://www.gymlibrary.dev/environments/atari/complete_list/\n","tipoObsTS = \"Grayscale Screen\" #@param [\"Game RAM\", \"Grayscale Screen\", \"RGB Screen\"]\n","entornoDeterministico = True #@param{type:\"boolean\"}\n","maxima_cantidad_pasos_juego = 1500 #@param{type:\"integer\"}\n","\n","# selecciona juego\n","if seleccionaJuego == \"Freeway\":\n","  gym_env_name = 'ALE/Freeway-v5'\n","elif seleccionaJuego == \"Enduro\":\n","  gym_env_name = 'ALE/Enduro-v5'\n","elif seleccionaJuego == \"Pong\":\n","  gym_env_name = 'ALE/Pong-v5'\n","elif seleccionaJuego == \"Asteroids\":\n","  gym_env_name = 'ALE/Asteroids-v5'\n","elif seleccionaJuego == \"Breakout\":\n","  gym_env_name = 'ALE/Breakout-v5'\n","elif seleccionaJuego == \"Space Invaders\":\n","  gym_env_name = 'ALE/SpaceInvaders-v5'\n","else:\n","  raise ValueError(\"No se puede defnir gym_env_name!!!\")\n","\n","# determina tipo de OBS\n","if tipoObsTS == \"Game RAM\":\n","  obsType = 'ram'\n","elif tipoObsTS == \"RGB Screen\":\n","  obsType = 'rgb'\n","elif tipoObsTS == \"Grayscale Screen\":\n","  obsType = 'grayscale'\n","else:\n","  raise ValueError(\"No se puede defnir obsType!!!\")  \n","\n","# si es negativo le asigna 0 que es cantidad infinita\n","if maxima_cantidad_pasos_juego < 0:\n","  maxima_cantidad_pasos_juego = 0\n","\n","# librerías especiales\n","from tf_agents.environments import suite_gym\n","\n","\n","# función para inicializar juego\n","def inicializar_gym_env(gym_env_name, obs_type, entornoDeterministico=False, max_episode_steps=1000, grabaSteps=True):\n","    if grabaSteps:\n","      gym_env_wrappers = [wrap_env_recorder]\n","    else:\n","      gym_env_wrappers = None\n","\n","    if entornoDeterministico:\n","      # crea el entorno con parámetros deterministicos\n","      env =suite_gym.load(gym_env_name, \n","                                gym_env_wrappers = gym_env_wrappers,\n","                                max_episode_steps=max_episode_steps, \n","                                gym_kwargs={'frameskip': 1,\n","                                            'repeat_action_probability':False,\n","                                            'full_action_space':False,\n","                                            'obs_type':obsType,\n","                                            'render_mode':'rgb_array'}\n","                          )\n","    else:\n","       # crea el entorno con parámetros estocásticos\n","      env =suite_gym.load(gym_env_name, \n","                                gym_env_wrappers = gym_env_wrappers,\n","                                max_episode_steps=max_episode_steps, \n","                                gym_kwargs={'full_action_space':False,\n","                                            'obs_type':obsType,\n","                                            'render_mode':'rgb_array'}\n","                          )\n","\n","    env.metadata['render_fps'] = 30\n","    return env\n","\n","\n","# crea el entorno\n","atari_py_env = inicializar_gym_env(\n","                  gym_env_name, \n","                  obs_type = obsType, \n","                  entornoDeterministico = entornoDeterministico, \n","                  max_episode_steps = maxima_cantidad_pasos_juego, \n","                  grabaSteps = True)\n","\n","# Definir wrapper para convertir en entornos TF\n","atari_env = tf_py_environment.TFPyEnvironment(atari_py_env)\n","\n","# asigna nombre variables por compatibilidad código para entrenar\n","train_py_env = atari_py_env\n","eval_py_env = atari_py_env\n","train_env = atari_env\n","eval_env = atari_env\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(atari_env.time_step_spec(), \n","                                                atari_env.action_spec())\n","# muesta información del entorno\n","print(\"\\n\")\n","print('- Entorno: ', gym_env_name)\n","print('\\n- Specification:')\n","for det in atari_py_env._env._gym_env.spec.kwargs:\n","  print(\"  \", det, \"=\", atari_py_env._env._gym_env.spec.kwargs[det])\n","#print(\"   max_episode_steps=\", atari_py_env._env._gym_env._max_episode_steps)\n","print('\\n- Time Step Spec:')\n","print(\"  \", atari_env.time_step_spec())\n","print('\\n-Action Spec:')\n","print(\"  \", atari_env.action_spec())\n","print('\\n-Reward range:')\n","print(\"  \", atari_py_env._env._gym_env.reward_range)\n","\n","# muestra pantalla ejemplo\n","print(\"\\n-Ejemplo pantalla: \")\n","atari_env.reset()\n","import PIL.Image\n","PIL.Image.fromarray(atari_py_env.render())"],"metadata":{"id":"-jibiVExeuhR","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Ejemplo de juego jugando al Azar\n","simular_entorno(atari_env, random_policy, True, 1, True)\n"],"metadata":{"id":"mPf8a-KIj42z","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##DQN"],"metadata":{"id":"3iip7y2pJI-b"}},{"cell_type":"code","metadata":{"id":"diEOEg3JaMHa","cellView":"form"},"source":["#@title Definir el Agente tipo DQN\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","DQNpolicy = None\n","\n","if entrenar_DQN:\n","  # agente categorical oculto por ahora (problemas en librería)\n","  tipo_agente = \"DQN\" #param [\"DQN\", \"DQN Categorico (C51)\"]\n","  learning_rate = 1e-3  # @param {type:\"number\"}\n","  cant_neuronas_ocultas = \"100, 50, 25\" # @param {type:\"string\"}\n","  DQNCat_num_atoms = 51  #param {type:\"integer\"}\n","\n","  # controla cantidad de atoms para DQN Cat\n","  if DQNCat_num_atoms <= 1:\n","    DQNCat_num_atoms = 51\n","\n","  #define las capas convolutional\n","\n","  #define las capas convolutional\n","  if obsType == 'rgb':\n","    # como es una matriz/imagen se usa CNN\n","    CNN_preprocessing_layers = tf.keras.models.Sequential(\n","                                        [tf.keras.layers.LayerNormalization(axis=1),                                       \n","                                        tf.keras.layers.Conv2D(2, 2, activation='relu', padding=\"same\"),\n","                                        tf.keras.layers.MaxPooling2D(pool_size=(2)),\n","                                        tf.keras.layers.Conv2D(2, 2, activation='relu', padding=\"same\"),\n","                                        tf.keras.layers.MaxPooling2D(pool_size=(2)),\n","                                        tf.keras.layers.Flatten()])\n","    print(\"Agrega capas CNN para preprocesamiento de entrada RGB.\")\n","  elif obsType == 'grayscale':\n","    # como es una matriz/imagen se usa CNN\n","    CNN_preprocessing_layers = tf.keras.models.Sequential(\n","                                        [tf.keras.layers.LayerNormalization(axis=1),                                        \n","                                        tf.keras.layers.Conv1D(2, 2, activation='relu', padding=\"same\"),\n","                                        tf.keras.layers.MaxPooling1D(pool_size=(2)),\n","                                        tf.keras.layers.Flatten()])\n","    print(\"Agrega capas CNN para preprocesamiento de entrada grayscale.\")\n","  else:\n","    # como es un vector no se usa CNN\n","    CNN_preprocessing_layers = None\n","\n","  # Define cantidad de neuronas ocultas para RNA-Q\n","  hidden_layers = []\n","  for val in cant_neuronas_ocultas.split(','):\n","    if  int(val) < 1:\n","      hidden_layers.append( 10 )\n","    else:\n","      hidden_layers.append( int(val) )\n","  fc_layer_params = tuple(hidden_layers, )\n","  #print(fc_layer_params)\n","\n","  if tipo_agente==\"DQN\":\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    train_step_counter = tf.Variable(0)\n","\n","    # Define RNA-Q\n","    q_net = q_network.QNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        preprocessing_layers=CNN_preprocessing_layers,\n","        fc_layer_params=fc_layer_params)\n","\n","    # Define el agente de tipo Q\n","    ag = dqn_agent.DqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        q_network=q_net,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        train_step_counter=train_step_counter)\n","\n","    ag.initialize()\n","\n","    print(\"Agente DQN inicializado. \")\n","\n","  elif tipo_agente == \"DQN Categorico (C51)\":\n","    \n","    # Define RNA-Q Categórico\n","    categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        num_atoms=DQNCat_num_atoms,\n","        preprocessing_layers=CNN_preprocessing_layers,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.compat.v2.Variable(0)\n","    \n","    # parámetros especificos (por defecto)\n","    n_step_update = 2\n","    gamma = 0.99\n","\n","    # Define el agente de tipo Q Categórico\n","    ag = CategoricalDqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        categorical_q_network=categorical_q_net,\n","        optimizer=optimizer,\n","        n_step_update=n_step_update,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        gamma=gamma,\n","        train_step_counter=train_step_counter)\n","    \n","    ag.initialize()\n","    \n","    print(\"Agente DQN Categorico (C51) inicializado. \")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form"},"source":["#@title Métricas para evaluación y Preparar datos para Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  # parámetros\n","  initial_collect_steps = 500  # @param {type:\"integer\"} \n","  collect_steps_per_iteration = 10# @param {type:\"integer\"}\n","  replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","  batch_size = 64  # @param {type:\"integer\"}\n","  num_eval_episodes = 10  # @param {type:\"integer\"}\n","\n","  # Definir Métricas para evaluación para Agente DQN\n","    \n","  #eval_policy = ag.policy\n","  #collect_policy = ag.collect_policy\n","  #time_step = train_env.reset()\n","\n","  # Se usa el promedio de la recompensa (la más común)\n","  # See also the metrics module for standard implementations of different metrics.\n","  # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n","\n","  def compute_avg_return(environment, policy, num_episodes=10):\n","    if num_episodes == 0:\n","        return 0.0 \n","    total_return = 0.0\n","    for _ in range(num_episodes):\n","\n","      time_step = environment.reset()\n","      episode_return = 0.0\n","\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = environment.step(action_step.action)\n","        episode_return += time_step.reward\n","      total_return += episode_return\n","\n","    avg_return = total_return / num_episodes\n","    return avg_return.numpy()[0]\n","\n","  ##compute_avg_return(eval_env, random_policy, num_eval_episodes)\n","\n","  # Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\n","\n","  table_name = 'uniform_table'\n","  replay_buffer_signature = tensor_spec.from_spec(\n","        ag.collect_data_spec)\n","  replay_buffer_signature = tensor_spec.add_outer_dim(\n","      replay_buffer_signature)\n","\n","  table = reverb.Table(\n","      table_name,\n","      max_size=replay_buffer_max_length,\n","      sampler=reverb.selectors.Uniform(),\n","      remover=reverb.selectors.Fifo(),\n","      rate_limiter=reverb.rate_limiters.MinSize(1),\n","      signature=replay_buffer_signature)\n","\n","  reverb_server = reverb.Server([table])\n","\n","  replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n","      ag.collect_data_spec,\n","      table_name=table_name,\n","      sequence_length=2,\n","      local_server=reverb_server)\n","\n","  rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n","    replay_buffer.py_client,\n","    table_name,\n","    sequence_length=2)      \n","\n","  print(\"\\nDatos recolectados.\")\n","\n","  # Dataset generates trajectories with shape [Bx2x...]\n","  dataset = replay_buffer.as_dataset(\n","      num_parallel_calls=3,\n","      sample_batch_size=batch_size,\n","      num_steps=2).prefetch(3)\n","  iterator = iter(dataset)\n","  \n","  print(\"\\nDataset creado para datos recolectadso.\")\n","\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Entrenar al Agente DQN\n","\n","if entrenar_DQN:\n","\n","  cant_ciclos_entrenamiento_finalizar = 20000# @param {type:\"integer\"}\n","  log_cada_ciclos = 200  # @param {type:\"integer\"}\n","  mostar_recompensa_cada = 500  # @param {type:\"integer\"}\n","  cant_episodios_evaluacion =  10# @param {type:\"integer\"}\n","  minima_recompensa_promedio_finalizar = 10.0 # @param {type:\"number\"}\n","  \n","  # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n","  ag.train = common.function(ag.train)\n","\n","  # Reset the train step.\n","  ag.train_step_counter.assign(0)\n","\n","  # Evaluate the agent's policy once before training.  \n","  avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","  ar_cicloL = []\n","  ar_cicloR = []\n","  ar_returns = []\n","  ar_loss = []\n","\n","  # Reset the environment.\n","  time_step = train_py_env.reset()\n","\n","  # Create a driver to collect experience.\n","  collect_driver = py_driver.PyDriver(\n","      train_py_env,\n","      py_tf_eager_policy.PyTFEagerPolicy(\n","        ag.collect_policy, use_tf_function=True),\n","      [rb_observer],\n","      max_steps=collect_steps_per_iteration)\n","\n","  print(\"\\n** Comienza el Entrenamiento **\\n\")\n","  for _ in range(cant_ciclos_entrenamiento_finalizar):\n","\n","    # Collect a few steps and save to the replay buffer.\n","    time_step, _ = collect_driver.run(time_step)\n","\n","    # Sample a batch of data from the buffer and update the agent's network.\n","    experience, unused_info = next(iterator)\n","    try:\n","      train_loss = ag.train(experience).loss\n","    except:\n","      # valor para error \n","      train_loss = -999\n","\n","    step = ag.train_step_counter.numpy()    \n","\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\n","      if train_loss == -999:\n","        print('step = {0}: ERROR al calcular LOSS!'.format(step))      \n","      else:\n","        print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \n","        ar_cicloL.append( step )\n","        ar_loss.append( train_loss )\n","    \n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\n","      avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","      ar_cicloR.append( step )\n","      ar_returns.append( avg_return )\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\n","\n","      if (avg_return >= minima_recompensa_promedio_finalizar):\n","        print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \n","        break\n","\n","  DQNpolicy = ag.policy\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"metadata":{"cellView":"form","id":"LQbSlCJW8BeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"9EBBl7mRkQYa"},"source":["#@title Mostrar Gráficos del Entrenamiento del Agente DQN\n","if entrenar_DQN:\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloR, ar_returns)\n","  plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloR))   \n","  plt.grid(True)\n","  plt.show()\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloL, ar_loss, color=\"red\" )\n","  plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloL))   \n","  plt.grid(True)\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IjAkgViOJI-e"},"source":["#@title Probar el Agente DQN Entrenado \n","simular_entorno(atari_env, DQNpolicy, True, 1, True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Cargar o Guardar el Agente DQN entrenado\n","\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-Atari\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","  import os\n","  from google.colab import drive\n","  from tf_agents.policies import TFPolicy, policy_saver\n","  # determina lugar donde se guarda el modelo\n","  policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  policy_dir = os.path.join(policy_dir, gym_env_name)\n","  # Montar Drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  if (DQNpolicy is not None) and isinstance(DQNpolicy, TFPolicy):\n","    # guarda la politica del agente DQN entrenado\n","    tf_policy_saver = policy_saver.PolicySaver(DQNpolicy)\n","    tf_policy_saver.save(policy_dir)\n","    print(\"\\nPolítica DQN guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  DQNpolicy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica DQN recuperada de \", policy_dir)\n"],"metadata":{"cellView":"form","id":"XTTpaVBfBOGi"},"execution_count":null,"outputs":[]}]}