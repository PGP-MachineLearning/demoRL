{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QL & DQN - Blackjack.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMRKZnvzXC++ffumP5kNeFF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"976f3dde9b8a4392ba79f0f27476bf36":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Dame Otra Carta","disabled":false,"icon":"","layout":"IPY_MODEL_a37b35cc25564aa0ab44485b5dfbdf4e","style":"IPY_MODEL_d92fd232e68c4a758efceabd7fd23818","tooltip":""}},"a37b35cc25564aa0ab44485b5dfbdf4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92fd232e68c4a758efceabd7fd23818":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"52ff142f411a4a4394b570694f804155":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Plantarse","disabled":false,"icon":"","layout":"IPY_MODEL_22a72ffb8b53441d8150b0e893b007b6","style":"IPY_MODEL_c7892e049679440f961bde0f0f7e05dc","tooltip":""}},"22a72ffb8b53441d8150b0e893b007b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7892e049679440f961bde0f0f7e05dc":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"e8843eb7d58349d59d76582186baced5":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Reiniciar Partida","disabled":false,"icon":"","layout":"IPY_MODEL_235d8993871a409fb9e2b1d64c192fa6","style":"IPY_MODEL_87b2863cd95d465f8ed10a8be741ccae","tooltip":""}},"235d8993871a409fb9e2b1d64c192fa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87b2863cd95d465f8ed10a8be741ccae":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"122781ec73164df693c4c1d416d77dcd":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_d14632828d254a258b1a480ffe6da868","msg_id":"","outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Nueva Partida:\n","\n","* Juega Agente QL #1: pide carta As -> SUMA=11\n","* Juega Agente DQN #1: pide carta 5 -> SUMA=5\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #1: pide carta Q -> SUMA=10\n"," \n","* Juega Agente QL #2: decide no pedir m치s cartas -> SUMA=11\n","* Juega Agente DQN #2: pide carta 3 -> SUMA=8\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #2: pide carta 7 -> SUMA=17\n"," \n","* Juega Agente DQN #3: pide carta 5 -> SUMA=13\n"]},{"output_type":"stream","name":"stdout","text":["+ Juega Humano #3: decide no pedir m치s cartas -> SUMA=17\n"," \n","* Juega Agente DQN #4: decide no pedir m치s cartas -> SUMA=13\n","\n","> Resultados: \n","\t Agente QL: 11\n","\t Agente DQN: 13\n","\t Humano: 17\n","\t = GANA: Humano\n","\n","\n","\n","\n"]}]}},"d14632828d254a258b1a480ffe6da868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para jugar al Blackjack (21) usando primero Q-Learning y luego una red DQN\n","\n"," Basado inicialmente en los tutoriales de Tensor Flow: https://www.tensorflow.org/agents/tutorials/2_environments_tutorial"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014757753,"user_tz":180,"elapsed":91159,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"8c3105dc-d342-4f7b-cd2f-719e23c76a6d"},"source":["#@title Instalar Paquete de TF-Agents\n","##!pip install -q tf-agents\n","\n","# usar esta versi칩n para evitar error \n","!pip install tf-agents[reverb]\n","!git clone https://github.com/tensorflow/agents.git\n","!cd agents\n","!git checkout v0.13.0\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf-agents[reverb]\n","  Downloading tf_agents-0.13.0-py3-none-any.whl (1.3 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 1.3 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.2.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.1.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 18.3 MB 107 kB/s \n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n","Collecting dm-reverb~=0.8.0\n","  Downloading dm_reverb-0.8.0-cp37-cp37m-manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 6.5 MB 25.3 MB/s \n","\u001b[?25hCollecting rlds\n","  Downloading rlds-0.1.4-py3-none-manylinux2010_x86_64.whl (37 kB)\n","Collecting tensorflow~=2.9.0\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 511.7 MB 5.8 kB/s \n","\u001b[?25hRequirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (1.3.9)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (0.1.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.7.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.16.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (14.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.47.0)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.26.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.6.3)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 1.6 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (57.4.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 438 kB 72.8 MB/s \n","\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 5.8 MB 49.0 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (21.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-agents[reverb]) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-agents[reverb]) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.35.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.9)\n","Installing collected packages: gast, tensorflow-estimator, tensorboard, pygame, keras, flatbuffers, tf-agents, tensorflow, rlds, dm-reverb\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","Successfully installed dm-reverb-0.8.0 flatbuffers-1.12 gast-0.4.0 keras-2.9.0 pygame-2.1.0 rlds-0.1.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tf-agents-0.13.0\n","Cloning into 'agents'...\n","remote: Enumerating objects: 19500, done.\u001b[K\n","remote: Counting objects: 100% (3422/3422), done.\u001b[K\n","remote: Compressing objects: 100% (1102/1102), done.\u001b[K\n","remote: Total 19500 (delta 2407), reused 3141 (delta 2312), pack-reused 16078\u001b[K\n","Receiving objects: 100% (19500/19500), 11.86 MiB | 9.17 MiB/s, done.\n","Resolving deltas: 100% (14748/14748), done.\n","fatal: not a git repository (or any of the parent directories): .git\n","TF-Agentes instalado.\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"wJl4YsniURev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014760368,"user_tz":180,"elapsed":2730,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"d302ddb8-ea75-4800-febc-02b9c5e6d237"},"source":["#@title Cargar Librer칤as\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import abc\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from random import randint\n","\n","import random\n","import pandas as pd\n","\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_py_environment\n","\n","from tf_agents.environments import utils\n","from tf_agents.specs import array_spec\n","\n","from tf_agents.policies import random_tf_policy\n","\n","from tf_agents.trajectories import time_step as ts\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.networks import q_network\n","from tf_agents.utils import common\n","\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","\n","tf.compat.v1.enable_v2_behavior()\n","\n","print(\"Librer칤as cargadas.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Librer칤as cargadas.\n"]}]},{"cell_type":"markdown","source":["## Clases sobre el Problema a resolver"],"metadata":{"id":"1KXNPjrJSmh3"}},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014760953,"user_tz":180,"elapsed":592,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"a3d2b3b4-fa85-4294-f6a4-bbaa0f84be23"},"source":["#@title Definir Entorno del Problema \n","\n","# Un entorno que represente el juego podr칤a verse as칤:\n","##Acciones: Tenemos dos acciones. \n","##             Acci칩n 0: obtener una nueva carta \n","##             Acci칩n 1: terminar la ronda actual.\n","##Observaciones: Suma de las cartas de la ronda actual.\n","##Recompensa: El objetivo es acercarse lo m치s posible a 21 sin pasarse, \n","##            por lo que podemos lograrlo usando la siguiente recompensa al final de la ronda: \n","##            suma_de_tarjetas - 21 si suma_de_tarjetas <= 21, de lo contrario -21\n","\n","posiblesAccionesDescrip = [\"pide carta\", \"decide no pedir m치s cartas\"]\n","posiblesEstadosDescrip = ['SUMA={:}'.format(x) for x in range(0, 33)]\n","posiblesCartas = ['-', 'As', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']\n","class CardGameEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(2,), dtype=np.int32, minimum=0, name='observation')\n","    self._state = 0\n","    self._episode_ended = False\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    self._state = 0\n","    self._episode_ended = False\n","    return ts.restart(np.array([self._state, 0], dtype=np.int32))\n","\n","  def _step(self, action):\n","    # aplica una acci칩n sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno est치 finalizado, lo resetea\n","      return self.reset()\n","\n","    # Aplica la acci칩n\n","    if action == 1:\n","      # indica no seguir jugando\n","      self._episode_ended = True\n","      new_card = 0\n","    elif action == 0:\n","      # solicita una carta\n","      new_card = randint(1, 12)\n","      if new_card == 1:\n","        # el as puede valer 1 o 11\n","        if (self._state + 11) < 21:\n","          self._state = self._state + 11\n","        else:\n","          self._state = self._state + 1  \n","      elif new_card >= 10:\n","        # las figuras valen 10\n","        self._state = self._state + 10\n","      else:\n","        # el resto vale su valor\n","        self._state = self._state + new_card\n","      # se llega a un valor de 21, no se puede pedir m치s\n","      if self._state >= 21:\n","        self._episode_ended = True\n","    else:\n","      raise ValueError('La acci칩n debe ser 0 o 1.')\n","\n","    # finaliza \n","    if self._episode_ended:\n","      # si finaliza\n","      # determina el reward (siempre se maximiza)\n","      # en este caso el m치ximo es 21!\n","      if self._state > 21:\n","        # se paso de 21, devuelve cantidad que se paso (negativo)\n","        reward = 21 - self._state \n","      else:\n","        # no se paso de 21 (devuelve a donde lleg칩)\n","        reward = self._state \n","      return ts.termination(np.array([self._state, new_card], dtype=np.int32), reward)\n","    else:\n","      # si no finaliza\n","      return ts.transition(\n","          np.array([self._state, new_card], dtype=np.int32), reward=0.0, discount=0.9)\n","\n","\n","print(\"Entorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y evaluaci칩n\n","train_py_env = CardGameEnv()\n","eval_py_env = CardGameEnv()\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define pol칤tica al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"Entornos de entrenamiento y prueba definidos. \")\n","\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(eval_env, policy, titulo, num_episodes=1, mostrarDetalleStep=False):\n","    if num_episodes <=0:\n","      num_episodes = 1    \n","    # inicializa acumulador auxiliar \n","    cumulative_reward_total = 0.0    \n","    print(\"\\n** \", titulo, \"**\")                   \n","    for i in range(num_episodes):\n","      if num_episodes>1:\n","        print(\"\\n> Episodio \", i+1, \": \")\n","      # muesta estado inicial\n","      time_step = eval_env.reset()  \n","      ob = time_step.observation.numpy()[0]\n","      if mostrarDetalleStep:\n","        print(\" Ini: estado\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")      \n","      else:\n","        print(\" Ini: estado\", posiblesEstadosDescrip[ob[0]])      \n","      j = 1\n","      while not time_step.is_last():\n","        # la pol칤tica determina la acci칩n a realizar\n","        action_step = policy.action(time_step)\n","        time_step = eval_env.step(action_step.action)\n","        # recupera la observaci칩n y muestra el nuevo estado \n","        ob = time_step.observation.numpy()[0]\n","        if ob[1] == 0:\n","          descAccion = posiblesAccionesDescrip[1]\n","        else:\n","          descAccion = posiblesAccionesDescrip[0] + \" \" + posiblesCartas[ob[1]]\n","        if mostrarDetalleStep:\n","          print(\" #\", j, \":\", descAccion, \"-> estado:\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")\n","        else:\n","          print(\" #\" + str(j) + \":\", descAccion, \"-> estado:\", posiblesEstadosDescrip[ob[0]])\n","        j = j + 1\n","        \n","      # muestra estado final\n","      ob = time_step.observation.numpy()[0]\n","      r = time_step.reward.numpy()\n","      if mostrarDetalleStep:\n","        print(\" Fin -> estado\", posiblesEstadosDescrip[ob[0]], \"[\", time_step, \"]\")\n","      else:\n","        print(\" Fin -> estado\", posiblesEstadosDescrip[ob[0]])\n","      print(' Recompensa final = ', r[0])\n","      cumulative_reward_total += r[0]\n","    if num_episodes > 1:\n","      promedioEpisodios = round(cumulative_reward_total/num_episodes,3)\n","      print(\"\\n= Recompensa Promedio Total: \", promedioEpisodios, \"\\n\")\n","      return promedioEpisodios\n","    else:\n","      return cumulative_reward_total\n","\n","\n","# funci칩n auxiliar para comparar\n","def compararRtdosPolicy(cantidad_probar, eval_env, policy1, policy2, descPol1=\"Agente 1\", descPol2=\"Agente 2\"):\n","  prom1 = 0\n","  prom2 = 0\n","  for i in range(cantidad_probar):\n","    print(\"\\n> Prueba \", i+1, \":\")\n","    # Probar 1\n","    valor1 = SimularEntorno(eval_env, policy1, \"Resultados de \" + descPol1, False) \n","    prom1 = prom1 + valor1\n","    # Probar 2\n","    valor2 = SimularEntorno(eval_env, policy2, \"Resultados de \" + descPol2, False) \n","    prom2 = prom2 + valor2\n","    # Decide Ganador\n","    strMostrar = \"\\n--> \" + descPol1 + \" (%s) genera\" % valor1\n","    if valor1 > valor2:\n","      strMostrar = strMostrar + \" MEJOR \"\n","    elif valor1 < valor2:\n","      strMostrar = strMostrar + \" PEOR \"\n","    else:\n","      strMostrar = strMostrar + \" IGUAL \"\n","    strMostrar = strMostrar + \"resultado que \" + descPol2 + \" (%s).\" % valor2\n","    print(strMostrar)\n","  # Decide Ganador General\n","  if cantidad_probar > 0:\n","    prom1 = prom1 / cantidad_probar\n","    prom2 = prom2 / cantidad_probar\n","    print(\"\\n================================================================================================\\n\")\n","    strMostrar = \" * En Promedio \" + descPol1 + \" (%s) genera\" % prom1\n","    if prom1 > prom2:\n","      strMostrar = strMostrar + \" MEJORES \"\n","    elif prom1 < prom2:\n","      strMostrar = strMostrar + \" PEORES \"\n","    else:\n","      strMostrar = strMostrar + \" IGUALES \"\n","    strMostrar = strMostrar + \"resultados que \" + descPol2 + \" (%s).\" % prom2\n","    print(strMostrar)\n","    print(\"\\n================================================================================================\\n\")\n","\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Pol칤tica Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","MostarDetalleSteps = False #@param {type:\"boolean\"}\n","\n","if Probar_Entorno:\n","  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema al azar\", 1, MostarDetalleSteps)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entorno del Problema definido.\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema al azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n"]}]},{"cell_type":"markdown","source":["##Q-Learning"],"metadata":{"id":"-Jl6zZnToIh5"}},{"cell_type":"code","source":["#@title Define clase auxiliar Policy para Q-Learning\n","\n","## funciones auxiliares basadas en:\n","##  https://rubikscode.net/2019/06/24/introduction-to-q-learning-with-python-and-open-ai-gym/\n","\n","from tf_agents.policies.fixed_policy import FixedPolicy\n","from tf_agents.trajectories import policy_step\n","from tf_agents.utils import nest_utils\n","from tf_agents.specs import tensor_spec\n","from tf_agents.typing import types\n","from typing import Optional, Text\n","\n","## funciones auxiliares para manejar TS->Obs\n","## (se puede definir otras si es necesario)\n","\n","# devuelve el primer valor de OBS \n","# como n칰mero de estado\n","def obtenerEstadoDeObs(obs):  \n","  return obs[0]\n","\n","# devuelve valor unico en base a vector OBS \n","# para determinar fila de estado\n","def generarNroEstadoVector(obs):\n","  ntarget = np.array(obs).flatten()\n","  valorTotal = 0        \n","  for i in range(1, len(ntarget)):\n","    if ntarget[i] == -1:\n","      valorTotal = valorTotal + 2**i    \n","    elif ntarget[i] == 1:\n","      valorTotal = valorTotal + 3**i      \n","  return valorTotal\n","\n","# devuelve la cantidad de estados m치xima\n","# que se pueden generar usando un ejemplo \n","# del vector de OBS \n","def calcMaxNroEstadoVector(ejemploObs):\n","  ntarget = np.array(ejemploObs).flatten()  \n","  cant = len(ntarget)\n","  return ( 3**cant + 2**cant + 1 )\n","\n","\n","# Clase para Q-Learning\n","# (se hereda de FixedPolicy porque es una simple para tener como base)\n","class QL_TF_Policy(FixedPolicy):\n","    \n","  def __init__(self,\n","               posiblesEstadosList, \n","               posiblesAccionesList,\n","               time_step_spec: ts.TimeStep,            \n","               action_spec: types.NestedTensorSpec,                              \n","               funcDevuelveNroEstado = obtenerEstadoDeObs,   \n","               policy_info: types.NestedTensorSpec = (),\n","               info_spec: types.NestedTensorSpec = (),\n","               name: Optional[Text] = None):    \n","    \n","      # llama al padre\n","      super(FixedPolicy, self).__init__(time_step_spec, action_spec, clip=False,\n","                                      info_spec=info_spec,\n","                                      name=name,\n","                                      emit_log_probability=False)             \n","      # guarda valores auxiliares\n","      self._policy_info = policy_info\n","      self._time_step_spec = tensor_spec.from_spec(time_step_spec)\n","      self._action_spec = tensor_spec.from_spec(action_spec)   \n","      self._funcDevuelveNroEstado = funcDevuelveNroEstado\n","      # inicializa parametros de matriz\n","      if (posiblesEstadosList is None) or (len(posiblesEstadosList)<2):\n","       raise ValueError('No se ha definida la lista de posibles estdos!' )\n","      self._posiblesEstadosList = posiblesEstadosList\n","      self._cantEstados = len(posiblesEstadosList)\n","      if (posiblesAccionesList is None) or (len(posiblesAccionesList)<2):\n","       raise ValueError('No se ha definida la lista de posibles acciones!' )\n","      self._posiblesAccionesList = posiblesAccionesList\n","      self._cantAcciones = len(posiblesAccionesList)\n","      self.ResetQ()\n","  \n","  # funcion auxiliar para inicializar la matriz\n","  def ResetQ(self):     \n","      self._Qtable = np.zeros([self._cantEstados, self._cantAcciones])\n","      self._QtableEntrenada = False \n","\n","  # funci칩n auxiliar de entrenamiento\n","  def TrainQ(self, env, train_policy, alpha = 0.1, gamma = 0.6, epsilon = 0.1, cant_ciclos_entrenamiento = 100000, log_cada_ciclos = 1000, mostrarDetalleAcciones=False):\n","      ##print(self._Qtable.shape)\n","      ##print(self._usarOBSVector)\n","      # ejecuta el entrenamiento\n","      for step in range(1, cant_ciclos_entrenamiento+1):\n","          # Resetea el enviroment\n","          time_step = env.reset()  \n","          ob = time_step.observation.numpy()[0]\n","          state = self._funcDevuelveNroEstado(ob)\n","          secuenciaAcciones = \"\"\n","          j = 1\n","          # Simula      \n","          while not time_step.is_last():\n","              # Considera lo aprendido o toma al azar depende de azar y epsilon\n","              if random.uniform(0, 1) < epsilon:\n","                  # toma de Matriz-Q\n","                  accionAplicar = np.argmax( self._Qtable[state] )              \n","              else:\n","                  # toma al azar\n","                  action_step = train_policy.action( time_step ) \n","                  accionAplicar = action_step.action.numpy()[0]\n","              if j > 1:\n","                secuenciaAcciones = secuenciaAcciones + \" + \"\n","              secuenciaAcciones = secuenciaAcciones + self._posiblesAccionesList[ accionAplicar ]\t\n","              j = j + 1\n","              # Aplica la Accion  \n","              time_step = env.step( accionAplicar )\n","              ob = time_step.observation.numpy()[0]\n","              next_state = self._funcDevuelveNroEstado(ob)\n","              r = time_step.reward.numpy()[0] \n","              # Recalcula Q\n","              q_value = self._Qtable[state, accionAplicar]\n","              max_value = np.max(self._Qtable[next_state])\n","              new_q_value = (1 - alpha) * q_value + alpha * (r + gamma * max_value)                       \n","              # Actualiza Matriz-Q\n","              self._Qtable[state, accionAplicar] = new_q_value\n","              state = next_state        \n","          # muestra estado\n","          if log_cada_ciclos > 0:\n","            if (step == 1) or (step % log_cada_ciclos == 0):\n","              if mostrarDetalleAcciones:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) = {3} '.format(step, r, (j-1), secuenciaAcciones))  \n","              else:\n","                print('step = {0}: Recompensa = {1} - Acciones({2}) '.format(step, r, (j-1)))  \n","      # Devuelve Matriz-Q\n","      self._QtableEntrenada = True\n","      return self._Qtable   \n","\n","  # obtiene Q-Table como DataFrame\n","  def getQ_DF(self):\n","        cm = self._Qtable \n","        cmtx = pd.DataFrame(\n","            cm, \n","            index=self._posiblesEstadosList, \n","            columns=self._posiblesAccionesList\n","          )\n","        return cmtx\n","\n","  # graba Q-Table como CSV\n","  def saveQ(self, fileName):\n","        # obtiene data frame de matriz Q\n","        df = self.getQ_DF()\n","        # graba matriz Q como CSV\n","        df.to_csv(fileName, index=True)\n","        return \n","\n","  # recupera Q-Table de  CSV\n","  def loadQ(self, fileName):\n","        # lee matriz Q de CSV\n","        df = pd.read_csv(fileName)         \n","        # carga elementos en memoria\n","          # saca nombre de estados\n","        self._posiblesEstadosList = list(df.pop(df.columns[0]))  \n","          # acciones\n","        self._posiblesAccionesList = list(df.columns.tolist())\n","          # borra el primer elemento de titulo\n","        ##self._posiblesAccionesList = np.delete(self._posiblesAccionesList, 0, axis=0)      \n","         # matriz Q\n","        self._Qtable = list(df.to_numpy())\n","        # re-inicializa las cantidades\n","        self._cantEstados = len(self._posiblesEstadosList)\n","        self._cantAcciones = len(self._posiblesAccionesList)\n","        self._QtableEntrenada = True\n","        # muestra resultados\n","        self.MostrarQ(\"Matriz-Q recuperada:\")\n","        return \n","\n","  # funci칩n auxiliar para mostrar matri Q\n","  def MostrarQ(self, titulo=\"Matriz-Q entrenada:\"):\n","        # muestra Q table\n","        cmtx = self.getQ_DF()\n","        print('\\n ' + titulo + ' ')        \n","        # agrega para poder mostrar la matrix de confusi칩n completa\n","        pd.options.display.max_rows = 100\n","        pd.options.display.max_columns = 100\n","        print(cmtx)\n","        print(\"\\n\")\n","        return \n","\n","  # devuelve la accion que se debe aplicar usando la matrix Q entrenada\n","  def _action(self, time_step, policy_state, seed):    \n","      # determina la accion a realizar\n","      # obtiene estado actual\n","      ob = time_step.observation.numpy()[0]\n","      state = self._funcDevuelveNroEstado(ob)\n","      # toma de Matriz-Q      \n","      accionAplicar = np.argmax( self._Qtable[state] )\n","      # formatea el valor a devolver usando la action_spec y time_step_spec\n","      def convert(action, spec):\n","        return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n","      self._action_value = tf.nest.map_structure(convert, accionAplicar,\n","                                                  self._action_spec)\n","      outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n","      action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n","                                   self._action_value)\n","      # devuelve la accion\n","      return policy_step.PolicyStep(action, policy_state, self._policy_info)\n","\n","\n","print(\"Clase QL_TF_Policy creada.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"vNLy2mNyi2xy","executionInfo":{"status":"ok","timestamp":1659014760954,"user_tz":180,"elapsed":16,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"89004011-34f0-4a8d-e7f9-a930bbc1cce7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase QL_TF_Policy creada.\n"]}]},{"cell_type":"code","source":["#@title Entrenar con algoritmo Q-Learning\n","\n","# par치metros\n","entrenar_QL = True #@param {type:\"boolean\"}\n","alpha = 0.1 #@param {type:\"number\"}\n","gamma = 0.6 #@param {type:\"number\"}\n","epsilon = 0.6 #@param {type:\"number\"}\n","cant_ciclos_entrenamiento_finalizar = 5000 #@param {type:\"integer\"}\n","mostar_recompensa_cada = 500  #@param {type:\"integer\"}\n","if alpha <= 0.0:\n","   alpha = 0.001\n","if gamma <= 0.0:\n","    gamma = 0.001\n","if epsilon <= 0.0:\n","   epsilon = 0.001    \n","if cant_ciclos_entrenamiento_finalizar <= 10:\n","   cant_ciclos_entrenamiento_finalizar = 10    \n","if mostar_recompensa_cada < 1:\n","  mostar_recompensa_cada = 1\n","\n","# instancia pol칤tica de Q Learning\n","ql_policy = QL_TF_Policy(posiblesEstadosList = posiblesEstadosDescrip,\n","                         posiblesAccionesList = posiblesAccionesDescrip,                         \n","                         time_step_spec = train_env.time_step_spec(),\n","                         action_spec = train_env.action_spec(),\n","                         funcDevuelveNroEstado = obtenerEstadoDeObs\n","                         )\n","\n","if entrenar_QL:\n","  # hace el entrenamiento\n","  print(\"** Comienza el Entrenamiento:\\n\")\n","  ql_policy.TrainQ(env = train_env, \n","                   train_policy = random_policy, \n","                   alpha = alpha, gamma = gamma, epsilon = epsilon, \n","                   cant_ciclos_entrenamiento = cant_ciclos_entrenamiento_finalizar, \n","                   log_cada_ciclos = mostar_recompensa_cada,\n","                   mostrarDetalleAcciones = True)\n","  print(\"\\n** Entrenamiento Finalizado **\")\n","  # muestra matriz\n","  ql_policy.MostrarQ()\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente Q-Learning.\")  "],"metadata":{"cellView":"form","id":"lQZNJ9kVoDLG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017047322,"user_tz":180,"elapsed":16840,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"4bbc40ee-6eb5-43ba-b3db-c52a9f348508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** Comienza el Entrenamiento:\n","\n","step = 1: Recompensa = -9.0 - Acciones(3) = pide carta + pide carta + pide carta \n","step = 500: Recompensa = 18.0 - Acciones(3) = pide carta + pide carta + decide no pedir m치s cartas \n","step = 1000: Recompensa = 9.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 1500: Recompensa = 10.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 2000: Recompensa = 17.0 - Acciones(4) = pide carta + pide carta + pide carta + decide no pedir m치s cartas \n","step = 2500: Recompensa = 11.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 3000: Recompensa = 10.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 3500: Recompensa = 8.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 4000: Recompensa = 13.0 - Acciones(3) = pide carta + pide carta + decide no pedir m치s cartas \n","step = 4500: Recompensa = 9.0 - Acciones(2) = pide carta + decide no pedir m치s cartas \n","step = 5000: Recompensa = 20.0 - Acciones(4) = pide carta + pide carta + pide carta + decide no pedir m치s cartas \n","\n","** Entrenamiento Finalizado **\n","\n"," Matriz-Q entrenada: \n","         pide carta  decide no pedir m치s cartas\n","SUMA=0    11.941562                    7.539054\n","SUMA=1     0.000000                    0.000000\n","SUMA=2    14.513891                   10.801132\n","SUMA=3    16.483953                   12.540074\n","SUMA=4    17.897921                   14.196789\n","SUMA=5    18.742169                   15.767144\n","SUMA=6    19.501762                   17.615219\n","SUMA=7    19.948881                   18.881656\n","SUMA=8    22.847002                   21.276742\n","SUMA=9    24.547185                   22.739032\n","SUMA=10   22.751682                   25.000000\n","SUMA=11   23.048137                   27.499998\n","SUMA=12   19.628852                   29.994769\n","SUMA=13   16.766255                   32.483619\n","SUMA=14   16.146406                   34.968759\n","SUMA=15   15.259022                   37.414406\n","SUMA=16    9.047976                   39.934137\n","SUMA=17    8.835959                   42.297735\n","SUMA=18    2.842644                   44.452347\n","SUMA=19    1.666163                   45.450827\n","SUMA=20   -3.414759                   48.091603\n","SUMA=21    0.000000                    0.000000\n","SUMA=22    0.000000                    0.000000\n","SUMA=23    0.000000                    0.000000\n","SUMA=24    0.000000                    0.000000\n","SUMA=25    0.000000                    0.000000\n","SUMA=26    0.000000                    0.000000\n","SUMA=27    0.000000                    0.000000\n","SUMA=28    0.000000                    0.000000\n","SUMA=29    0.000000                    0.000000\n","SUMA=30    0.000000                    0.000000\n","SUMA=31    0.000000                    0.000000\n","SUMA=32    0.000000                    0.000000\n","\n","\n"]}]},{"cell_type":"code","source":["#@title Probar Q-Learning Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if ql_policy._QtableEntrenada:\n","  compararRtdosPolicy(cantidad_probar, eval_env, ql_policy, random_policy, \"Agente Q-Learning entrenado\", \"el Azar\")\n"],"metadata":{"cellView":"form","id":"KgQWppPm_nDd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017047968,"user_tz":180,"elapsed":673,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"fedd7390-78d2-4da9-f6e5-fb1a1835eb78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 8 -> estado: SUMA=8\n"," #2: pide carta 2 -> estado: SUMA=10\n"," #3: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta Q -> estado: SUMA=14\n"," #3: decide no pedir m치s cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente Q-Learning entrenado (14.0) genera MEJOR resultado que el Azar (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 6 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (13.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 4 -> estado: SUMA=9\n"," #3: pide carta 8 -> estado: SUMA=17\n"," #4: decide no pedir m치s cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta J -> estado: SUMA=20\n"," #3: decide no pedir m치s cartas -> estado: SUMA=20\n"," Fin -> estado SUMA=20\n"," Recompensa final =  20.0\n","\n","--> Agente Q-Learning entrenado (17.0) genera PEOR resultado que el Azar (20.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: decide no pedir m치s cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (11.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 6 -> estado: SUMA=9\n"," #3: pide carta 9 -> estado: SUMA=18\n"," #4: decide no pedir m치s cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 5 -> estado: SUMA=12\n"," #3: pide carta 3 -> estado: SUMA=15\n"," #4: decide no pedir m치s cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","--> Agente Q-Learning entrenado (18.0) genera MEJOR resultado que el Azar (15.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 4 -> estado: SUMA=9\n"," #3: pide carta 7 -> estado: SUMA=16\n"," #4: decide no pedir m치s cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: pide carta 7 -> estado: SUMA=17\n"," #3: pide carta 2 -> estado: SUMA=19\n"," #4: pide carta 9 -> estado: SUMA=28\n"," Fin -> estado SUMA=28\n"," Recompensa final =  -7.0\n","\n","--> Agente Q-Learning entrenado (16.0) genera MEJOR resultado que el Azar (-7.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (10.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente Q-Learning entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta 6 -> estado: SUMA=15\n"," #3: decide no pedir m치s cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente Q-Learning entrenado (15.0) genera MEJOR resultado que el Azar (0.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente Q-Learning entrenado (13.4) genera MEJORES resultados que el Azar (4.1).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"markdown","source":["##DQN"],"metadata":{"id":"2a6HbaIgoJL4"}},{"cell_type":"code","metadata":{"cellView":"form","id":"diEOEg3JaMHa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014767939,"user_tz":180,"elapsed":16,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"97579a0f-c8cf-4081-90f5-9bbbef4638ad"},"source":["#@title Definir el Agente tipo DQN\n","entrenar_DQN = True # @param {type:\"boolean\"}\n","DQNpolicy = None\n","\n","if entrenar_DQN:\n","  tipo_agente = \"DQN\" #@param [\"DQN\", \"DQN Categorico (C51)\"]\n","  learning_rate = 1e-3  # @param {type:\"number\"}\n","  cant_neuronas_ocultas = \"100\" # @param {type:\"string\"}\n","  DQNCat_num_atoms = 51  # param {type:\"integer\"}\n","\n","  # controla cantidad de atoms para DQN Cat\n","  if DQNCat_num_atoms <= 1:\n","    DQNCat_num_atoms = 51\n","\n","  # Define cantidad de neuronas ocultas para RNA-Q\n","  hidden_layers = []\n","  for val in cant_neuronas_ocultas.split(','):\n","    if  int(val) < 1:\n","      hidden_layers.append( 10 )\n","    else:\n","      hidden_layers.append( int(val) )\n","  fc_layer_params = tuple(hidden_layers, )\n","\n","  if tipo_agente==\"DQN\":\n","\n","    #define las capas convolutional\n","    CNN_preprocessing_layers = None\n","\n","    # Define RNA-Q\n","    q_net = q_network.QNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.Variable(0)\n","\n","    # Define el agente de tipo Q\n","    ag = dqn_agent.DqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        q_network=q_net,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        train_step_counter=train_step_counter)\n","\n","    ag.initialize()\n","\n","    print(\"Agente DQN inicializado. \")\n","\n","  elif tipo_agente == \"DQN Categorico (C51)\":\n","    \n","    # Define RNA-Q Categ칩rico\n","    categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","        train_env.observation_spec(),\n","        train_env.action_spec(),\n","        num_atoms=DQNCat_num_atoms,\n","        fc_layer_params=fc_layer_params)\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    train_step_counter = tf.compat.v2.Variable(0)\n","    \n","    # par치metros especificos (por defecto)\n","    n_step_update = 2\n","    gamma = 0.99\n","\n","    # Define el agente de tipo Q Categ칩rico\n","    ag = CategoricalDqnAgent(\n","        train_env.time_step_spec(),\n","        train_env.action_spec(),\n","        categorical_q_network=categorical_q_net,\n","        optimizer=optimizer,\n","        n_step_update=n_step_update,\n","        td_errors_loss_fn=common.element_wise_squared_loss,\n","        gamma=gamma,\n","        train_step_counter=train_step_counter)\n","    \n","    ag.initialize()\n","    \n","    print(\"Agente DQN Categorico (C51) inicializado. \")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Agente DQN inicializado. \n"]}]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659014777240,"user_tz":180,"elapsed":9311,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"ad260f9c-7609-497a-ec61-e88f119bf445"},"source":["#@title M칠tricas para evaluaci칩n y Preparar datos para Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  # Definir M칠tricas para evaluaci칩n para Agente DQN\n","    \n","  # Se usa el promedio de la recompensa (la m치s com칰n)\n","  # See also the metrics module for standard implementations of different metrics.\n","  # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n","\n","  def compute_avg_return(environment, policy, num_episodes=10):\n","    if num_episodes == 0:\n","      return 0.0 \n","    total_return = 0.0\n","    for _ in range(num_episodes):\n","\n","      time_step = environment.reset()\n","      episode_return = 0.0\n","\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = environment.step(action_step.action)\n","        episode_return += time_step.reward\n","      total_return += episode_return\n","\n","    avg_return = total_return / num_episodes\n","    return avg_return.numpy()[0]\n","\n","  initial_collect_steps = 1000  # @param {type:\"integer\"} \n","  collect_steps_per_iteration =   50# @param {type:\"integer\"}\n","  replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","  batch_size = 64  # @param {type:\"integer\"}\n","\n","  # Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\n","  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","      data_spec = ag.collect_data_spec,\n","      batch_size = train_env.batch_size,\n","      max_length = replay_buffer_max_length)\n","\n","  # Recolecta datos generados al azar\n","  # This loop is so common in RL, that we provide standard implementations. \n","  # For more details see the drivers module.\n","  # https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n","\n","  def collect_step(environment, policy, buffer):\n","    time_step = environment.current_time_step()\n","    action_step = policy.action(time_step)\n","    next_time_step = environment.step(action_step.action)\n","    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","    # Add trajectory to the replay buffer\n","    buffer.add_batch(traj)\n","\n","  def collect_data(env, policy, buffer, steps):\n","    for _ in range(steps):\n","      collect_step(env, policy, buffer)\n","\n","  collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n","\n","  print(\"\\nDatos recolectados.\")\n","\n","  # Muestra ejemplo de los datos recolectados\n","  ##iter(replay_buffer.as_dataset()).next()\n","\n","  # Preparar los datos recolectados con trajectories de shape [Bx2x...]\n","  dataset = replay_buffer.as_dataset(\n","      num_parallel_calls=3, \n","      sample_batch_size=batch_size, \n","      num_steps=2).prefetch(3)\n","  iterator = iter(dataset)\n","  # Muestra ejemplo \n","  ##iterator.next()\n","  print(\"\\nDataset creado.\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Datos recolectados.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n","\n","Dataset creado.\n"]}]},{"cell_type":"code","source":["#@title Entrenar al Agente DQN\n","\n","if entrenar_DQN:\n","\n","  cant_ciclos_entrenamiento_finalizar =  5000# @param {type:\"integer\"}\n","  log_cada_ciclos = 200  # @param {type:\"integer\"}\n","  mostar_recompensa_cada = 500  # @param {type:\"integer\"}\n","  cant_episodios_evaluacion =  10# @param {type:\"integer\"}\n","  minima_recompensa_promedio_finalizar = 21.0 # @param {type:\"number\"}\n","  \n","  #  Optimize by wrapping some of the code in a graph using TF function (Optional)\n","  ag.train = common.function(ag.train)\n","\n","  # Reset the train step\n","  ag.train_step_counter.assign(0)\n","\n","  # Evaluate the agent's policy once before training.\n","  avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","  ar_cicloL = []\n","  ar_cicloR = []\n","  ar_returns = []\n","  ar_loss = []\n","\n","  print(\"\\n** Comienza el Entrenamiento **\\n\")\n","  for _ in range(cant_ciclos_entrenamiento_finalizar):\n","\n","    # Collect a few steps using collect_policy and save to the replay buffer.\n","    collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\n","\n","    # Sample a batch of data from the buffer and update the agent's network.\n","    experience, unused_info = next(iterator)\n","    train_loss = ag.train(experience).loss\n","\n","    step = ag.train_step_counter.numpy()\n","\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\n","      print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \n","      ar_cicloL.append( step )\n","      ar_loss.append( train_loss )\n","    \n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\n","      avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\n","      ar_cicloR.append( step )\n","      ar_returns.append( avg_return )\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\n","\n","      if (avg_return >= minima_recompensa_promedio_finalizar):\n","        print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \n","        break\n","\n","  DQNpolicy = ag.policy\n","  print(\"\\n** Entrenamiento Finalizado **\\n\")\n","else:\n","  print(\"No se ejecuta entrenamiento de Agente DQN.\")  "],"metadata":{"cellView":"form","id":"LQbSlCJW8BeN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017028394,"user_tz":180,"elapsed":2251169,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"f47f3add-b8c4-44ef-dbaa-961596f92f13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","** Comienza el Entrenamiento **\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.foldr(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","step = 1: loss = 24.622\n","step = 1: Promedio Recompensa = 0.0\n","step = 200: loss = 28.070\n","step = 400: loss = 31.736\n","step = 500: Promedio Recompensa = 14.1\n","step = 600: loss = 8.072\n","step = 800: loss = 4.521\n","step = 1000: loss = 5.269\n","step = 1000: Promedio Recompensa = 13.4\n","step = 1200: loss = 4.882\n","step = 1400: loss = 14.475\n","step = 1500: Promedio Recompensa = 14.2\n","step = 1600: loss = 6.526\n","step = 1800: loss = 5.982\n","step = 2000: loss = 1.922\n","step = 2000: Promedio Recompensa = 15.2\n","step = 2200: loss = 2.982\n","step = 2400: loss = 18.821\n","step = 2500: Promedio Recompensa = 16.5\n","step = 2600: loss = 5.374\n","step = 2800: loss = 10.127\n","step = 3000: loss = 11.777\n","step = 3000: Promedio Recompensa = 13.4\n","step = 3200: loss = 13.062\n","step = 3400: loss = 8.338\n","step = 3500: Promedio Recompensa = 14.6\n","step = 3600: loss = 0.852\n","step = 3800: loss = 3.453\n","step = 4000: loss = 7.652\n","step = 4000: Promedio Recompensa = 15.1\n","step = 4200: loss = 8.276\n","step = 4400: loss = 2.012\n","step = 4500: Promedio Recompensa = 14.0\n","step = 4600: loss = 3.113\n","step = 4800: loss = 1.977\n","step = 5000: loss = 1.611\n","step = 5000: Promedio Recompensa = 15.8\n","\n","** Entrenamiento Finalizado **\n","\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"9EBBl7mRkQYa","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1659017028407,"user_tz":180,"elapsed":38,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"e2f7e7e7-ee52-41fa-c81d-a2dd4296c3a9"},"source":["#@title Mostrar Gr치ficos del Entrenamiento del Agente DQN\n","\n","if entrenar_DQN:\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloR, ar_returns)\n","  plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloR))   \n","  plt.grid(True)\n","  plt.show()\n","\n","  plt.figure(figsize=(12,5)) \n","  plt.plot( ar_cicloL, ar_loss, color=\"red\" )\n","  plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\n","  #plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\n","  plt.ylabel('Valor')\n","  plt.xlabel('Ciclo')\n","  plt.xlim(right=max(ar_cicloL))   \n","  plt.grid(True)\n","  plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 864x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017029603,"user_tz":180,"elapsed":1224,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"310ba82e-bd32-4d55-ff59-ad59b635521d"},"source":["#@title Probar el Agente DQN Entrenado contra el Azar\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if DQNpolicy is not None:\n","  compararRtdosPolicy(cantidad_probar, eval_env, DQNpolicy, random_policy, \"Agente DQN entrenado\", \"el Azar\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 2 -> estado: SUMA=5\n"," #3: pide carta 10 -> estado: SUMA=15\n"," #4: decide no pedir m치s cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente DQN entrenado (15.0) genera MEJOR resultado que el Azar (0.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 4 -> estado: SUMA=7\n"," #3: pide carta 9 -> estado: SUMA=16\n"," #4: decide no pedir m치s cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN entrenado (16.0) genera MEJOR resultado que el Azar (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta J -> estado: SUMA=10\n"," #2: pide carta 2 -> estado: SUMA=12\n"," #3: pide carta As -> estado: SUMA=13\n"," #4: pide carta 10 -> estado: SUMA=23\n"," Fin -> estado SUMA=23\n"," Recompensa final =  -2.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: decide no pedir m치s cartas -> estado: SUMA=0\n"," Fin -> estado SUMA=0\n"," Recompensa final =  0.0\n","\n","--> Agente DQN entrenado (-2.0) genera PEOR resultado que el Azar (0.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: pide carta 10 -> estado: SUMA=21\n"," Fin -> estado SUMA=21\n"," Recompensa final =  21.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: decide no pedir m치s cartas -> estado: SUMA=6\n"," Fin -> estado SUMA=6\n"," Recompensa final =  6.0\n","\n","--> Agente DQN entrenado (21.0) genera MEJOR resultado que el Azar (6.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 4 -> estado: SUMA=8\n"," #3: pide carta 8 -> estado: SUMA=16\n"," #4: decide no pedir m치s cartas -> estado: SUMA=16\n"," Fin -> estado SUMA=16\n"," Recompensa final =  16.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: decide no pedir m치s cartas -> estado: SUMA=9\n"," Fin -> estado SUMA=9\n"," Recompensa final =  9.0\n","\n","--> Agente DQN entrenado (16.0) genera MEJOR resultado que el Azar (9.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta Q -> estado: SUMA=19\n"," #3: decide no pedir m치s cartas -> estado: SUMA=19\n"," Fin -> estado SUMA=19\n"," Recompensa final =  19.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: decide no pedir m치s cartas -> estado: SUMA=5\n"," Fin -> estado SUMA=5\n"," Recompensa final =  5.0\n","\n","--> Agente DQN entrenado (19.0) genera MEJOR resultado que el Azar (5.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: pide carta As -> estado: SUMA=17\n"," #3: decide no pedir m치s cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN entrenado (17.0) genera MEJOR resultado que el Azar (10.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta As -> estado: SUMA=11\n"," #3: pide carta 3 -> estado: SUMA=14\n"," #4: decide no pedir m치s cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta 4 -> estado: SUMA=11\n"," #3: decide no pedir m치s cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","--> Agente DQN entrenado (14.0) genera MEJOR resultado que el Azar (11.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: pide carta 5 -> estado: SUMA=11\n"," #3: decide no pedir m치s cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: decide no pedir m치s cartas -> estado: SUMA=7\n"," Fin -> estado SUMA=7\n"," Recompensa final =  7.0\n","\n","--> Agente DQN entrenado (11.0) genera MEJOR resultado que el Azar (7.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente DQN entrenado **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: pide carta 6 -> estado: SUMA=17\n"," #3: decide no pedir m치s cartas -> estado: SUMA=17\n"," Fin -> estado SUMA=17\n"," Recompensa final =  17.0\n","\n","**  Resultados de el Azar **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN entrenado (17.0) genera MEJOR resultado que el Azar (10.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente DQN entrenado (14.4) genera MEJORES resultados que el Azar (7.1).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"markdown","source":["## Comparar Q-Learning y DQN"],"metadata":{"id":"dxPfnPsTpQmz"}},{"cell_type":"code","source":["\n","\n","\n","#@title Cargar o Guardar los Agentes Q-Learning y DQN entrenados\n","\n","# par치metros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoRL/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-Blackjack\" #@param {type:\"string\"}\n","accion_realizar = \"Grabar Modelo\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","if accion_realizar != \"-\":\n","  import os\n","  from google.colab import drive\n","  from tf_agents.policies import TFPolicy, policy_saver\n","  # determina lugar donde se guarda el modelo\n","  policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","  qlCSV = policy_dir + \"/QM-QLearning.csv\"\n","  # Montar Drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  if (DQNpolicy is not None) and isinstance(DQNpolicy, TFPolicy):\n","    # guarda la politica del agente DQN entrenado\n","    tf_policy_saver = policy_saver.PolicySaver(DQNpolicy)\n","    tf_policy_saver.save(policy_dir)\n","    print(\"\\nPol칤tica DQN guardada en \", policy_dir)\n","  if ql_policy is not None:\n","    if not os.path.exists(policy_dir):\n","         os.makedirs(policy_dir)\n","    ql_policy.saveQ(qlCSV)\n","    print(\"\\nPol칤tica Q-Learning guardada en \", qlCSV)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la pol칤tica del modelo\n","  DQNpolicy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPol칤tica DQN recuperada de \", policy_dir)\n","  ql_policy.loadQ(qlCSV)\n","  print(\"\\nPol칤tica Q-Learning recuperada de \", qlCSV)\n"],"metadata":{"cellView":"form","id":"XTTpaVBfBOGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017222485,"user_tz":180,"elapsed":3329,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"52596b7c-de76-45f2-96f1-059d06391d35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\n","Pol칤tica Q-Learning guardada en  /content/gdrive/MyDrive/IA/demoRL/Modelos/policy-Blackjack/QM-QLearning.csv\n"]}]},{"cell_type":"code","source":["#@title Probar Q-Learning Entrenado contra Agente DQN Entrenado\n","cantidad_probar = 10 # @param {type:\"integer\"}\n","\n","if DQNpolicy is not None and ql_policy._QtableEntrenada:\n","  compararRtdosPolicy(cantidad_probar, eval_env, DQNpolicy, ql_policy, \"Agente DQN\", \"Agente Q-Learning\")\n"],"metadata":{"cellView":"form","id":"JP6KZI29H47G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659017029607,"user_tz":180,"elapsed":37,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"b5a8aeed-14b7-4b7a-9c62-4c521f4b7e10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","> Prueba  1 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta 9 -> estado: SUMA=18\n"," #3: decide no pedir m치s cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: decide no pedir m치s cartas -> estado: SUMA=7\n"," Fin -> estado SUMA=7\n"," Recompensa final =  7.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (7.0).\n","\n","> Prueba  2 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: pide carta 8 -> estado: SUMA=18\n"," #3: decide no pedir m치s cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (13.0).\n","\n","> Prueba  3 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 10 -> estado: SUMA=10\n"," #2: pide carta 5 -> estado: SUMA=15\n"," #3: decide no pedir m치s cartas -> estado: SUMA=15\n"," Fin -> estado SUMA=15\n"," Recompensa final =  15.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: decide no pedir m치s cartas -> estado: SUMA=10\n"," Fin -> estado SUMA=10\n"," Recompensa final =  10.0\n","\n","--> Agente DQN (15.0) genera MEJOR resultado que Agente Q-Learning (10.0).\n","\n","> Prueba  4 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 9 -> estado: SUMA=9\n"," #2: pide carta J -> estado: SUMA=19\n"," #3: decide no pedir m치s cartas -> estado: SUMA=19\n"," Fin -> estado SUMA=19\n"," Recompensa final =  19.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 6 -> estado: SUMA=6\n"," #2: decide no pedir m치s cartas -> estado: SUMA=6\n"," Fin -> estado SUMA=6\n"," Recompensa final =  6.0\n","\n","--> Agente DQN (19.0) genera MEJOR resultado que Agente Q-Learning (6.0).\n","\n","> Prueba  5 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 5 -> estado: SUMA=10\n"," #3: pide carta J -> estado: SUMA=20\n"," #4: decide no pedir m치s cartas -> estado: SUMA=20\n"," Fin -> estado SUMA=20\n"," Recompensa final =  20.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 10 -> estado: SUMA=14\n"," #3: decide no pedir m치s cartas -> estado: SUMA=14\n"," Fin -> estado SUMA=14\n"," Recompensa final =  14.0\n","\n","--> Agente DQN (20.0) genera MEJOR resultado que Agente Q-Learning (14.0).\n","\n","> Prueba  6 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 8 -> estado: SUMA=12\n"," #3: decide no pedir m치s cartas -> estado: SUMA=12\n"," Fin -> estado SUMA=12\n"," Recompensa final =  12.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 3 -> estado: SUMA=3\n"," #2: pide carta 9 -> estado: SUMA=12\n"," #3: decide no pedir m치s cartas -> estado: SUMA=12\n"," Fin -> estado SUMA=12\n"," Recompensa final =  12.0\n","\n","--> Agente DQN (12.0) genera IGUAL resultado que Agente Q-Learning (12.0).\n","\n","> Prueba  7 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 6 -> estado: SUMA=11\n"," #3: decide no pedir m치s cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 5 -> estado: SUMA=5\n"," #2: pide carta 8 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (11.0) genera PEOR resultado que Agente Q-Learning (13.0).\n","\n","> Prueba  8 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 7 -> estado: SUMA=7\n"," #2: pide carta As -> estado: SUMA=18\n"," #3: decide no pedir m치s cartas -> estado: SUMA=18\n"," Fin -> estado SUMA=18\n"," Recompensa final =  18.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta As -> estado: SUMA=11\n"," #2: decide no pedir m치s cartas -> estado: SUMA=11\n"," Fin -> estado SUMA=11\n"," Recompensa final =  11.0\n","\n","--> Agente DQN (18.0) genera MEJOR resultado que Agente Q-Learning (11.0).\n","\n","> Prueba  9 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta 2 -> estado: SUMA=2\n"," #2: pide carta 9 -> estado: SUMA=11\n"," #3: pide carta J -> estado: SUMA=21\n"," Fin -> estado SUMA=21\n"," Recompensa final =  21.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 8 -> estado: SUMA=8\n"," #2: decide no pedir m치s cartas -> estado: SUMA=8\n"," Fin -> estado SUMA=8\n"," Recompensa final =  8.0\n","\n","--> Agente DQN (21.0) genera MEJOR resultado que Agente Q-Learning (8.0).\n","\n","> Prueba  10 :\n","\n","**  Resultados de Agente DQN **\n"," Ini: estado SUMA=0\n"," #1: pide carta Q -> estado: SUMA=10\n"," #2: pide carta 3 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","**  Resultados de Agente Q-Learning **\n"," Ini: estado SUMA=0\n"," #1: pide carta 4 -> estado: SUMA=4\n"," #2: pide carta 9 -> estado: SUMA=13\n"," #3: decide no pedir m치s cartas -> estado: SUMA=13\n"," Fin -> estado SUMA=13\n"," Recompensa final =  13.0\n","\n","--> Agente DQN (13.0) genera IGUAL resultado que Agente Q-Learning (13.0).\n","\n","================================================================================================\n","\n"," * En Promedio Agente DQN (16.5) genera MEJORES resultados que Agente Q-Learning (10.7).\n","\n","================================================================================================\n","\n"]}]},{"cell_type":"code","metadata":{"id":"qlwElBTZrVcX","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["976f3dde9b8a4392ba79f0f27476bf36","a37b35cc25564aa0ab44485b5dfbdf4e","d92fd232e68c4a758efceabd7fd23818","52ff142f411a4a4394b570694f804155","22a72ffb8b53441d8150b0e893b007b6","c7892e049679440f961bde0f0f7e05dc","e8843eb7d58349d59d76582186baced5","235d8993871a409fb9e2b1d64c192fa6","87b2863cd95d465f8ed10a8be741ccae","122781ec73164df693c4c1d416d77dcd","d14632828d254a258b1a480ffe6da868"]},"executionInfo":{"status":"ok","timestamp":1659017030495,"user_tz":180,"elapsed":909,"user":{"displayName":"pgp tensorflow","userId":"04809512947468796788"}},"outputId":"3909fac9-f344-41e2-906a-a8782848fda0"},"source":["#@title Probar Agentes entrenados contra Usuario Humano \n","#@markdown (notar que agentes no conocen las cartas que salieron al otro)\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","def resetear():\n","  global agentesDescrip, envAgentes, nroPasoAgentes, TSAgentes\n","  # Inicializar variables\n","  envAgentes = []\n","  nroPasoAgentes = []\n","  TSAgentes = []\n","  # Crea los entornos e inicializa variables\n","  for i in range(len(agentesDescrip)):\n","    env = tf_py_environment.TFPyEnvironment( CardGameEnv() )\n","    ts = env.reset()  \n","    #ob = time_step.observation.numpy()[0]    \n","    envAgentes.append( env )\n","    nroPasoAgentes.append( 1 )\n","    TSAgentes.append( ts )\n","\n","def jugar(prefijoJugador, nombreJugador, env, paso, action):\n","  if paso < 0:\n","    # no se hace porque ya termino la partida\n","    return paso\n","  else:\n","    # aplica la acci칩n\n","    time_step = env.step(action)\n","    # recupera la observaci칩n y muestra el nuevo estado \n","    ob = time_step.observation.numpy()[0]\n","    if ob[1] == 0:\n","      descAccion = posiblesAccionesDescrip[1]\n","    else:\n","      descAccion =posiblesAccionesDescrip[0] + \" \" + posiblesCartas[ ob[1] ]\n","    print(prefijoJugador, \"Juega\", nombreJugador, \"#\" + str(paso) + \":\", descAccion, \"->\", posiblesEstadosDescrip[ob[0]] )\n","    # ve si termino o sigue\n","    if time_step.is_last():\n","      paso = -1\n","    else:\n","      paso = paso + 1\n","    return paso, time_step\n","\n","def jugarAgentes():\n","    global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","    algunoPuedeSeguirJugando = False\n","    # inicia jugando primero a los agentes\n","    for i in range(len(agentesPolicy)):\n","      # no es humano\n","      if i != idPosHumano:\n","        # puede seguir jugando\n","        if nroPasoAgentes[i] >= 0:\n","          aux_action_step  = agentesPolicy[i].action( TSAgentes[i] )\n","          auxPaso, auxTS = jugar(\"*\", agentesDescrip[i], envAgentes[i], nroPasoAgentes[i], aux_action_step.action)\n","          nroPasoAgentes[i] = auxPaso\n","          TSAgentes[i] = auxTS\n","          algunoPuedeSeguirJugando = algunoPuedeSeguirJugando or (auxPaso >= 0)\n","    return algunoPuedeSeguirJugando\n","\n","# define funciones para los botones\n","def on_buttonR_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca bot칩n para reiniciar la partida\n","  with output:\n","    clear_output()\n","    print(\"\\n> Nueva Partida:\\n\")\n","    # resetea ambientes\n","    resetear()\n","    # inicia jugando primero a los agentes\n","    jugarAgentes()\n","    # fuerza que le de la primera carta al humano\n","    on_button0_clicked(button0)\n","  # muestra botones\n","  display(button0, button1, buttonR, output)\n","\n","def on_button0_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca bot칩n de pedir nueva carta\n","  with output:\n","    # juega humano\n","    if nroPasoAgentes[idPosHumano] >= 0:\n","      auxPaso, auxTS = jugar(\"+\", agentesDescrip[idPosHumano], envAgentes[idPosHumano], nroPasoAgentes[idPosHumano], np.array(0, dtype=np.int32))\n","      nroPasoAgentes[idPosHumano] = auxPaso\n","      TSAgentes[idPosHumano] = auxTS\n","    print(\" \")\n","    # juega agentes \n","    puedenAGSeguir = True\n","    while puedenAGSeguir:\n","      puedenAGSeguir = jugarAgentes()\n","      if nroPasoAgentes[idPosHumano] >= 0:\n","        # si el humano no termin칩, lo hace jugar una vez solamente\n","        break\n","    # si todos finalizaron, muestra el estado final\n","    if not(puedenAGSeguir) and (nroPasoAgentes[idPosHumano] < 0):\n","      mostrarEstadoFinal()\n","\n","def on_button1_clicked(b):\n","  global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes\n","  # se toca bot칩n de finalizar partida (plantarse)\n","  with output:\n","    # juega humano\n","    if nroPasoAgentes[idPosHumano] >= 0:\n","      auxPaso, auxTS = jugar(\"+\", agentesDescrip[idPosHumano], envAgentes[idPosHumano], nroPasoAgentes[idPosHumano], np.array(1, dtype=np.int32))\n","      nroPasoAgentes[idPosHumano] = auxPaso\n","      TSAgentes[idPosHumano] = auxTS\n","    print(\" \")\n","    # juega agentes hasta que terminen\n","    puedenAGSeguir = True\n","    while puedenAGSeguir:\n","      puedenAGSeguir = jugarAgentes()\n","    # muesta el estado final\n","    mostrarEstadoFinal()\n","    print(\"\\n\")\n","\n","def mostrarEstadoFinal():\n","    global agentesDescrip, agentesPolicy, envAgentes, nroPasoAgentes, TSAgentes        \n","    # muestra estado final\n","    print(\"\\n> Resultados: \")\n","    rewL = []\n","    for i in range(len(agentesDescrip)):\n","      r = int( TSAgentes[i].reward.numpy()[0] )\n","      rewL.append( r )\n","      resul = \"%s\" % r\n","      if (r > 21) or (r < 0):\n","        resul = resul + \"!\"\n","      print(\"\\t \" + agentesDescrip[i] + \": \" + resul)\n","    print(\"\\t = GANA: \"  + agentesDescrip[ np.argmax(rewL) ])  \n","    print(\"\\n\")\n","    return\n","\n","\n","# inicializa variables auxiliares\n","idPosHumano = 2\n","agentesDescrip = []\n","agentesPolicy = []\n","# ql\n","if ql_policy._QtableEntrenada:\n","  agentesDescrip.append( \"Agente QL\" )\n","  agentesPolicy.append( ql_policy )\n","# dqn\n","if DQNpolicy is not None:\n","  agentesDescrip.append( \"Agente DQN\" )\n","  agentesPolicy.append( DQNpolicy )\n","# humano\n","agentesDescrip.append( \"Humano\" )\n","agentesPolicy.append( None )\n","resetear()\n","\n","# define botones\n","button0 = widgets.Button(description=\"Dame Otra Carta\")\n","button1 = widgets.Button(description=\"Plantarse\")\n","buttonR = widgets.Button(description=\"Reiniciar Partida\")\n","output = widgets.Output()\n","buttonR.on_click(on_buttonR_clicked)\n","button1.on_click(on_button1_clicked)\n","button0.on_click(on_button0_clicked)    \n","\n","# comienza a jugar\n","on_buttonR_clicked(buttonR)\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Dame Otra Carta', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976f3dde9b8a4392ba79f0f27476bf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Plantarse', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ff142f411a4a4394b570694f804155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Reiniciar Partida', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8843eb7d58349d59d76582186baced5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122781ec73164df693c4c1d416d77dcd"}},"metadata":{}}]}]}